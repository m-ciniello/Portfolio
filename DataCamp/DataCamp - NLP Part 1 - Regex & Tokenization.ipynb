{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue size=5><b> 1. Regular Expression & Word Tokenization</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green size=3><b> 1.1 Regex </font>\n",
    "\n",
    "Regular Expressions are strings that have special syntax, which allow you to match patterns in other strings. Use it to:\n",
    "- find links in webpages\n",
    "- parse emails address, remove replace unwanter characters\n",
    "\n",
    "IDENTIFIERS\n",
    "- \\d any number\n",
    "- \\D anything BUT a number\n",
    "- \\s space\n",
    "- \\w any character\n",
    "- \\W anything BUT a character\n",
    "- \\\\. is period\n",
    "- . ANY character, except for a newline\n",
    "- \\b the whitespace around words\n",
    "\n",
    "MODIFIERS\n",
    "- {x,y} we're expected x-y digits \n",
    "- \\+ Match 1 or more\n",
    "- ? Match 0 or 1\n",
    "- \\* Match 0 or more\n",
    "- $ Match the end of a string\n",
    "- ^ Match the begning of a string\n",
    "- | either or\n",
    "- [] range or \"variance\"\n",
    "- {x} expected x amount\n",
    "\n",
    "WHITE SPACE CHARACTERS\n",
    "- \\n new line\n",
    "- \\s space\n",
    "- \\t tab\n",
    "- \\e escape\n",
    "- \\f form feed\n",
    "- \\r return\n",
    "\n",
    "*Using any of these as a CAPITAL negates them*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15', '27', '97', '102']\n",
      "['Jessica', 'Daniel', 'Edward', 'Oscar']\n",
      "{'Jessica': '15', 'Daniel': '27', 'Edward': '97', 'Oscar': '102'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "examplestring = '''\n",
    "Jessica is 15 years old, and Daniel is 27 yeas old.\n",
    "Edward is 97, and his grandfather, Oscar, is 102.\n",
    "'''\n",
    "#EXAMPLE 1:\n",
    "ages = re.findall(r'\\d{1,3}',examplestring)\n",
    "names = re.findall(r'[A-Z][a-z]*',examplestring)\n",
    "#the astrix is basically saying as many repitions as there are, as soon as it hits something else it stops\n",
    "\n",
    "print(ages)\n",
    "print(names)\n",
    "\n",
    "agedict= {}\n",
    "x=0\n",
    "#put them into a disctionary\n",
    "for eachname in names:\n",
    "    agedict[eachname] = ages[x]\n",
    "    x +=1\n",
    "print(agedict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'write', 'RegEx']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GET THIS RESULT: ['Let', 's', 'write', 'RegEx']\n",
    "import re\n",
    "my_string = \"Let's write RegEx!\"\n",
    "re.findall(\"\\w+\", my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASKS: Practicing regular expressions: re.split() and re.findall()**\n",
    "\n",
    "<font color =red><br>Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. **For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "my_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\" #THIS WILL EXCLUDE \"I\"  AND OTHER SINGLE LETTER CAPITALIZED WORDS\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green size=3><b> 1.2 Tokenization </font>\n",
    "\n",
    "String tokenization: transforming string or document into smaller chunks called tokens.\n",
    "\n",
    "nltk: natural language toolkit is the most popular library for this task. \n",
    "\n",
    "Key uses:\n",
    "- easier to map parts of speech\n",
    "- matching common words\n",
    "- remmoving unwanted tokens\n",
    "\n",
    "<font color=red>IMPORTANT NOTE:<br>\n",
    "- **re.match** looks for patterns FROM THE BEGINING OF A STRING\n",
    "<br>- **re.search** looks for a pattern in the ENTIRE STRING</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASKS: Word tokenization with NLTK:**\n",
    "- Import the sent_tokenize and word_tokenize functions from nltk.tokenize.\n",
    "- Tokenize all the sentences in scene_one using the sent_tokenize() function.\n",
    "- Tokenize the fourth sentence in sentences, which you can access as sentences[3], using the word_tokenize() function.\n",
    "- Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set using set().\n",
    "- Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and mast'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monty python scene 1 document!\n",
    "scene_one= \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
    "scene_one[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bird', 'goes', 'not', 'Ridden', 'you', 'from', 'is', 'since', 'that', 'all', 'strangers', 'or', '?', 'Will', 'SCENE', 'maybe', 'winter', 'wants', 'Well', 'martin', 'Saxons', 'carry', 'velocity', 'me', 'I', 'our', 'grip', 'migrate', 'You', 'No', 'do', 'just', 'interested', 'In', ':', 'suggesting', 'Camelot', 'question', 'tell', '.', 'Found', 'Please', 'five', '[', 'will', 'They', 'Britons', 'Am', 'go', 'carried', 'mean', 'breadth', ']', 'to', 'on', \"n't\", 'African', 'my', 'found', 'KING', \"'m\", 'Listen', 'ARTHUR', 'sovereign', \"'s\", 'ratios', 'We', 'back', 'them', 'seek', 'it', 'castle', 'temperate', 'an', 'could', 'climes', 'speak', 'horse', 'beat', 'here', 'Oh', 'That', 'this', 'they', 'other', 'simple', 'why', 'ounce', 'But', 'land', 'halves', 'fly', 'its', \"'ve\", 'coconut', 'defeator', 'who', 'zone', 'are', 'Wait', 'yeah', 'Arthur', 'guiding', 'but', 'empty', 'Whoa', 'covered', 'forty-three', 'one', \"'em\", 'these', 'wings', 'Halt', 'creeper', 'together', 'clop', 'King', 'Patsy', 'Where', 'ridden', 'Are', 'tropical', 'pound', 'needs', 'yet', 'at', \"'d\", 'weight', 'non-migratory', 'minute', 'got', 'ask', 'wind', 'line', 'course', 'swallow', 'Pendragon', 'Yes', 'length', 'grips', 'bangin', 'Supposing', 'order', 'then', '1', 'Pull', 'strand', '#', 'with', 'of', 'master', 'knights', 'anyway', 'trusty', 'bring', 'use', 'every', 'search', 'be', \"'re\", 'he', '2', ',', 'So', 'sun', 'What', 'carrying', 'warmer', 'a', '...', 'agree', 'kingdom', '--', 'The', 'Mercea', 'European', 'right', 'and', 'house', 'times', 'does', 'Uther', 'second', 'where', 'matter', 'using', 'by', \"'\", 'snows', 'your', 'have', '!', 'son', 'England', 'servant', 'dorsal', 'Who', 'court', 'must', 'feathers', 'in', 'if', 'It', 'am', 'the', 'lord', 'two', 'maintain', 'held', 'SOLDIER', 'join', 'get', 'may', 'south', 'air-speed', 'husk', 'there', 'Court', 'swallows', 'Not', 'plover', 'coconuts', 'A', 'point', 'through', 'under'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one)) #sets only contain unique values!!!\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASKS: More regex with re.search()**\n",
    "\n",
    "- Use re.search() to search for the first occurance of the word \"coconuts\" in scene_one. Store the result in match.\n",
    "- Print the start and end indexes of match using its .start() and .end() methods, respectively.\n",
    "- Write a regular expression called pattern1 to find anything in square brackets.\n",
    "- Use re.search() with the previous pattern to find the first text in square brackets in the scene. Print the result.\n",
    "- Use re.match() to match the script notation in the fourth line (ARTHUR:) and print the result. The tokenized sentences of scene_one are available in your namespace as sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(580, 588), match='coconuts'>\n",
      "580 588\n",
      "<_sre.SRE_Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<_sre.SRE_Match object; span=(9, 15), match='[wind]'>\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "print(match)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "pattern2 = r\"\\[[a-z]*\\]\" #WHY IS THIS DIFFERENT?!?!?!?!\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "print(re.search(pattern2, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern3 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern3, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green size=3><b> 1.3 Advanced Tokenization </font>\n",
    "\n",
    "- OR is represented using \"|\"\n",
    "- you can define a group using \"()\"\n",
    "- you can define explicity character ranges using \"[]\"\n",
    "\n",
    "![](pictures/regex.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASKS: Regex wth NLTK tokenization**\n",
    "- Import the regexp_tokenize and TweetTokenizer from nltk.tokenize.\n",
    "- A regex pattern to define hashtags called pattern1 has been defined for you. Call regexp_tokenize() with this hashtag pattern on the first tweet in tweets.\n",
    "- Write a new pattern called pattern2 to match mentions or hashtags. A mention is something like @DataCamp. Then, call regexp_tokenize() with your new hashtag pattern on the last tweet in tweets. You can access the last element of a list using -1 as the index, for example, tweets[-1].\n",
    "- Create an instance of TweetTokenizer called tknzr and use it inside a list comprehension to tokenize each tweet into a new list called all_tokens. To do this, use the .tokenize() method of tknzr, with t as your iterator variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python'] \n",
      "\n",
      "['@datacamp', '#nlp', '#python'] \n",
      "\n",
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "print(regexp_tokenize(tweets[0], pattern1),'\\n')\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([#|@]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "print(regexp_tokenize(tweets[-1], pattern2),'\\n')\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASKS: Non-ascii tokenization**\n",
    "\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "\"Wann gehen wir zum Pizza? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•\"\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "\n",
    "- '\\U0001F300'-'\\U0001F5FF'), \n",
    "- ('\\U0001F600-\\U0001F64F'), \n",
    "- ('\\U0001F680-\\U0001F6FF'), and \n",
    "- ('\\u2600'-\\u26FF-\\u2700-\\u27BF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_text = \"Wann gehen wir zum Pizza? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'zum', 'Pizza', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•'] \n",
      "\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber'] \n",
      "\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "from  nltk.tokenize import regexp_tokenize, word_tokenize\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words,'\\n')\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words),'\\n')\n",
    "\n",
    "# Tokenize and print only emoji (NOTE I DONT THINK YOU NEED TO \"|\" operator here)\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green size=3><b> 1.4 Charting Word Length with NLTK </font>\n",
    "\n",
    "**TASKS: Charting Practice**\n",
    "\n",
    "- Split the script into lines using the newline ('\\n') character.\n",
    "- Use re.sub() inside a list comprehension to replace the prompts such as - ARTHUR: and SOLDIER #1.\n",
    "- Use a list comprehension to tokenize lines with regexp_tokenize(), keeping only words. Recall that the pattern for words is \"\\w+\".\n",
    "- Use a list comprehension to create a list of line lengths called line_num_words.\n",
    "    - Use t_line as your iterator variable to iterate over tokenized_lines, and then len() function to compute line lengths.\n",
    "- Plot a histogram of line_num_words using plt.hist(). Don't forgot to use plt.show() as well to display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SCENE 1: [wind] [clop clop clop] \\\\nKING ARTHUR: Whoa there!  [clop clop clop] \\\\nSOLDIER #1: Halt!  Who goes there?\\\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\\\nSOLDIER #1: Pull the other one!\\\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('holy_grail.txt', 'r') as myfile:\n",
    "    holy_grail=myfile.read()\n",
    "    \n",
    "holy_grail[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING ARTHUR: Whoa there!  [clop clop clop] \n",
      " Whoa there!  [clop clop clop] \n"
     ]
    }
   ],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines2 = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "print(lines[1])\n",
    "print(lines2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whoa', 'there', 'clop', 'clop', 'clop']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines2]\n",
    "tokenized_lines[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "line_num_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b10c9c9048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words, bins=100)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
