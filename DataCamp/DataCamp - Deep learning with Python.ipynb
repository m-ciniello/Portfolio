{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color= red> MODULE 3: BUILDING DEEP LEARNING MODELS WITH KERAS </font>\n",
    "\n",
    "# Model Building Steps:\n",
    "\n",
    "1. Specify Architecture\n",
    "    - Eg. # of nodes, layers, activation functions, etc\n",
    "2. Compile Model\n",
    "    - Specify loss funciton and some details about how optimization works\n",
    "3. Fit Model\n",
    "    - Cycle through feedforward predictions and back propogations and using gradient descent to optimize weights\n",
    "4. Make Predictions and Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['wage_per_hour', 'union', 'education_yrs', 'experience_yrs', 'age',\n",
      "       'female', 'marr', 'south', 'manufacturing', 'construction'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "import pandas as pd\n",
    "df=pd.read_csv('worker_wages.csv')\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  8 21 35  1  1  0  1  0]\n",
      " [ 0  9 42 57  1  1  0  1  0]\n",
      " [ 0 12  1 19  0  0  0  1  0]\n",
      " [ 0 12  4 22  0  0  0  0  0]\n",
      " [ 0 12 17 35  0  1  0  0  0]] \n",
      "\n",
      "[ 5.1   4.95  6.67  4.    7.5 ]\n"
     ]
    }
   ],
   "source": [
    "#data looks good! Lets split the predictors and target into numpy arrays\n",
    "predictor_cols = ['union', 'education_yrs', 'experience_yrs', 'age','female', 'marr', 'south', 'manufacturing', 'construction']\n",
    "predictors = df[predictor_cols].as_matrix()\n",
    "target = df.wage_per_hour.as_matrix()\n",
    "\n",
    "print(predictors[0:5],'\\n')\n",
    "print(target[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying a Model\n",
    "\n",
    "Now you'll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters.\n",
    "\n",
    "To start, you'll take the skeleton of a neural network and add a hidden layer and an output layer. You'll then fit that model and see Keras do the optimization so your model continually gets better.\n",
    "\n",
    "As a start, you'll predict workers wages based on characteristics like their industry, education and level of experience. You can find the dataset in a pandas dataframe called df. For convenience, everything in df except for the target has been converted to a NumPy matrix called predictors. The target, wage_per_hour, is available as a NumPy matrix called target.\n",
    "\n",
    "For all exercises in this chapter, we've imported the Sequential model constructor, the Dense layer constructor, and pandas.\n",
    "\n",
    "**Instructions**\n",
    "- Store the number of columns in the predictors data to n_cols. This has been done for you.\n",
    "- Start by creating a Sequential model called model.\n",
    "- Use the .add() method on model to add a Dense layer.\n",
    "  - Add 50 units, specify activation='relu', and the input_shape parameter to be the tuple (n_cols,) which means it has n_cols items in each row of data, and any number of rows of data are acceptable as inputs.\n",
    "- Add another Dense layer. This should have 32 units and a 'relu' activation.\n",
    "- Finally, add an output layer, which is a Dense layer with a single node. Don't use any activation function here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VIDEO: Specifying Model**\n",
    "\n",
    "Number of Columns:\n",
    "- Remember, we must specify the number of columns (save as n_cols) of the predictors, as this will be the number of nodes we have in the input layer!\n",
    "\n",
    "Sequential Models:\n",
    "- There are two ways of building a model, Sequential is the easiest way so we will focus on that for now. \n",
    "- Sequential models requires that each layers has weights or connections only to the one layer coming directly after it in the network diagram. \n",
    "- There are more exotic models with complex patterns of connecitons, but we will come there later\n",
    "\n",
    "'Dense' layers\n",
    "- Add the first layer using model.add(Dense...)\n",
    "- Dense is the standard layer that we have seen in the diagrams so far\n",
    "- It is called 'Dense' because all the nodes in the previous layer connect to all of the nodes in the current layer\n",
    "- As you advance in Deep Learning you may start using layers that are NOT dense, but lets stick with these for now\n",
    "- In first layer, you specify:\n",
    " - first arg: number of nodes\n",
    " - second arg: activation function\n",
    " - third layer: INPUT SHAPE (this only applies to the first layers)\n",
    "   - input_shape = (n_cols,), this means that input data will have n_cols inputs, and we leave the space after the column blank because we can have ANY number of observations (rows)\n",
    "\n",
    "Second layer\n",
    " - model.add(Dense(100,activation='Relu',input_shape=(n_cols,)))\n",
    "\n",
    "Third Layer\n",
    " - model.add(Dense(1))\n",
    " - Notice that this layer only has ONE node. This is the output layer, and matches the diagrams where we ended with just a prediciton.\n",
    " \n",
    "**So this model has TWO hidden layers, and an output layer. You may be struck that each hidden layers has 100 rows. This is normal, and its quite common to have even 1000s of nodes in hidden layers. We will learn more about choosing appropriate number of nodes layer**\n",
    "\n",
    "![](modelspec.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer \n",
    "# this effecitvely adds the input layer as well with the last arg!!!\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling a Model\n",
    "\n",
    "**VIDEO: Compiling and Fitting a Model**\n",
    "After specified a model, next step is to complie while sets up network for optimization... for instance creating internaal funciton to do back propogation efficiently.\n",
    "\n",
    "Compile method has TWO different arguments for you to choose:\n",
    "1. Specify the optimizer\n",
    " - Controls the learning rate\n",
    " - In practice, choosing the right learning rate can effect how fast out model finds the right weights, and even HOW GOOD the weights it finds are\n",
    " - There are a few algos that automatically tune the learning rate. BUT even many experts dont know all of these, so best approach is to choose a versatile algo: 'Adam'\n",
    " - **'Adam' is a excellent choice for goto optimizers. It adjusts learning rate as it goes through gradient descent, to ensure reasonable values throughout weight optimization process. **\n",
    "2. Choose the loss function (error metric)\n",
    "  - mean_squarred_error is the most common for regression problems\n",
    "  - we will use a different default metric when we use keras for classifiers\n",
    "\n",
    "Fitting a Model\n",
    "- As we know, fitting a model is applying backpropogation and gradient descent with your data to update the weights\n",
    "- Scaling data: even with the 'Adam optimizer', it can imporve optimization process if, on average, each feature has similar size values\n",
    "- one common approach is to normalize by:\n",
    "  - subtract each feature by that features mean, and divide by that features standard deviation\n",
    "\n",
    "\n",
    "\n",
    "![](compilefit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now going to compile the model you specified earlier. To compile the model, you need to specify the optimizer and loss function to use. In the video, Dan mentioned that the Adam optimizer is an excellent choice. You can read more about it as well as other keras optimizers here, and if you are really curious to learn more, you can read the original paper that introduced the Adam optimizer (https://arxiv.org/pdf/1412.6980v8.pdf).\n",
    "\n",
    "In this exercise, you'll use the Adam optimizer and the mean squared error loss function. Go for it!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Compile the model using model.compile(). Your optimizer should be 'adam' and the loss should be 'mean_squared_error'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Model\n",
    "\n",
    "You're at the most fun part. You'll now fit the model. Recall that the data to be used as predictive features is loaded in a NumPy matrix called predictors and the data to be predicted is stored in a NumPy matrix called target. Your model is pre-written and it has been compiled with the code from the previous exercise.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Fit the model. Remember that the first argument is the predictive features (predictors), and the data to be predicted (target) is the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "534/534 [==============================] - 0s - loss: 36.2535     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s - loss: 26.9723     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s - loss: 23.7572     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s - loss: 22.6871     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s - loss: 22.1638     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s - loss: 22.2581     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s - loss: 22.0867    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s - loss: 21.5714     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s - loss: 21.2874     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s - loss: 21.1596     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16830c18>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model\n",
    "\n",
    "**VIDEO: Classification Models**\n",
    "\n",
    "Deep learning works similar for predicting outcomes from set of discrete options (classification). There are a few main differences.\n",
    "\n",
    "**First: Set loss function to **'categorical_crossentropy'** instead of mean squared error. This isn't the only loss funciton possible for classification, but it is by far the most common.**\n",
    "- This loss function is similar to log loss (lower is better)\n",
    "- However this score is still hard to interperet. So we **add 'metrics=['accuracy''** to compile step for easy-to-understand diagnostics. This print out accuracy score at the end of each epoch.\n",
    "  \n",
    "**Second: Modify last (Output) layer has separate node for each possible outcome and uses 'softmax' activation.**\n",
    "- Softmax activation function ensures predictions sum to one so they can be intepreted as probabilities.\n",
    "\n",
    "Here is the code for buidling this classifier model:\n",
    "1. **from keras.utils import to_categorical:** First we import utility function to turn a single categorical column into multiple 0 or 1 columns (so each new column represents a possible categorical outcome... we've done this before... I think it is the same as pd.to_categorical or something like that. This is called onehot encoding I beleive\n",
    "2. then read in data. use drop command without target columns, and store it as a numpy matrix. Set target column to caterogical columns!\n",
    "3. then build model! This looks simliar to the model we've built before, except for that **the last layer has TWO NODES for the two possible outcomes, and it uses the softmax function**\n",
    "\n",
    "![](classification.png)\n",
    "\n",
    "As we can see from results below, both accuracy and loss improve measurable for first three epochs. We will soon see a more effective way of determining how many epochs or how long to train a model, but training for ten epochs seemed to work well in this case!\n",
    "\n",
    "![](results1.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding your classification data\n",
    "\n",
    "Now you will start modeling with a new dataset for a classification problem. This data includes information about passengers on the Titanic. You will use predictors such as age, fare and where each passenger embarked from to predict who will survive. This data is from a tutorial on data science competitions. Look here for descriptions of the features.\n",
    "\n",
    "The data is pre-loaded in a pandas DataFrame called df.\n",
    "\n",
    "It's smart to review the maximum and minimum values of each variable to ensure the data isn't misformatted or corrupted. What was the maximum age of passengers on the Titanic? Use the .describe() method in the IPython Shell to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>age_was_missing</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age  sibsp  parch     fare  male age_was_missing  \\\n",
       "0         0       3  22.0      1      0   7.2500     1           False   \n",
       "1         1       1  38.0      1      0  71.2833     0           False   \n",
       "2         1       3  26.0      0      0   7.9250     0           False   \n",
       "3         1       1  35.0      1      0  53.1000     0           False   \n",
       "4         0       3  35.0      0      0   8.0500     1           False   \n",
       "\n",
       "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "0                        0                         0   \n",
       "1                        1                         0   \n",
       "2                        0                         0   \n",
       "3                        0                         0   \n",
       "4                        0                         0   \n",
       "\n",
       "   embarked_from_southampton  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last steps in classification models\n",
    "\n",
    "You'll now create a classification model using the titanic dataset, which has been pre-loaded into a DataFrame called df. You'll take information about the passengers and predict which ones survived.\n",
    "\n",
    "The predictive variables are stored in a NumPy array predictors. The target to predict is in df.survived, though you'll have to manipulate it for keras. The number of predictive features is stored in n_cols.\n",
    "\n",
    "Here, you'll use the 'sgd' optimizer, which stands for Stochastic Gradient Descent. You'll learn more about this in the next chapter!\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Convert df.survived to a categorical variable using the to_categorical() function.\n",
    "- Specify a Sequential model called model.\n",
    "- Add a Dense layer with 32 nodes. Use 'relu' as the activation and (n_cols,) as the input_shape.\n",
    "- Add the Dense output layer. Because there are two outcomes, it should have 2 units, and because it is a classification model, the activation should be 'softmax'.\n",
    "- Compile the model, using 'sgd' as the optimizer, 'categorical_crossentropy' as the loss function, and metrics=['accuracy'] to see the accuracy (what fraction of predictions were correct) at the end of each epoch.\n",
    "- Fit the model using the predictors and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data from DataFrame to matrix (predictors).\n",
    "# we don't need to convert target to a matrix, as the to_categorical function will do that for us\n",
    "predictor_cols = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'male',\n",
    "       'age_was_missing', 'embarked_from_cherbourg',\n",
    "       'embarked_from_queenstown', 'embarked_from_southampton']\n",
    "\n",
    "predictors = df[predictor_cols].as_matrix()\n",
    "\n",
    "# get number of input nodes\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 2.3033 - acc: 0.5903      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 1.2271 - acc: 0.6431     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 0.8583 - acc: 0.6364     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.6840 - acc: 0.6588     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.6717 - acc: 0.6622     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.6341 - acc: 0.6768     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.5933 - acc: 0.7026     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.5970 - acc: 0.6902     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.6171 - acc: 0.7003     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.6098 - acc: 0.6813     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16dc7668>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "target = to_categorical(df.survived)\n",
    "\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Models\n",
    "\n",
    "The things you'll want to do after you've used these models is:\n",
    "- save a model after training\n",
    "- reload model\n",
    "- make predictions with the model\n",
    "\n",
    "Models are saved in format called 'HTF5', for which h5 is the common extension. \n",
    "\n",
    "Use **my_model.summary()** to get a summary of the models architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "The trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as pred_data. Use model to make predictions on your new data.\n",
    "\n",
    "In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Create your predictions using the model's .predict() method on pred_data.\n",
    "- Use NumPy indexing to find the column corresponding to predicted probabilities of survival being True. This is the second column (index 1) of predictions. Store the result in predicted_prob_true and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data to make predictions on\n",
    "pred_data = pd.read_csv('predictor_data.csv').as_matrix()\n",
    "pred_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 2.3976 - acc: 0.5769     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 1.9229 - acc: 0.6016     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 0.9183 - acc: 0.6364     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.8106 - acc: 0.6330     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.7867 - acc: 0.6498     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.8597 - acc: 0.6285     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.7114 - acc: 0.6263     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.6034 - acc: 0.6880     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.6131 - acc: 0.6768     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.6113 - acc: 0.6599     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "[ 0.28163964  0.49404117  0.5404374   0.5046894   0.23845844  0.21868858\n",
      "  0.09941278  0.38632867  0.24095413  0.59939677  0.25841245  0.3723951\n",
      "  0.23943114  0.47980434  0.22570612  0.15710132  0.34001005  0.52052552\n",
      "  0.13143177  0.47368377  0.68139154  0.26085764  0.10387259  0.38486686\n",
      "  0.42879286  0.21093926  0.55512893  0.47828084  0.22053394  0.68594265\n",
      "  0.45981669  0.44799685  0.19420877  0.28293905  0.33606198  0.62766743\n",
      "  0.31255943  0.23327926  0.56369454  0.45967251  0.31040168  0.42223528\n",
      "  0.5162738   0.15567863  0.35783315  0.1475779   0.49717897  0.16680172\n",
      "  0.52839655  0.57451952  0.44569027  0.03612223  0.49634212  0.55651921\n",
      "  0.44741324  0.42299712  0.64230889  0.35427868  0.47878513  0.19420877\n",
      "  0.27054411  0.40901768  0.43635529  0.51632136  0.37422901  0.26649791\n",
      "  0.40734705  0.58266431  0.2521666   0.46039173  0.25852513  0.60253966\n",
      "  0.19150524  0.13446528  0.4615458   0.3656553   0.3407872   0.31906122\n",
      "  0.23132965  0.70179904  0.5265317   0.20600009  0.39238667  0.29728866\n",
      "  0.25513932  0.52349806  0.34506929  0.55553514  0.49230373  0.50227791\n",
      "  0.20282623]\n"
     ]
    }
   ],
   "source": [
    "# Specify, compile, and fit the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(predictors, target)\n",
    "\n",
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(pred_data)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color= red> MODULE 4: FINE-TUNING KERAS MODELS </font>\n",
    "\n",
    "# Understanding Model Optimization\n",
    "\n",
    "In practice, model optimization is super tough. The optimal value of any one weight, depends on the values of the other weights. \n",
    "Even if the slope tells us what weights to increase and what weights to decrease, it may not improve our model meaningfully. A small learning reate might cause updates that model doesnt improve materially, and a large learning rate might take us too far in the right direction. A smart optimization algorithm like 'Adam' helps, but problems can still occur. \n",
    "\n",
    "The easiest way to see the effect of difference learning rates is to use the simplest optimizer, **stochastic gradient descent**. This optimizer uses a fixed learning rate, usually around 0.01, but you can specify with the lr argument as seen below. \n",
    "We create models in a 'for loop', and each time around, we pass in the SDG optimizer using a different learning rate (low, medium, and high). Then you compare the results to see which is best!\n",
    "![](sgd.png)\n",
    "\n",
    "**The dying neuron problem**\n",
    "\n",
    "But even if the learning rate is well tuned, you can run into dying neuron problem. This problem occurs when a neuron takes the value of zero, for ALL rows of your data.\n",
    "\n",
    "Remember how in the Relu function any node with a negative value will have an ouput value of zero (see below), and it also has a slope of zero. **Because the slope of zero, the slope of any weights going into that node are ALSO zero, so those weights dont get updated.  **\n",
    "In other words:\n",
    "- Once the node starts getting negative inputs, it may continue ONLY getting negative inputs. \n",
    "- therefore, it will contribute nothing to the model, and hence the claim that the node or neuron is dead.\n",
    "\n",
    "**Alternatives?**\n",
    "\n",
    "At first, this problem suggests that it would be good to use an activation function whose slope is NEVER EXACTLY ZERO. However those types of functions were indeed used for MANY years. Using the 'tanh' activation function for instance, values that were outside the middle of the 'S' were relatively flat, and had small slopes. In a deep network, the repeated multiplicaiton of small slopes causes the slopes to get close to zero which means updates in backpropogation were close to zero. **This is known the 'Vanishing Gradient Problem'.**. Not a huge issue to think about right now, but its go to know, as you may need to change the activation function of your model at some point. \n",
    "\n",
    "\n",
    "\n",
    "![](dyingneuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing optimization parameters\n",
    "\n",
    "It's time to get your hands dirty with optimization. You'll now try optimizing a model at a very low learning rate, a very high learning rate, and a \"just right\" learning rate.\" You'll want to look at the results after running this exercise, **remembering that a low value for the loss function is good.**\n",
    "\n",
    "For these exercises, we've pre-loaded the predictors and target values from your previous classification models (predicting who would survive on the Titanic). You'll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function get_new_model() that creates an unoptimized model to optimize.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import SGD from keras.optimizers.\n",
    "- Create a list of learning rates to try optimizing with called lr_to_test. The learning rates in it should be .000001, 0.01, and 1.\n",
    "- Using a for loop to iterate over lr_to_test:\n",
    "  - Use the get_new_model() function to build a new, unoptimized model.\n",
    "  - Create an optimizer called my_optimizer using the SGD() constructor with keyword argument lr=lr.\n",
    "  - Compile your model. Set the optimizer parameter to be the SGD object you created above, and because this is a classification problem, use 'categorical_crossentropy' for the loss parameter.\n",
    "  - Fit your model using the predictors and target.\n",
    "  \n",
    "<font color=red> Keep in mind here that you are using the data from above: the 'predictors' and 'target' data sets from the titanic analysis. You do not reload them below so you need to run the above code first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (n_cols,)\n",
    "def get_new_model(input_shape = input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu',input_shape = input_shape))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 4.1443     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 4.1010     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 4.0564     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 4.0107     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 3.9644     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 3.9178     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 3.8712     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 3.8245     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 3.7774     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 3.7305     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 1.4885     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 0.8785     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 0.7218     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.6407     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.6063     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.6173     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.5962     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.6096     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.5830     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.6130     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 9.5625     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 9.9314      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "# Import the SGD optimizer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [.000001, 0.01, 1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model()\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(predictors, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neat!!! The model with the middle learning rate of 0.01 produced the lowest loss function values. That makes sense.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "Recall that your models performance on training data is not a good indication on how it will perform on NEW data. This is why we use 'validation data', which is data that is explicitly held out from training and used only to test model performance.\n",
    "\n",
    "Kfold cross validation is an example of this. **But in practice, few people run kfold cross validation on deep learning models because deep learning is typically used on large data sets, so the computational expense of using kfolds on a neural net model would be large. ** So we usually use a single validation score for these models, because these validaiton runs are reasonably large. \n",
    "\n",
    "Keras makes it easy to use some of your data as validation data, and we see that it the code below where we specify the 'validation_split' when calling the fit method. The 0.3 refers to the fraction of the date we want to use for validation.  \n",
    "\n",
    "\n",
    "*In the example below we have a classificaiton model, so we include measure of accuracy in the 'compile' method.*\n",
    "![](Picture1.png)\n",
    "\n",
    "**Early Stopping:**\n",
    "\n",
    "Our goal is to have the best validaiton score possible, so we should keep training while validation score is improving, and stop when it isnt improving. We do this with something called **early stopping**. \n",
    "\n",
    "The 'patience' arg in the EarlyStopping function, which we import from keras.callbacks, tells the model how many epochs the model can go without improving, before we stop training! usually 2 or 3 are good. The EarlyStopping is implemented as shown in the code below.\n",
    "\n",
    "**Notice in the code below we also add 'epochs=20'. Model default is for 10 epochs, but since we have the early stopping in the model now, we can set the epochs to much higher, because it will simply stop when the model is no longer improving.**\n",
    "\n",
    "![](Picture2.png)\n",
    "\n",
    "Let's look at the outputs below! We see that after the 9th epoch, the val_loss score does not improve for 2 epochs... ie, the 10th and 11th epochs produce a higher val_loss score, so the model stops training. Note that the loss score still improves however, but because the val_loss score does not, training stops!\n",
    "**Remember that val_loss is the loss from training on the 30% of data that we specified above... at least I think thats right.... lol**\n",
    "\n",
    "![](Picture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model accuracy on validation dataset\n",
    "\n",
    "Now it's your turn to monitor model accuracy with a validation data set. A model definition has been provided as model. Your job is to add the code to compile it and then fit it. You'll check the validation score in each epoch.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Compile your model using 'adam' as the optimizer and 'categorical_crossentropy' for the loss. To see what fraction of predictions are correct (the accuracy) in each epoch, specify the additional keyword argument metrics=['accuracy'] in model.compile().\n",
    "- Fit the model using the predictors and target. Create a validation split of 30% (or 0.3). This will be reported in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/10\n",
      "623/623 [==============================] - 0s - loss: 1.4220 - acc: 0.5987 - val_loss: 0.8268 - val_acc: 0.6567\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "623/623 [==============================] - 0s - loss: 0.8807 - acc: 0.5971 - val_loss: 0.5460 - val_acc: 0.7127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "623/623 [==============================] - 0s - loss: 0.6352 - acc: 0.6581 - val_loss: 0.6323 - val_acc: 0.6903\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "623/623 [==============================] - 0s - loss: 0.6423 - acc: 0.6629 - val_loss: 0.5778 - val_acc: 0.7127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "623/623 [==============================] - 0s - loss: 0.5993 - acc: 0.6918 - val_loss: 0.5673 - val_acc: 0.7090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "623/623 [==============================] - 0s - loss: 0.7231 - acc: 0.6340 - val_loss: 0.5481 - val_acc: 0.7090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "623/623 [==============================] - 0s - loss: 0.6255 - acc: 0.7255 - val_loss: 0.5312 - val_acc: 0.7313\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "623/623 [==============================] - 0s - loss: 0.5912 - acc: 0.7047 - val_loss: 0.6084 - val_acc: 0.6978\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "623/623 [==============================] - 0s - loss: 0.5887 - acc: 0.6902 - val_loss: 0.4816 - val_acc: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "623/623 [==============================] - 0s - loss: 0.5769 - acc: 0.7223 - val_loss: 0.5019 - val_acc: 0.7724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(predictors, target, validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping: Optimizing the optimization\n",
    "\n",
    "Now that you know how to monitor your model performance throughout optimization, you can use early stopping to stop optimization when it isn't helping any more. Since the optimization stops automatically when it isn't helping, you can also set a high value for epochs in your call to .fit(), as Dan showed in the video.\n",
    "\n",
    "The model you'll optimize has been specified as model. As before, the data is pre-loaded as predictors and target.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Import EarlyStopping from keras.callbacks.\n",
    "- Compile the model, once again using 'adam' as the optimizer, 'categorical_crossentropy' as the loss function, and metrics=['accuracy'] to see the accuracy at each epoch.\n",
    "- Create an EarlyStopping object called early_stopping_monitor. Stop optimization when the validation loss hasn't improved for 2 epochs by specifying the patience parameter of EarlyStopping() to be 2.\n",
    "- Fit the model using the predictors and target. Specify the number of epochs to be 30 and use a validation split of 0.3. In addition, pass [early_stopping_monitor] to the callbacks parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/30\n",
      "623/623 [==============================] - 0s - loss: 1.0888 - acc: 0.6244 - val_loss: 0.7931 - val_acc: 0.6567\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/30\n",
      "623/623 [==============================] - 0s - loss: 0.6747 - acc: 0.6485 - val_loss: 0.5556 - val_acc: 0.7537\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/30\n",
      "623/623 [==============================] - 0s - loss: 0.6219 - acc: 0.6870 - val_loss: 0.6058 - val_acc: 0.7164\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/30\n",
      "623/623 [==============================] - 0s - loss: 0.6142 - acc: 0.6774 - val_loss: 0.6814 - val_acc: 0.6493\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/30\n",
      "623/623 [==============================] - 0s - loss: 0.5888 - acc: 0.6918 - val_loss: 0.5195 - val_acc: 0.7649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/30\n",
      "623/623 [==============================] - 0s - loss: 0.6407 - acc: 0.6982 - val_loss: 0.4909 - val_acc: 0.7612\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/30\n",
      "623/623 [==============================] - 0s - loss: 0.5963 - acc: 0.6822 - val_loss: 0.5088 - val_acc: 0.7575\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/30\n",
      "623/623 [==============================] - 0s - loss: 0.6082 - acc: 0.7111 - val_loss: 0.5102 - val_acc: 0.7164\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/30\n",
      "623/623 [==============================] - 0s - loss: 0.5558 - acc: 0.7352 - val_loss: 0.4892 - val_acc: 0.7799\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/30\n",
      "623/623 [==============================] - 0s - loss: 0.5320 - acc: 0.7303 - val_loss: 0.5066 - val_acc: 0.7836\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 11/30\n",
      "623/623 [==============================] - 0s - loss: 0.5349 - acc: 0.7544 - val_loss: 0.5242 - val_acc: 0.7612\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 12/30\n",
      "623/623 [==============================] - 0s - loss: 0.5564 - acc: 0.7496 - val_loss: 0.6094 - val_acc: 0.7164\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "# Import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model, SET THE CALLBACKS ARG AS EARLY STOPPING AND SET EPOCHS TO 30\n",
    "model = model.fit(predictors, target, epochs=30, validation_split=0.30, callbacks=[early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCJRwhARkUiUMEREJJL/B5oJQQViWnkEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c9b5eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the plot\n",
    "plt.plot(model.history['val_loss'], 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
