{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue size=5><b> 2. Simple Topic Identification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green size=3><b> 2.1 Bag of Words </font>\n",
    "\n",
    "- Basic method for finding topics in text\n",
    "- Need to first tokenize and then count the tokens\n",
    "- Basic theory is that the more frequent a word, the more important it might be\n",
    "- Can be a great way to determine the significant word sin a text\n",
    "\n",
    "**TASKS: Building a Counter with bag-of-words**\n",
    "\n",
    "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
    "\n",
    "- Import Counter from collections.\n",
    "- Use word_tokenize() to split the article into tokens.\n",
    "- Use a list comprehension with t as the iterator variable to convert all the tokens into lowercase. The .lower() method converts text into - lowercase.\n",
    "- Create a bag-of-words counter called bow_simple by using Counter() with lower_tokens as the argument.\n",
    "- Use the .most_common() method of bow_simple to print the 10 most common tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\'\\\\'\\\\'Debugging\\\\'\\\\'\\\\' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \\\\n\\\\nNumerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statist\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"wiki_article.txt\") as file:\n",
    "    article = file.read()\n",
    "    \n",
    "article[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\\\\'\\\\'\\\\'Debugging\\\\'\\\\'\\\\\",\n",
       " \"'\",\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'finding',\n",
       " 'and',\n",
       " 'resolving',\n",
       " 'of']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\\\\'\\\\'\\\\'debugging\\\\'\\\\'\\\\\",\n",
       " \"'\",\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'finding',\n",
       " 'and',\n",
       " 'resolving',\n",
       " 'of']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "lower_tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 147), ('of', 81), ('.', 70), ('to', 61), ('a', 59), (\"''\", 42), ('and', 41), ('in', 41), ('(', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green size=3><b> 2.2 Text Preprocessing </font>\n",
    "\n",
    "**TASKS: Text Preprocessing Practice**\n",
    "\n",
    "- Import the WordNetLemmatizer class from nltk.stem.\n",
    "- Create a list called alpha_only that iterates through lower_tokens and retains only alphabetical characters. You can use the .isalpha() method to check for this.\n",
    "- Create another list called no_stops in which you remove all stop words, which are held in a list called english_stops.\n",
    "- Initialize a WordNetLemmatizer object called wordnet_lemmatizer and use its .lemmatize() method on the tokens in no_stops to create a new list called lemmatized.\n",
    "- Finally, create a new Counter called bow with the lemmatized words and show the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#import enlgish_stops data\n",
    "with open('english_stops.txt') as f:\n",
    "    english_stops = f.read()\n",
    "\n",
    "#clean english_stops file\n",
    "english_stops = re.sub(r'\\n|\\'|\\s',\"\",english_stops)\n",
    "english_stops=english_stops.split(',')\n",
    "english_stops[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'the', 'process', 'of', 'finding', 'and', 'resolving', 'of']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "alpha_only[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process',\n",
       " 'finding',\n",
       " 'resolving',\n",
       " 'defects',\n",
       " 'prevent',\n",
       " 'correct',\n",
       " 'operation',\n",
       " 'computer',\n",
       " 'software',\n",
       " 'system',\n",
       " 'books',\n",
       " 'written',\n",
       " 'debugging',\n",
       " 'see',\n",
       " 'reading',\n",
       " 'involves',\n",
       " 'numerous',\n",
       " 'aspects',\n",
       " 'including',\n",
       " 'interactive']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops ]\n",
    "no_stops[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process',\n",
       " 'finding',\n",
       " 'resolving',\n",
       " 'defect',\n",
       " 'prevent',\n",
       " 'correct',\n",
       " 'operation',\n",
       " 'computer',\n",
       " 'software',\n",
       " 'system',\n",
       " 'book',\n",
       " 'written',\n",
       " 'debugging',\n",
       " 'see',\n",
       " 'reading',\n",
       " 'involves',\n",
       " 'numerous',\n",
       " 'aspect',\n",
       " 'including',\n",
       " 'interactive']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "lemmatized[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 30), ('system', 23), ('software', 16), ('computer', 14), ('bug', 14), ('problem', 14), ('term', 13), ('tool', 13), ('process', 12), ('used', 12)]\n"
     ]
    }
   ],
   "source": [
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green size=3><b> 2.3 gensim </font>\n",
    "\n",
    "**What is a word vector?**\n",
    "\n",
    "Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.\n",
    "\n",
    "In the graphic below, we can see that the vector operations King minus Queen, is approximately equal to man minus woman. The deep learning algo used to create word vectors has been able to distill this meaning based on how these words are used throughout the text. \n",
    "![](pictures/wordvec.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'movie', 'was', 'about', 'a', 'spaceship', 'and', 'aliens', '.'],\n",
       " ['i', 'really', 'liked', 'the', 'movie', '!'],\n",
       " ['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.'],\n",
       " ['the', 'movie', 'was', 'awful', '!', 'i', 'hate', 'alien', 'films', '.'],\n",
       " ['space', 'is', 'cool', '!', 'i', 'liked', 'the', 'movie', '.'],\n",
       " ['more', 'space', 'films', ',', 'please', '!']]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "                'I really liked the movie!',\n",
    "                'Awesome action scenes, but boring characters.',\n",
    "                'The movie was awful! I hate alien films.',\n",
    "                'Space is cool! I liked the movie.',\n",
    "                'More space films, please!']\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc.lower())for doc in my_documents]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 12,\n",
       " ',': 16,\n",
       " '.': 8,\n",
       " 'a': 4,\n",
       " 'about': 3,\n",
       " 'action': 14,\n",
       " 'alien': 22,\n",
       " 'aliens': 7,\n",
       " 'and': 6,\n",
       " 'awesome': 13,\n",
       " 'awful': 20,\n",
       " 'boring': 18,\n",
       " 'but': 17,\n",
       " 'characters': 19,\n",
       " 'cool': 26,\n",
       " 'films': 23,\n",
       " 'hate': 21,\n",
       " 'i': 9,\n",
       " 'is': 25,\n",
       " 'liked': 11,\n",
       " 'more': 27,\n",
       " 'movie': 1,\n",
       " 'please': 28,\n",
       " 'really': 10,\n",
       " 'scenes': 15,\n",
       " 'space': 24,\n",
       " 'spaceship': 5,\n",
       " 'the': 0,\n",
       " 'was': 2}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASKS: Creating and querying a corpus with gensim**\n",
    "\n",
    "- Import Dictionary from gensim.corpora.dictionary.\n",
    "- Initialize a gensim Dictionary with the tokens in articles.\n",
    "- Obtain the id for \"computer\" from dictionary. To do this, use its .token2id method which returns ids from text, and then chain .get() which returns tokens from ids. Pass in \"computer\" as an argument to .get().\n",
    "- Use a list comprehension in which you iterate over articles to create a gensim MmCorpus from dictionary.\n",
    "- In the output expression, use the .doc2bow() method on dictionary with article as the argument.\n",
    "- Print the first 10 word ids with their frequency counts from the fifth document. This has been done for you, so hit 'Submit Answer' to see the results!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recreate 'articles' variable from datacamp\n",
    "with open('articles.txt') as f:\n",
    "    articles = f.read()\n",
    "    \n",
    "#clean file\n",
    "articles = re.sub(r'\\n|\\'|\\s|\\[|\\]',\"\",articles)\n",
    "articles=articles.split(',')\n",
    "num_words = round(len(articles)/12)\n",
    "\n",
    "#recreate list (wont be exact, because we cant get the full list easily.. but we will have 12 documents!!!)\n",
    "words = len(articles)\n",
    "num_list = list(range(0,12))\n",
    "articles_list = [articles[num_words*k:num_words*(k+1)] for k in num_list]\n",
    "len(articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x2a5375f1978>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles_list)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n"
     ]
    }
   ],
   "source": [
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 9), (3, 9), (5, 12), (9, 1), (10, 3), (14, 1), (22, 4), (26, 1), (28, 36)]\n"
     ]
    }
   ],
   "source": [
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles_list]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASKS: Gensim bag-of-words**\n",
    "\n",
    "Now, you'll use your new gensim corpus and dictionary to see **the most common terms per document and across all documents.** You can use your dictionary to look up the terms. Take a guess at what the topics are!\n",
    "\n",
    "- Print the top five words of bow_doc using each word_id with the dictionary alongside word_count. The word_id can be accessed using the .get() method of dictionary.\n",
    "- Create a defaultdict called total_word_count in which the keys are all the token ids (word_id) and the values are the sum of their occurrence across all documents (word_count). Remember to specify int when creating the defaultdict, and inside the for loop, increment each word_id of total_word_count by word_count.\n",
    "- Create a sorted list from the defaultdict, using words across the entire corpus. To achieve this, use the .items() method on total_word_count inside sorted().\n",
    "- Similar to how you printed the top five words of bow_doc earlier, print the top five words of sorted_word_count as well as the number of occurrences of each word across all the documents.\n",
    "\n",
    "**HINT**\n",
    "- To print the word_id inside the for loop, pass it into dictionary.get(), such that  dictionary.get(word_id) is the first argument of print().\n",
    "- Use defaultdict(int) to create total_word_count, and be sure you're correctly incrementing total_word_count[word_id] by word_count.\n",
    "- Use the .items() method on total_word_count as the first argument to sorted(), to ensure that words across the entire corpus are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(28, 36), (40, 23), (1568, 16), (2132, 14), (5, 12)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True) #sorting for second element of each tuple (which is the frequency!)\n",
    "bow_doc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\" 36\n",
      "engineering 23\n",
      "reverse 16\n",
      "medal 14\n",
      "software 12\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\n",
      "engineering\n",
      "reverse\n",
      "medal\n",
      "software\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.get(28))\n",
    "print(dictionary.get(40))\n",
    "print(dictionary.get(1568))\n",
    "print(dictionary.get(2132))\n",
    "print(dictionary.get(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green size=3><b> 2.4 Tf-idf with Genism </font>\n",
    "\n",
    "**Term Frequency - inverse document frequency**\n",
    "\n",
    "- Allows you determine the msot important words in each document\n",
    "- each corpus may have shared words beyond just stopwords\n",
    "- these words should be down-weighted in importance\n",
    "- example from astronomy: \"sky\"\n",
    "- Ensure most common words don't show up as key words\n",
    "- keeps document specific frequent words are weighted high\n",
    "\n",
    "Tf-idf formula:\n",
    "\n",
    "![](pictures/tfids.jpg)\n",
    "\n",
    "- The weight will be LOW if the term doesnt appear very often in the document\n",
    "- **the weight will ALSO be low if the low if the internal equatio is low (N/df_i is very high, because as the the log of 1 is zero!!!). This effectively penalizes words that are common across ALL documents. **\n",
    "\n",
    "**TASKS: Tf-idf with Wikipedia**\n",
    "\n",
    "- Import TfidfModel from gensim.models.tfidfmodel.\n",
    "- Initialize a new TfidfModel called tfidf using corpus.\n",
    "- Use doc to calculate the weights. You can do this by passing [doc] to tfidf.\n",
    "- Print the first five term ids with weights.\n",
    "- Sort the term ids and weights in a new list from highest to lowest weight. This has been done for you.\n",
    "- Print the top five weighted words (term_id) from sorted_tfidf_weights along with their weighted score (weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.008517407350738034), (1, 0.009622778920251195), (3, 0.009622778920251195), (5, 0.02688445418094919), (9, 0.004982363903548477)]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfModel\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medal 0.3082405004703436\n",
      "reverse 0.2725570352236171\n",
      "3d 0.24427668764610064\n",
      "ribbon.svg|border|22px 0.21374210169033805\n",
      "engineering 0.19590036906697475\n"
     ]
    }
   ],
   "source": [
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
