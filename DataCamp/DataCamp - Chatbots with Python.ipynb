{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Chatbots!\n",
    "- Part 1: Rule based chatbots\n",
    "- Part 2: Understanding natural language\n",
    "- Part 3: Building a virtual assistant\n",
    "- Part 4: Dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green size=5> <b>1. Basics for rule-based CBs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Set basic send_message function and rules for responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set basic send_message function\n",
    "bot_template = \"BOT : {0}\"\n",
    "user_template = \"USER : {0}\"\n",
    "\n",
    "# Define a function that sends a message to the bot: send_message\n",
    "def send_message(message):\n",
    "    # Print user_template including the user_message\n",
    "    print(user_template.format(message))\n",
    "    # Get the bot's response to the message\n",
    "    response = respond(message)\n",
    "    # Print the bot template including the bot's response.\n",
    "    time.sleep(1) #add response delay for authenticity!!!\n",
    "    print(bot_template.format(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Substituting Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your last birthday\n",
      "when me went to florida\n",
      "i had your own castle\n"
     ]
    }
   ],
   "source": [
    "# Define replace_pronouns()\n",
    "def replace_pronouns(message):\n",
    "\n",
    "    message = message.lower()\n",
    "    if 'me' in message:\n",
    "        # Replace 'me' with 'you'\n",
    "        return re.sub('me', 'you', message)\n",
    "    if 'my' in message:\n",
    "        # Replace 'my' with 'your'\n",
    "        return re.sub('my', 'your', message)\n",
    "    if 'your' in message:\n",
    "        # Replace 'your' with 'my'\n",
    "        return re.sub('your', 'my', message)\n",
    "    if 'you' in message:\n",
    "        # Replace 'you' with 'me'\n",
    "        return re.sub('you', 'me', message)\n",
    "\n",
    "    return message\n",
    "\n",
    "print(replace_pronouns(\"my last birthday\"))\n",
    "print(replace_pronouns(\"when you went to Florida\"))\n",
    "print(replace_pronouns(\"I had my own castle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.3 Basic intent matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : hello!\n",
      "BOT : Hello you! :)\n",
      "USER : bye byeee\n",
      "BOT : goodbye for now\n",
      "USER : thanks very much!\n",
      "BOT : you are very welcome\n"
     ]
    }
   ],
   "source": [
    "#Set base variables for responses!!!\n",
    "responses = {'default': 'default message',\n",
    " 'goodbye': 'goodbye for now',\n",
    " 'greet': 'Hello you! :)',\n",
    " 'thankyou': 'you are very welcome'}\n",
    "\n",
    "patterns = {'goodbye': re.compile(r'bye|farewell', re.UNICODE),\n",
    " 'greet': re.compile(r'hello|hi|hey', re.UNICODE),\n",
    " 'thankyou': re.compile(r'thank|thx', re.UNICODE)}\n",
    "\n",
    "# Define a function to find the intent of a message\n",
    "def match_intent(message):\n",
    "    matched_intent = None\n",
    "    for intent, pattern in patterns.items():\n",
    "        # Check if the pattern occurs in the message \n",
    "        if pattern.search(message):\n",
    "            matched_intent = intent\n",
    "    return matched_intent\n",
    "\n",
    "# Define a respond function\n",
    "def respond(message):\n",
    "    # Call the match_intent function\n",
    "    intent = match_intent(message)\n",
    "    # Fall back to the default response\n",
    "    key = \"default\"\n",
    "    if intent in responses:\n",
    "        key = intent\n",
    "    return responses[key]\n",
    "\n",
    "# Send messages\n",
    "send_message(\"hello!\")\n",
    "send_message(\"bye byeee\")\n",
    "send_message(\"thanks very much!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Basic name identificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : my name is Hogward The Intruder\n",
      "BOT : Hello, Hogward The Intruder!\n"
     ]
    }
   ],
   "source": [
    "# Define find_name()\n",
    "def find_name(message):\n",
    "    name = None\n",
    "    # Create a pattern for checking if the keywords occur\n",
    "    name_keyword = re.compile('name|call')\n",
    "    # Create a pattern for finding capitalized words\n",
    "    name_pattern = re.compile('[A-Z]{1}[a-z]*')\n",
    "    if name_keyword.search(message):\n",
    "        # Get the matching words in the string\n",
    "        name_words = name_pattern.findall(message)\n",
    "        if len(name_words) > 0:\n",
    "            # Return the name if the keywords are present\n",
    "            name = ' '.join(name_words)\n",
    "    return name\n",
    "\n",
    "# Define respond()\n",
    "def respond(message):\n",
    "    # Find the name\n",
    "    name = find_name(message)\n",
    "    if name is None:\n",
    "        return \"Hi there!\"\n",
    "    else:\n",
    "        return \"Hello, {0}!\".format(name)\n",
    "\n",
    "# Send messages\n",
    "send_message(\"my name is Hogward The Intruder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green size=5> <b>2. Understanding Natural Language </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Word vectors\n",
    "- Word vectors assign to each word, a vector that describes its meaning\n",
    "- words that appear in similar contexts often will have similar word vectors\n",
    "- If you create these vectors which have BILLIONs of words, you create vectors that capture a ton of implicit meaning\n",
    "- training word vectors can take a lot of computing power and a lot of data\n",
    "- Fortunately, we can get pre-trained vecotrs using spacy, which is has vectors trained using GloVe (a cousin of word2vec)\n",
    "- Word vectors tend to have a length of a few hundrd elements, check this with nlp.vocab.vectors_length attribute of a spacey object (see below)\n",
    "\n",
    "#### Cosine Similarity:\n",
    "- In word vector space, it is the **DIRECTION** of the word vectors that matters most. \n",
    "- the \"distance\" between words = angle between the vectors\n",
    "- Measure of angle is **Cosine simlarity:**\n",
    "    - 1 if vectors point in the same direction\n",
    "    - 0 if they are perpendicular\n",
    "    - -1 if they point in the opposite direciton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple word vector example\n",
    "nlp = spacy.load('en') #create spacy object (loads default english model)\n",
    "doc = nlp('hello, can you help me?') #produces iterator over tokens in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello : [ 0.25233001  0.10176    -0.67484999]\n",
      ", : [-0.082752    0.67203999 -0.14986999]\n",
      "can : [-0.23857     0.35457    -0.30219001]\n",
      "you : [-0.11076     0.30785999 -0.51980001]\n",
      "help : [-0.29370001  0.32253    -0.44779   ]\n",
      "me : [-0.15396     0.31894001 -0.54887998]\n",
      "? : [-0.086864    0.19160999  0.10915   ]\n"
     ]
    }
   ],
   "source": [
    "#each vector word has its own vector\n",
    "doc_total = np.ones(300,)\n",
    "for token in doc:\n",
    "    print(\"{} : {}\".format(token, token.vector[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat-Dog 0.801685470553\n",
      "Cat-fish 0.418065391278\n",
      "Cat-table 0.285514238043\n",
      "man-woman 0.74017436681\n",
      "woman-oil 0.294893511308\n"
     ]
    }
   ],
   "source": [
    "#cosine similarity\n",
    "doc_1 = nlp(\"cat\")\n",
    "doc_2 = nlp(\"dog\")\n",
    "doc_3 = nlp(\"fish\")\n",
    "doc_4 = nlp(\"table\")\n",
    "doc_5 = nlp(\"man\")\n",
    "doc_6 = nlp(\"woman\")\n",
    "doc_7 = nlp(\"fart\")\n",
    "\n",
    "print(\"Cat-Dog\", doc_1.similarity(doc_2))\n",
    "print(\"Cat-fish\",doc_1.similarity(doc_3))\n",
    "print(\"Cat-table\",doc_1.similarity(doc_4))\n",
    "print(\"man-woman\",doc_5.similarity(doc_6))\n",
    "print(\"woman-oil\",doc_6.similarity(doc_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15067001, -0.024468  , -0.23367999, -0.23378   , -0.18381999], dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"cat\").vector[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27621502,  0.173051  , -0.1061995 , -0.28751498, -0.067141  ], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"cat dog\").vector[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> To get the vector of a string of words, you take the average of the vectors of all the words in the sentence.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27621502,  0.173051  , -0.1061995 , -0.28751498, -0.067141  ], dtype=float32)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nlp(\"cat\").vector[0:5] + nlp(\"dog\").vector[0:5])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Intents and classification\n",
    "\n",
    "- Now that we know how to use spacy to create vector respresentations of words and documents, we can use machine learning algorithms to identify intents.\n",
    "- Intent recognition is a classification problem. \n",
    "- Support vector classifiers work very well for classifying intents\n",
    "\n",
    "**Here we will compare the accuracy of the SVC and KNN models in prediction intention:**\n",
    "\n",
    "- <font color=red> Data is saved in the module 'datacamp_datasets.pt' in C:\\Users\\mciniello\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_env.  \n",
    "- A copy is also saved in C:\\Users\\mciniello\\Desktop\\Python\\Updated projects\\Clean\\data. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacamp_datasets4 import X_train, y_train, X_test, y_test\n",
    "X_train = X_train()\n",
    "y_train = y_train()\n",
    "X_test = X_test()\n",
    "y_test = y_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1209, 300)\n",
      "(1209,)\n",
      "(201, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 162 correctly out of 201 test examples\n"
     ]
    }
   ],
   "source": [
    "# Import SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a support vector classifier\n",
    "clf = SVC(C=1)\n",
    "\n",
    "# Fit the classifier using the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Count the number of correct predictions\n",
    "n_correct = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        n_correct += 1\n",
    "\n",
    "print(\"Predicted {0} correctly out of {1} test examples\".format(n_correct, len(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Entity Extraction\n",
    "- It can be quite difficult to recognize entities that the training data has not yet seen\n",
    "- To generalize, we can look at how a word is spelled, which words come before or after it\n",
    "\n",
    "**spaCy pre-built Named Entity Recognition models**\n",
    "- use thesse pre-trained models (trained with A LOT of training data) to do NER on your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:  Mary \tLABEL:  PERSON\n",
      "TEXT:  Google \tLABEL:  ORG\n",
      "TEXT:  2009 \tLABEL:  DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"my friend Mary has worked at Google since 2009\")\n",
    "for ent in doc.ents:\n",
    "    print('TEXT: ', ent.text, '\\tLABEL: ',ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependency Parsing:**\n",
    "\n",
    "Entities can have different roles. For example:\n",
    "- I want a flight from TelAviv to Toronto\n",
    "\n",
    "So though these are both entities, one is the origin and one is a detination, and that distinction is VERY IMPORTANT. This is where dependency parsing comes in handy!\n",
    "\n",
    "This is too big a topic to cover here (cover it yourself later!) but you can easily generate them with spacy. Heres a basic overview:\n",
    "- parse  tree is a hierarchical structure that specifies parent-child relationships between the words in a phras\n",
    "- it is **INDEPENDENT OF WORD ORDER**\n",
    "\n",
    "**IN BOTH PHRASES:** \"I want a flight to Shanghai from Singapore\" AND \"I want a flight from Singapore to Shanghai\"\n",
    "- \"to\" is the parent of Shanghai\n",
    "- \"from\" is the parent of Singapore\n",
    "\n",
    "\n",
    "![](pictures/parsetree.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example1\n",
    "doc = nlp('a flight to Shanghai from Singapore')\n",
    "shanghai, singapore = doc[3], doc[5] #entities\n",
    "list(shanghai.ancestors) #get parents of shanghai entity using .attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using spaCy's entity recogniser**\n",
    "\n",
    "In this exercise you'll use spaCy's built-in entity recognizer to extract names, dates, and organizations from search queries. Your job is to define a function called extract_entities() which takes in a single argument message and returns a dictionary with the included entity types as keys, and the extracted entities as values. The included entity types are contained in a list called include_entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ayyy': None, 'sup': None}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict.fromkeys(['ayyy','sup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PERSON': 'Mary', 'DATE': '2010', 'ORG': 'Google'}\n",
      "{'PERSON': None, 'DATE': '1999', 'ORG': 'MIT'}\n",
      "{'PERSON': None, 'DATE': '1999', 'ORG': None}\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "############### EXTRACT ONLY ENTITIES YOU ARE INTERESTED IN ###############\n",
    "###########################################################################\n",
    "\n",
    "# Define included entities\n",
    "include_entities = ['DATE', 'ORG', 'PERSON']\n",
    "\n",
    "# Define extract_entities()\n",
    "def extract_entities(message):\n",
    "    # Create a dict to hold the entities\n",
    "    ents = dict.fromkeys(include_entities)\n",
    "    # Create a spacy document\n",
    "    doc = nlp(message)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in include_entities:\n",
    "            # Save interesting entities\n",
    "            ents[ent.label_] = ent.text\n",
    "    return ents\n",
    "\n",
    "print(extract_entities('friends called Mary who have worked at Google since 2010'))\n",
    "print(extract_entities('people who graduated from MIT in 1999'))\n",
    "print(extract_entities('people who graduated from university in Canada in 1999'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PERSON': 'Mary', 'ORG': 'Google', 'DATE': '2010'}\n",
      "{'ORG': 'MIT', 'DATE': '1999'}\n",
      "{'GPE': 'Canada', 'DATE': '1999'}\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "############### EXTRACT ALL ENTITIES ###############\n",
    "####################################################\n",
    "\n",
    "# Define extract_entities()\n",
    "def extract_entities(message):\n",
    "    ents = {}\n",
    "    # Create a spacy document\n",
    "    doc = nlp(message)\n",
    "    for ent in doc.ents:\n",
    "        # Save interesting entities\n",
    "        ents[ent.label_] = ent.text\n",
    "    return ents\n",
    "\n",
    "print(extract_entities('friends called Mary who have worked at Google since 2010'))\n",
    "print(extract_entities('people who graduated from MIT in 1999'))\n",
    "print(extract_entities('people who graduated from university in Canada in 1999'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assigning roles using spaCy's parser**\n",
    "\n",
    "In this exercise you'll use spaCy's powerful syntax parser to assign roles to the entities in your users' messages. To do this, you'll define two functions, find_parent_item() and assign_colors(). In doing so, you'll use a parse tree to assign roles, similar to how Alan did in the video.\n",
    "\n",
    "Recall that you can access the ancestors of a word using its .ancestors attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the document\n",
    "doc = nlp(\"let's see that jacket in red, some blue jeans, and a neon hoverboard\")\n",
    "\n",
    "colors = ['black','red','blue','grey','green', 'neon']\n",
    "items = ['shoes', 'handback', 'jacket', 'jeans','hoverboard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n",
      "color\n"
     ]
    }
   ],
   "source": [
    "#classify words as entity types or colurs\n",
    "def entity_type(word):\n",
    "    _type = None\n",
    "    if word.text in colors:\n",
    "        _type = \"color\"\n",
    "    elif word.text in items:\n",
    "        _type = \"item\"\n",
    "    return _type\n",
    "\n",
    "print(doc[6])\n",
    "print(entity_type(doc[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n",
      "Ancestors:  [in, jacket, see, let]\n",
      "jacket\n"
     ]
    }
   ],
   "source": [
    "# Iterate over parents in parse tree until an item entity is found\n",
    "def find_parent_item(word):\n",
    "    # Iterate over the word's ancestors\n",
    "    for parent in word.ancestors:\n",
    "        # Check for an \"item\" entity\n",
    "        if entity_type(parent) == \"item\":\n",
    "            return parent.text\n",
    "    return None\n",
    "\n",
    "print(doc[6])\n",
    "print(\"Ancestors: \",[x for x in doc[6].ancestors])\n",
    "print(find_parent_item(doc[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item: jacket has color : red\n",
      "item: jeans has color : blue\n",
      "item: hoverboard has color : neon\n"
     ]
    }
   ],
   "source": [
    "# For all color entities, find their parent item\n",
    "def assign_colors(doc):\n",
    "    # Iterate over the document\n",
    "    for word in doc:\n",
    "        # Check for \"color\" entities\n",
    "        if entity_type(word) == \"color\":\n",
    "            # Find the parent of the color\n",
    "            item = find_parent_item(word)\n",
    "            print(\"item: {0} has color : {1}\".format(item, word))\n",
    "\n",
    "# Assign the colors\n",
    "assign_colors(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust language understand with Rasu NLU\n",
    "\n",
    "- Rasa provides high level API for intent recognition and entity extraction\n",
    "- its based on spaCy, scikit-learn, and other libs\n",
    "- built in support for chatbot specific tasks\n",
    "\n",
    "**Using Rasa:**\n",
    "- provide trianing data in a json file (json format is based on key value pairs)\n",
    "- training data should have **a list of dictionaries called training examples**. Each list contains:\n",
    "    - example message\n",
    "    - message intent\n",
    "    - list of entites in message\n",
    "- you can convert one of these training examples to readable formats using the json.dumps function:\n",
    "\n",
    "        In [1]: from rasa_nlu.converters import load_data\n",
    "        In [2]: training_data = load_data(\"./training_data.json\")\n",
    "        In [3]: import json\n",
    "\n",
    "        In [4]: print(json.dumps(data.training_examples[22], indent=2))\n",
    "        Out[4]: {\n",
    "          \"text\": \"i'm looking for a place in the north of town\",\n",
    "          \"intent\": \"restaurant_search\",\n",
    "          \"entities\": [\n",
    "            {\n",
    "              \"start\": 31,\n",
    "              \"end\": 36,\n",
    "              \"value\": \"north\",\n",
    "              \"entity\": \"location\"}]}\n",
    "\n",
    "**Using rasa with Python:**\n",
    "- To use rasa nlu in python you use an interpreter object\n",
    "- this contains your trained models for intents and entities\n",
    "- to use it, pass a message through the interpreters parse function, like so:\n",
    "\n",
    "        In [1]: message = \"I want to book a flight to London\"\n",
    "        In [2]: interpreter.parse(message))\n",
    "        Out[2]: {\n",
    "          \"intent\": {\n",
    "            \"name\": \"flight_search\",\n",
    "            \"confidence\": 0.9\n",
    "          },\n",
    "          \"entities\": [\n",
    "            {\n",
    "              \"entity\": \"location\",\n",
    "              \"value\": \"London\",\n",
    "              \"start\": 27,\n",
    "              \"end\": 33}]}\n",
    "        \n",
    "**Create an interpreter:**\n",
    "\n",
    "        In [1]: from rasa_nlu.config import RasaNLUConfig\n",
    "\n",
    "        In [2]: from rasa_nlu.model import Trainer\n",
    "        In [3]: config = RasaNLUConfig(cmdline_args={\"pipeline\": \"spacy_sklearn\"})\n",
    "        In [4]: trainer = Trainer(config)\n",
    "        In [5]: interpreter = trainer.train(training_data)\n",
    "\n",
    "**Rasa pipeline (as seen in the RasaNLUconfig cmdline_args above):**\n",
    "- A rasa pipeline is a list of components that will be used to process text. The spacy_sklearn components uses both of thes packages to process test with the following steps:\n",
    "\n",
    "        In [1]: MIKES_PIPELINE = [\n",
    "          \"nlp_spacy\",\n",
    "          \"ner_crf\",\n",
    "          \"ner_synonyms\", \n",
    "          \"intent_featurizer_spacy\",\n",
    "          \"intent_classifier_sklearn\"]\n",
    "          \n",
    "    1. nlp_spacy: initializes the spacy english model ('en')\n",
    "    2. ner_crf: uses conditional random field model to extract entities (CDF is a ML model that is good for identifying Entities, even with small datasets). \n",
    "    3. ner_synonms: maps entities with the same meaning to the same key (like New York City and NYC)\n",
    "    4. intent_featurizer_spacy: creates vector representations of sentences (which takes average of individual spacy word vectors!)\n",
    "    5. intent_classifier_sklearn: scikit learn SCV!\n",
    "\n",
    "#### These two statements are identical:\n",
    "-When defining a RasaNLUconfi object, you can either pass a predifed pipeline (like spacy_sklearn), or define a list of components you want to use, as in below.\n",
    "\n",
    "        In [2]: RasaNLUConfig(cmdline_args={\"pipeline\": MIKES_PIPELINE})\n",
    "\n",
    "        In [3]: RasaNLUConfig(cmdline_args={\"pipeline\": \"spacy_sklearn\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasa NLU Exercise:\n",
    "In this exercise you'll use Rasa NLU to create an interpreter, which parses incoming user messages and returns a set of entities. **Your job is to train an interpreter using the MITIE entity recognition model in rasa NLU.**\n",
    "\n",
    "<font color = red> Code below wont work because I dont have access to the training data. But the outputs are there for reference</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Import necessary modules\n",
    "    from rasa_nlu.converters import load_data\n",
    "    from rasa_nlu.config import RasaNLUConfig\n",
    "    from rasa_nlu.model import Trainer\n",
    "\n",
    "    # Create args dictionary\n",
    "    args = {\"pipeline\": \"spacy_sklearn\"}\n",
    "\n",
    "    # Create a configuration and trainer\n",
    "    config = RasaNLUConfig(cmdline_args=args)\n",
    "    trainer = Trainer(config)\n",
    "\n",
    "    # Load the training data\n",
    "    training_data = load_data(\"./training_data.json\")\n",
    "\n",
    "    # Create an interpreter by training the model\n",
    "    interpreter = trainer.train(training_data)\n",
    "\n",
    "    # Try it out\n",
    "    print(interpreter.parse(\"I'm looking for a Mexican restaurant in the North of town\")) \n",
    "    \n",
    "    <script.py> output:\n",
    "        Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
    "        {'entities': [{'start': 18, 'end': 25, 'extractor': 'ner_crf', 'entity': 'cuisine', 'value': 'Mexican'}, {'start': 44, 'end': 49, 'extractor': 'ner_crf', 'entity': 'location', 'value': 'North'}], 'text': \"I'm looking for a Mexican restaurant in the North of town\", 'intent_ranking': [{'confidence': 0.5710798636909156, 'name': 'restaurant_search'}, {'confidence': 0.17661071712356397, 'name': 'goodbye'}, {'confidence': 0.14328579605798059, 'name': 'affirm'}, {'confidence': 0.10902362312753977, 'name': 'greet'}], 'intent': {'confidence': 0.5710798636909156, 'name': 'restaurant_search'}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green size=5> <b>3. Virtual Assistants and Accessing Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few notes on SQL injection:**\n",
    "- When using sqlite3, dont use .format or other traditional string operations to execute sql commands. \n",
    "- The safe way to pass params is to add them as an extra tuple arguments to the execute function as seen below. The execute function has safeguards implemented to make sure malitious code cant be inject into our queries :\n",
    "\n",
    "\n",
    "    # Bad Idea\n",
    "    query = \"SELECT name from restaurant where area='{}'\".format(area)\n",
    "    c.execute(query)\n",
    "    \n",
    "    # Better\n",
    "    t = (area,price)\n",
    "    c.execute('SELECT * FROM hotels WHERE area=? and price=?', t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
