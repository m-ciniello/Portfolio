{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC_Project - Building & Transferring a Deep Neural Net\n",
    "\n",
    "### Key steps:\n",
    "1. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "2. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 next. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "3. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "4. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce 5 better model?\n",
    "6. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
    "7. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a fresh new one.\n",
    "8. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "9. Try caching the frozen layers, and train the model again: how much faster is it now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#set up libs and data from first notebook!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First lets build a function to construct DNN quickly, so we don't have to do it over and over again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set function params\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None, \n",
    "        activation = tf.nn.elu, \n",
    "        initializer = tf.contrib.layers.variance_scaling_initializer()):\n",
    "    #if n_neurons is not a integer\n",
    "    if type(n_neurons) is not int:\n",
    "        if not all([type(k)==int for k in n_neurons]):\n",
    "            raise ValueError('All numbers must be integers! You idiot!')\n",
    "        if len(n_neurons) != n_layers:\n",
    "            raise ValueError('n_neurons must be equal to n_layers! Way to go, genius!')\n",
    "    #else just take n_neurons for each layer\n",
    "    else:\n",
    "        n_neurons = np.repeat(n_neurons,n_hidden_layers)\n",
    "    with tf.variable_scope(name, 'dnn'):\n",
    "        for layer, neurons in zip(range(n_hidden_layers), n_neurons):\n",
    "            inputs = tf.layers.dense(inputs, neurons, \n",
    "                                     activation=activation, \n",
    "                                     kernel_initializer=initializer, \n",
    "                                     name=\"SneakyHL_{0}\".format(layer+1))\n",
    "    #return the last inputs iteration (to be fed into the output layer)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up graph\n",
    "reset_graph()\n",
    "\n",
    "n_features = 28 * 28 # MNIST\n",
    "n_classes = 5\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "#run our function! Feed in the X variable as our first input\n",
    "last_HL = dnn(X, n_neurons=150,name=\"MikesGloriousDNN\")\n",
    "\n",
    "logits = tf.layers.dense(last_HL, n_classes, kernel_initializer=he_init, name=\"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'X' type=Placeholder>,\n",
       " <tf.Operation 'y' type=Placeholder>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/kernel' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/kernel/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/bias' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/bias/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/MatMul' type=MatMul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_1/Elu' type=Elu>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/kernel' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/kernel/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/bias' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/bias/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/MatMul' type=MatMul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_2/Elu' type=Elu>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/kernel' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/kernel/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/bias' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/bias/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/MatMul' type=MatMul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_3/Elu' type=Elu>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/kernel' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/kernel/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/bias' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/bias/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/MatMul' type=MatMul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_4/Elu' type=Elu>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/kernel' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/kernel/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/bias' type=VariableV2>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/bias/read' type=Identity>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/MatMul' type=MatMul>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'MikesGloriousDNN/SneakyHL_5/Elu' type=Elu>,\n",
       " <tf.Operation 'logits/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'logits/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'logits/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'logits/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'logits/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'logits/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'logits/kernel' type=VariableV2>,\n",
       " <tf.Operation 'logits/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'logits/kernel/read' type=Identity>,\n",
       " <tf.Operation 'logits/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'logits/bias' type=VariableV2>,\n",
       " <tf.Operation 'logits/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'logits/bias/read' type=Identity>,\n",
       " <tf.Operation 'logits/MatMul' type=MatMul>,\n",
       " <tf.Operation 'logits/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'Y_proba' type=Softmax>]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checkout operations\n",
    "tf.get_default_graph().get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'MikesGloriousDNN/SneakyHL_1/kernel:0' shape=(784, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'MikesGloriousDNN/SneakyHL_1/bias:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'MikesGloriousDNN/SneakyHL_2/kernel:0' shape=(150, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'MikesGloriousDNN/SneakyHL_2/bias:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'MikesGloriousDNN/SneakyHL_3/kernel:0' shape=(150, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'MikesGloriousDNN/SneakyHL_3/bias:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'MikesGloriousDNN/SneakyHL_4/kernel:0' shape=(150, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'MikesGloriousDNN/SneakyHL_4/bias:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'MikesGloriousDNN/SneakyHL_5/kernel:0' shape=(150, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'MikesGloriousDNN/SneakyHL_5/bias:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'logits/kernel:0' shape=(150, 5) dtype=float32_ref>,\n",
       " <tf.Variable 'logits/bias:0' shape=(5,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checkout operations\n",
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 next. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have to complete the graph by adding the training_op and the accurcay variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "#set loss function\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "mean_loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "\n",
    "#set training operation\n",
    "adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_operation = adam_optimizer.minimize(mean_loss, name=\"training_operation\")\n",
    "\n",
    "#set accuracy veriables\n",
    "correct_top1 = tf.nn.in_top_k(logits, y, 1)\n",
    "mean_accuracy = tf.reduce_mean(tf.cast(correct_top1,tf.float32),name=\"model_accuracy\")\n",
    "\n",
    "initialize_vars = tf.global_variables_initializer()\n",
    "model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great. Now lets extract the digits < 5 from the MNIST data set and run a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  (28038, 784)\n",
      "Validation size:  (2558, 784)\n",
      "Test size:  (5139, 784)\n"
     ]
    }
   ],
   "source": [
    "#extract MNIST observations 4 and below!\n",
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]\n",
    "\n",
    "print('Train size: ', X_train1.shape)\n",
    "print('Validation size: ', X_valid1.shape)\n",
    "print('Test size: ', X_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Epochs Complete:\tValidation Loss: 0.086732,\tBest Loss:  0.086732,\tAccuracy:  97.381%\n",
      "1 Epochs Complete:\tValidation Loss: 0.124854,\tBest Loss:  0.086732,\tAccuracy:  96.325%\n",
      "2 Epochs Complete:\tValidation Loss: 0.054358,\tBest Loss:  0.054358,\tAccuracy:  98.084%\n",
      "3 Epochs Complete:\tValidation Loss: 0.067607,\tBest Loss:  0.054358,\tAccuracy:  98.475%\n",
      "4 Epochs Complete:\tValidation Loss: 0.062015,\tBest Loss:  0.054358,\tAccuracy:  98.475%\n",
      "5 Epochs Complete:\tValidation Loss: 0.045546,\tBest Loss:  0.045546,\tAccuracy:  98.593%\n",
      "6 Epochs Complete:\tValidation Loss: 0.077384,\tBest Loss:  0.045546,\tAccuracy:  98.475%\n",
      "7 Epochs Complete:\tValidation Loss: 0.117975,\tBest Loss:  0.045546,\tAccuracy:  98.436%\n",
      "8 Epochs Complete:\tValidation Loss: 0.172980,\tBest Loss:  0.045546,\tAccuracy:  95.700%\n",
      "9 Epochs Complete:\tValidation Loss: 0.063484,\tBest Loss:  0.045546,\tAccuracy:  98.397%\n",
      "10 Epochs Complete:\tValidation Loss: 0.043767,\tBest Loss:  0.043767,\tAccuracy:  98.827%\n",
      "11 Epochs Complete:\tValidation Loss: 0.053166,\tBest Loss:  0.043767,\tAccuracy:  98.788%\n",
      "12 Epochs Complete:\tValidation Loss: 0.097072,\tBest Loss:  0.043767,\tAccuracy:  97.576%\n",
      "13 Epochs Complete:\tValidation Loss: 0.055638,\tBest Loss:  0.043767,\tAccuracy:  98.749%\n",
      "14 Epochs Complete:\tValidation Loss: 0.088586,\tBest Loss:  0.043767,\tAccuracy:  98.554%\n",
      "15 Epochs Complete:\tValidation Loss: 0.190284,\tBest Loss:  0.043767,\tAccuracy:  97.420%\n",
      "\n",
      "Model stopped EARLY at 16 epochs and 4760 steps\n",
      "Best Validation Accuracy: 98.8272%\n"
     ]
    }
   ],
   "source": [
    "#set paramaters for model run\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "n_rows = len(X_train1)\n",
    "val_dict = {X:X_valid1, y:y_valid1}\n",
    "\n",
    "#set params for early stopping\n",
    "max_epoch_no_progress = 5\n",
    "no_progress_epoch_count = 0\n",
    "best_loss = np.infty\n",
    "best_accuracy = 0\n",
    "\n",
    "#run dat model!\n",
    "with tf.Session() as sess:\n",
    "    #initialize\n",
    "    initialize_vars.run()\n",
    "    step=0\n",
    "    \n",
    "    #begin epoch and nested batch loops!\n",
    "    for epoch in range(n_epochs):\n",
    "        #create random permutation (no duplication!!!)\n",
    "        random_index = np.random.permutation(n_rows)\n",
    "        #split up random array into batch sizes\n",
    "        for rand_idx in np.array_split(random_index, n_rows//batch_size):\n",
    "            X_batch, y_batch = X_train1[rand_idx], y_train1[rand_idx]\n",
    "            #run training operations with batches\n",
    "            sess.run(training_operation, feed_dict={X:X_batch, y:y_batch})\n",
    "            step+=1\n",
    "        #extract loss val after each epoch\n",
    "        validation_loss, validation_acc = sess.run([mean_loss, mean_accuracy], feed_dict=val_dict)\n",
    "        #save model and implement early stopping\n",
    "        if validation_loss < best_loss:\n",
    "            the_best_model = model_saver.save(sess,\"./tf_logs/MC_ProjBTModel_0to4\")\n",
    "            best_loss = validation_loss\n",
    "            best_accuracy = validation_acc\n",
    "            no_progress_epoch_count = 0\n",
    "        else:\n",
    "            no_progress_epoch_count += 1\n",
    "            if no_progress_epoch_count > max_epoch_no_progress:\n",
    "                print(\"\\nModel stopped EARLY at {0} epochs and {1} steps\".format(epoch,step))\n",
    "                print(\"Best Validation Accuracy: {:.4f}%\".format(best_accuracy*100))\n",
    "                break\n",
    "        #print the results of the training epoch (keep 6 digits)\n",
    "        print(\"{0} Epochs Complete:\\tValidation Loss: {1:.6f},\\tBest Loss: {2: .6f},\\tAccuracy: {3: .3f}%\".format(epoch,validation_loss,best_loss,validation_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use SKlearn's GridSearchCV!!! Create a custom NNet Classifier that is compatible with GridSearch\n",
    "\n",
    "To speed things up, and avoiding duplicating our code a billion times, we will create a DNNClassifier class that we can train and pass into with Scikit-Learn's RandomizedSearchCV to perform hyperparameter tuning. Below is a breakdown of the Class:\n",
    "- the __init__() method (constructor) simply creates the instances variables for each hyperparam\n",
    "- the fit() method creates a graph, starts a session and trains the model. It calls on other functions to do so:\n",
    "    - the _construct_graph() method builds the graph. Once this is done is saves all important operations as instance (class) variables so you can access them easily in other methods\n",
    "    - the _DeepNeuralNetwork() method builds the hiddne layers. It also has support for batch normalization and dropout\n",
    "    - in the fit() method the user can input X_valid and y_valid variables which will implement early stopping. **NOTE: this implementation DOES NOT save the best model to disk, but instead to memory! It uses the _get_model_params() method to get all the graphs variables and values, and the _restore_model_params() method to restore variables from teh best model. This actually sppeds up trianing! Wooohoooo!**\n",
    "    - Once fit is complete, the model is trianed and the session remains open so that predictions can be made quickly... WITHOUT having to save a model to disk and restore it for every prediction. \n",
    "    - close_session() will allow you to close the session if need be!\n",
    "- predict_proba() uses the trained model to predict the class probas\n",
    "- predcit() method calls the predict_proba() method and return class with the highest probability for each instance!\n",
    "\n",
    "A few notes:\n",
    "- Use _leading_underscore (eg. self._variable) to decale 'private variables'; anything with this convention is ignored in \"from module import\"...  ***These are not actually private***, but its goo practice. These are actually known as “weak internal use indicators”.\n",
    "- Use single trailing underscore (eg. self.classes_) to to avoid conflict with Python keywords. For example: Tkinter.Toplevel(master, class_='ClassName') # Avoid conflict with 'class' keyword\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "\n",
    "class MikesGloriousDNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, learning_rate=0.01, k_in_top_val = 1, layer_name=\"hidden\",\n",
    "                 batch_size = 50,\n",
    "                 max_epochs_no_progress = 10,\n",
    "                 tf_logs_path = \"./DNN1_LOGS\",\n",
    "                 optimizer_class=tf.train.AdamOptimizer, \n",
    "                 activation_function=tf.nn.elu, \n",
    "                 initializer=tf.contrib.layers.variance_scaling_initializer(), \n",
    "                 tb_model_name = None,\n",
    "                 batch_norm_momentum_decay=None, \n",
    "                 dropout_rate=None, \n",
    "                 random_state=None):\n",
    "        \"\"\"Initialize the Classifier by storing all the hyperparams\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.k_in_top_val = k_in_top_val\n",
    "        self.layer_name = layer_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs_no_progress = max_epochs_no_progress\n",
    "        self.tf_logs_path = tf_logs_path\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.activation_function = activation_function\n",
    "        self.initializer = initializer\n",
    "        self.tb_model_name = tb_model_name #assign tb_model_name to activate tensor board model saving\n",
    "        self.batch_norm_momentum_decay = batch_norm_momentum_decay\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self.dropout_on=False\n",
    "        self.batch_norm_on = False\n",
    "        self.early_stopping_on = False\n",
    "        self._session = None\n",
    "        \n",
    "        #test inputs\n",
    "        assert (isinstance(self.n_hidden_layers, int)), \"n_hidden_layers parameter must be integer\"\n",
    "        assert (isinstance(self.learning_rate, float)), \"learning_rate parameter must be number\"\n",
    "        assert (isinstance(self.k_in_top_val, int)), \"k_in_top_val parameter must be integer\"\n",
    "        \n",
    "        if isinstance(self.n_neurons, list):\n",
    "            if not all([(isinstance(k, np.int) or isinstance(k, np.integer)) for k in n_neurons]):\n",
    "                raise ValueError('All numbers must be integers! You idiot!')\n",
    "            if len(self.n_neurons) != self.n_hidden_layers:\n",
    "                raise ValueError('n_neurons must be equal to n_layers! Way to go, genius!')\n",
    "        else:\n",
    "            assert (isinstance(self.n_neurons, int)), \"n_neurons parameter must be integer\"\n",
    "    \n",
    "    def _DeepNeuralNet(self, inputs):\n",
    "        \"\"\"Builds the hidden layers for the DNN, including support for batch normalization and dropout. Order of operations for a single layers is:\n",
    "            1. Dropout function\n",
    "            2. Normal dense layers\n",
    "            3. Batch normalization function\n",
    "            4. Activation function\n",
    "        Output layer is created in the '_build_graph' method\"\"\"\n",
    "        #first transform n_neurons\n",
    "        if not isinstance(self.n_neurons, list):\n",
    "            self.n_neurons = np.repeat(self.n_neurons, self.n_hidden_layers)\n",
    "                        \n",
    "        for layer, neurons in zip(range(self.n_hidden_layers), self.n_neurons):            \n",
    "            #if dropout_rate is None (default) if loop will return False\n",
    "            if self.dropout_rate:\n",
    "                self.dropout_on = True\n",
    "                #this will first create a dropout layers for X, and then a drop layer after every hidden layer\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._inTrainingMode)\n",
    "            \n",
    "            #create the normal hidden layers\n",
    "            inputs = tf.layers.dense(inputs, neurons, \n",
    "                                     kernel_initializer=self.initializer, \n",
    "                                     name=self.layer_name+\"_{}\".format(layer+1))\n",
    "            #if batch_normalization is None (default) if loop will return false\n",
    "            if self.batch_norm_momentum_decay:\n",
    "                self.batch_norm_on = True\n",
    "                inputs = tf.layers.batch_normalization(inputs, \n",
    "                                                       momentum=self.batch_norm_momentum_decay, \n",
    "                                                       training=self._inTrainingMode)\n",
    "            #create activation layer\n",
    "            inputs = self.activation_function(inputs, name=self.layer_name+\"_{}_ACTIVATED\".format(layer+1))\n",
    "            #return final activated layer\n",
    "        return inputs  \n",
    "    \n",
    "    def _construct_graph(self, n_features, n_classes):\n",
    "        \"\"\"Build graph that takes in X, y, and sets up the outputs\"\"\"\n",
    "        #set random states\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        #set X and y placeholders\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "        y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "        #set self._training if dropout or batch_norm is utilized\n",
    "        if self.batch_norm_momentum_decay or self.dropout_rate:\n",
    "            self._inTrainingMode = tf.placeholder_with_default(False, shape=(), name='inTrainingMode')\n",
    "        else:\n",
    "            self._inTrainingMode = None\n",
    "        #create layers\n",
    "        activated_last_HL = self._DeepNeuralNet(X)\n",
    "        \n",
    "        #create output logits and probabilities\n",
    "        output_logits = tf.layers.dense(activated_last_HL, n_classes, \n",
    "                                          kernel_initializer=self.initializer, \n",
    "                                          name='output_logits')\n",
    "        y_proba = tf.nn.softmax(output_logits, name='y_proba') #this isnt used in training but is useful later!\n",
    "        \n",
    "        #create loss funciton and calculate mean loss\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=output_logits)\n",
    "        mean_loss = tf.reduce_mean(cross_entropy, name='mean_loss')\n",
    "        \n",
    "        #create training operation and set accuracy variables\n",
    "        model_optimizer = self.optimizer_class(learning_rate = self.learning_rate)\n",
    "        training_op = model_optimizer.minimize(mean_loss, name='training_op')\n",
    "        correct_topk = tf.nn.in_top_k(output_logits, y, self.k_in_top_val)\n",
    "        mean_accuracy = tf.reduce_mean(tf.cast(correct_topk, tf.float32), name='mean_accuracy')\n",
    "        \n",
    "        #initialization and model saver variables\n",
    "        initialize_vars = tf.global_variables_initializer()\n",
    "        model_saver = tf.train.Saver()\n",
    "        \n",
    "        #track mean_loss for viewing in TB later!\n",
    "        if self.tb_model_name:\n",
    "            eastern = timezone('US/Eastern')\n",
    "            now = datetime.now(tz=eastern).strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "            logdir = \"{}/{}-{}\".format(self.tf_logs_path,self.tb_model_name, now)\n",
    "            mean_loss_summary = tf.summary.scalar('mean_loss_summary', mean_loss)\n",
    "            file_writer = tf.summary.FileWriter(logdir, self._graph)\n",
    "            #save values to instance\n",
    "            self._mean_loss_summary = mean_loss_summary\n",
    "            self._file_writer = file_writer\n",
    "        \n",
    "        #IMPORTANT: Save all important operations and variables for quick access later!\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba = y_proba\n",
    "        self._training_op = training_op\n",
    "        self._correct_intopK = correct_topk\n",
    "        self._mean_loss = mean_loss\n",
    "        self._mean_accuracy = mean_accuracy\n",
    "        self._init = initialize_vars\n",
    "        self._model_saver = model_saver\n",
    "\n",
    "    def close_session(self):\n",
    "        #check if session initialized (self._session anything other than None or False)\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "    \n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        #set class graph as default graph\n",
    "        with self._graph.as_default():\n",
    "            global_vars = tf.global_variables()\n",
    "        #use dictionary comp. to get names and values of each variable\n",
    "        return {gv_name.op.name: gv_value for gv_name, gv_value in zip(global_vars, self._session.run(global_vars))}\n",
    "   \n",
    "    def _restore_model_params(self, gv_names_vals):\n",
    "        #extract gv names\n",
    "        gv_names = list(gv_names_vals.keys())\n",
    "        #extract assign operations for each gv        \n",
    "        assign_ops = {gv_name:self._graph.get_operation_by_name(gv_name+\"/Assign\") for gv_name in gv_names}\n",
    "        #extract second operation of each assign op for each gv\n",
    "        init_values = {gv_name:assign_op.inputs[1] for gv_name, assign_op in assign_ops.items()}\n",
    "        #create dict of assign_ops and gv_values\n",
    "        feed_dict = {init_values[gv_name]:gv_names_vals[gv_name] for gv_name in gv_names}\n",
    "        #restore model params by updating assignment operations\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "            \n",
    "    def fit(self, X, y, X_valid=None, y_valid=None, n_epochs=100):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use EARLY STOPPING!!!\"\"\"\n",
    "        #close existing session if open\n",
    "        self.close_session()\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        #early stopping warning\n",
    "        if X_valid is None or y_valid is None:\n",
    "            warnings.warn(\"Early Stopping will not run without 'X_valid' and 'y_valid' inputs.\")\n",
    "        else:\n",
    "            self.early_stopping_on = True\n",
    "                \n",
    "        # Translate the labels vector to a vector of sequential class indeces from integers 0 to n_classes-1\n",
    "        self.classes_unique, y = np.unique(y, return_inverse=True)\n",
    "            \n",
    "        #extract n_inputs and n_outputs from the trainin set!!!\n",
    "        n_classes_ = len(self.classes_unique)\n",
    "        n_features_ = X.shape[1]\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            #BUILD THE GRAPH\n",
    "            self._construct_graph(n_features_, n_classes_)\n",
    "            #We need to explicitly run the extra update operations needed by batch normalization \n",
    "            #namely (sess.run([training_op, extra_update_ops],...).\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        train_text = \"Training DNN: \\nlayers={0}, neurons={1}, epochs={2},  batch_size={3}, early_stop:{4}, batch_norm:{5}, dropout:{6}\\n\"\n",
    "        print(train_text.format(self.n_hidden_layers,self.n_neurons,self.n_epochs,self.batch_size,\n",
    "                                self.early_stopping_on,self.batch_norm_on,self.dropout_on))\n",
    "        #set params to run model and perform early stopping (if we need it!)\n",
    "        no_progress_epoch_count = 0\n",
    "        best_loss = np.infty\n",
    "        best_accuracy = 0\n",
    "        best_params = None\n",
    "        \n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            step=0\n",
    "            for epoch in range(self.n_epochs):\n",
    "                random_index = np.random.permutation(len(X))\n",
    "                for rand_idx in np.array_split(random_index, len(X)//self.batch_size):\n",
    "                    X_batch, y_batch = X[rand_idx], y[rand_idx]\n",
    "                    batch_dict = {self._X:X_batch, self._y:y_batch}\n",
    "                    #if inTrainingMode is activated in the graph, then it needs to be set to true to activate dropout and/or batch_normalization\n",
    "                    if self._inTrainingMode is not None:\n",
    "                        batch_dict[self._inTrainingMode] = True\n",
    "                    sess.run(self._training_op, feed_dict=batch_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=batch_dict)\n",
    "                    #save model summary every 10 steps for TB\n",
    "                    if self.tb_model_name:\n",
    "                        if step % 10 == 0:\n",
    "                            summary_str = self._mean_loss_summary.eval(feed_dict=batch_dict)\n",
    "                            self._file_writer.add_summary(summary_str, step)\n",
    "                    step += 1\n",
    "\n",
    "                #if validation data provided, run early stopping\n",
    "                if self.early_stopping_on:\n",
    "                    val_dict = {self._X: X_valid, self._y:y_valid}\n",
    "                    validation_loss, validation_acc = sess.run([self._mean_loss, self._mean_accuracy], feed_dict=val_dict)\n",
    "\n",
    "                    #save model and implement early stopping\n",
    "                    if validation_loss < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = validation_loss\n",
    "                        best_accuracy = validation_acc\n",
    "                        no_progress_epoch_count = 0\n",
    "                    else:\n",
    "                        no_progress_epoch_count += 1\n",
    "                        if no_progress_epoch_count > self.max_epochs_no_progress:\n",
    "                            print(\"\\nModel stopped EARLY at {0} epochs and {1} steps\".format(epoch+1,step))\n",
    "                            print(\"Best Validation Accuracy: {:.4f}%\".format(best_accuracy*100))\n",
    "                            break                        \n",
    "                    #print the results of the training epoch (keep 6 digits)\n",
    "                    print(\"{0} Epochs:\\tVal Loss: {1:.6f},\\tBest_Loss: {2: .6f},\\tAcc: {3: .3f}%\\tNo_Prog: {4}\".format(epoch+1,validation_loss,best_loss,validation_acc*100,no_progress_epoch_count))\n",
    "                #no validation provided, calculate loss and acc on batches\n",
    "                else:\n",
    "                    train_dict = {self._X: X_batch, self._y:y_batch}\n",
    "                    train_loss, train_acc = sess.run([self._mean_loss, self._mean_accuracy], feed_dict=train_dict)\n",
    "                    print(\"{0} Epochs Complete:\\tTraining Batch Loss: {1:.6f}, \\tAccuracy: {2: .3f}%\".format(epoch+1,train_loss,train_acc*100))\n",
    "            #close filewriter if open\n",
    "            if self.tb_model_name:\n",
    "                self._file_writer.close()\n",
    "            #restore best model if we used early stopping\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            #Return self. This is again for compatibility reasons with common interface of scikit-learn.\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        #return error if no session initialized (ie model hasn't been fit yet)\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This {} instance has not yet been fitted!!!\".format(self.__class__.__name__))\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "    \n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_unique[class_index]] for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._model_saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the model without batch_norm or dropout!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN: \n",
      "layers=5, neurons=[50 50 50 50 50], epochs=1000,  batch_size=50, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.074042,\tBest_Loss:  0.074042,\tAcc:  97.694%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.085416,\tBest_Loss:  0.074042,\tAcc:  98.163%\tNo_Prog: 1\n",
      "3 Epochs:\tVal Loss: 0.072412,\tBest_Loss:  0.072412,\tAcc:  97.850%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.066085,\tBest_Loss:  0.066085,\tAcc:  98.749%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.057669,\tBest_Loss:  0.057669,\tAcc:  98.241%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.058243,\tBest_Loss:  0.057669,\tAcc:  98.593%\tNo_Prog: 1\n",
      "7 Epochs:\tVal Loss: 0.059500,\tBest_Loss:  0.057669,\tAcc:  98.749%\tNo_Prog: 2\n",
      "8 Epochs:\tVal Loss: 0.066636,\tBest_Loss:  0.057669,\tAcc:  98.475%\tNo_Prog: 3\n",
      "9 Epochs:\tVal Loss: 0.083246,\tBest_Loss:  0.057669,\tAcc:  97.850%\tNo_Prog: 4\n",
      "10 Epochs:\tVal Loss: 0.150970,\tBest_Loss:  0.057669,\tAcc:  97.068%\tNo_Prog: 5\n",
      "\n",
      "Model stopped EARLY at 11 epochs and 6160 steps\n",
      "Best Validation Accuracy: 98.2408%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MikesGloriousDNNClassifier(activation_function=<function elu at 0x000002297AEA8B70>,\n",
       "              batch_norm_momentum_decay=None, batch_size=50,\n",
       "              dropout_rate=None,\n",
       "              initializer=<function variance_scaling_initializer.<locals>._initializer at 0x0000022902276D90>,\n",
       "              k_in_top_val=1, layer_name='hidden', learning_rate=0.01,\n",
       "              max_epochs_no_progress=5, n_hidden_layers=5,\n",
       "              n_neurons=array([50, 50, 50, 50, 50]),\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42, tb_model_name=None,\n",
       "              tf_logs_path='./DNN1_LOGS')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "dnn_clf = MikesGloriousDNNClassifier(random_state=42, n_hidden_layers=5, n_neurons=50, max_epochs_no_progress=5)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model is trained!!! Evaluate the accuracy using the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99202179412337033"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wooohoo!!! Model is working and making predictions like a pro! Lets use RandomizedGridSearchCV to find better hyperparams! This will probably take a while... run it overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mciniello\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:584: DeprecationWarning: \"fit_params\" as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the \"fit\" method instead.\n",
      "  '\"fit\" method instead.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] n_neurons=50, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198ACE85620> \n",
      "Training DNN: \n",
      "layers=5, neurons=[50 50 50 50 50], epochs=100,  batch_size=100, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.065894,\tBest_Loss:  0.065894,\tAcc:  97.498%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.076526,\tBest_Loss:  0.065894,\tAcc:  97.733%\tNo_Prog: 1\n",
      "2 Epochs:\tVal Loss: 0.072168,\tBest_Loss:  0.065894,\tAcc:  98.124%\tNo_Prog: 2\n",
      "3 Epochs:\tVal Loss: 0.051950,\tBest_Loss:  0.051950,\tAcc:  98.514%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.060420,\tBest_Loss:  0.051950,\tAcc:  98.280%\tNo_Prog: 1\n",
      "5 Epochs:\tVal Loss: 0.072689,\tBest_Loss:  0.051950,\tAcc:  98.124%\tNo_Prog: 2\n",
      "6 Epochs:\tVal Loss: 0.082801,\tBest_Loss:  0.051950,\tAcc:  98.045%\tNo_Prog: 3\n",
      "7 Epochs:\tVal Loss: 0.070122,\tBest_Loss:  0.051950,\tAcc:  98.475%\tNo_Prog: 4\n",
      "8 Epochs:\tVal Loss: 0.081460,\tBest_Loss:  0.051950,\tAcc:  98.788%\tNo_Prog: 5\n",
      "9 Epochs:\tVal Loss: 0.096258,\tBest_Loss:  0.051950,\tAcc:  97.889%\tNo_Prog: 6\n",
      "10 Epochs:\tVal Loss: 0.108680,\tBest_Loss:  0.051950,\tAcc:  98.084%\tNo_Prog: 7\n",
      "11 Epochs:\tVal Loss: 0.105823,\tBest_Loss:  0.051950,\tAcc:  98.554%\tNo_Prog: 8\n",
      "12 Epochs:\tVal Loss: 0.111147,\tBest_Loss:  0.051950,\tAcc:  98.006%\tNo_Prog: 9\n",
      "13 Epochs:\tVal Loss: 0.105619,\tBest_Loss:  0.051950,\tAcc:  98.436%\tNo_Prog: 10\n",
      "14 Epochs:\tVal Loss: 0.139687,\tBest_Loss:  0.051950,\tAcc:  98.084%\tNo_Prog: 11\n",
      "15 Epochs:\tVal Loss: 0.107833,\tBest_Loss:  0.051950,\tAcc:  98.984%\tNo_Prog: 12\n",
      "16 Epochs:\tVal Loss: 0.104688,\tBest_Loss:  0.051950,\tAcc:  98.475%\tNo_Prog: 13\n",
      "17 Epochs:\tVal Loss: 0.212758,\tBest_Loss:  0.051950,\tAcc:  97.928%\tNo_Prog: 14\n",
      "18 Epochs:\tVal Loss: 0.200146,\tBest_Loss:  0.051950,\tAcc:  98.045%\tNo_Prog: 15\n",
      "19 Epochs:\tVal Loss: 0.143630,\tBest_Loss:  0.051950,\tAcc:  98.632%\tNo_Prog: 16\n",
      "20 Epochs:\tVal Loss: 0.154492,\tBest_Loss:  0.051950,\tAcc:  98.554%\tNo_Prog: 17\n",
      "21 Epochs:\tVal Loss: 0.156570,\tBest_Loss:  0.051950,\tAcc:  98.475%\tNo_Prog: 18\n",
      "22 Epochs:\tVal Loss: 0.162016,\tBest_Loss:  0.051950,\tAcc:  98.397%\tNo_Prog: 19\n",
      "23 Epochs:\tVal Loss: 0.178937,\tBest_Loss:  0.051950,\tAcc:  98.397%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 24 epochs and 4650 steps\n",
      "Best Validation Accuracy: 98.5145%\n",
      "[CV]  n_neurons=50, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198ACE85620>, total=  11.4s\n",
      "[CV] n_neurons=50, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198ACE85620> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN: \n",
      "layers=5, neurons=[50 50 50 50 50], epochs=100,  batch_size=100, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.082417,\tBest_Loss:  0.082417,\tAcc:  97.615%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.075253,\tBest_Loss:  0.075253,\tAcc:  97.733%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.068990,\tBest_Loss:  0.068990,\tAcc:  97.967%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.058670,\tBest_Loss:  0.058670,\tAcc:  98.475%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.076264,\tBest_Loss:  0.058670,\tAcc:  98.436%\tNo_Prog: 1\n",
      "5 Epochs:\tVal Loss: 0.097005,\tBest_Loss:  0.058670,\tAcc:  97.733%\tNo_Prog: 2\n",
      "6 Epochs:\tVal Loss: 0.082866,\tBest_Loss:  0.058670,\tAcc:  98.202%\tNo_Prog: 3\n",
      "7 Epochs:\tVal Loss: 0.066430,\tBest_Loss:  0.058670,\tAcc:  98.554%\tNo_Prog: 4\n",
      "8 Epochs:\tVal Loss: 0.086034,\tBest_Loss:  0.058670,\tAcc:  98.358%\tNo_Prog: 5\n",
      "9 Epochs:\tVal Loss: 0.127167,\tBest_Loss:  0.058670,\tAcc:  98.124%\tNo_Prog: 6\n",
      "10 Epochs:\tVal Loss: 0.097736,\tBest_Loss:  0.058670,\tAcc:  98.084%\tNo_Prog: 7\n",
      "11 Epochs:\tVal Loss: 0.098572,\tBest_Loss:  0.058670,\tAcc:  98.436%\tNo_Prog: 8\n",
      "12 Epochs:\tVal Loss: 0.091668,\tBest_Loss:  0.058670,\tAcc:  98.788%\tNo_Prog: 9\n",
      "13 Epochs:\tVal Loss: 0.098005,\tBest_Loss:  0.058670,\tAcc:  98.827%\tNo_Prog: 10\n",
      "14 Epochs:\tVal Loss: 0.167393,\tBest_Loss:  0.058670,\tAcc:  98.006%\tNo_Prog: 11\n",
      "15 Epochs:\tVal Loss: 0.106485,\tBest_Loss:  0.058670,\tAcc:  98.436%\tNo_Prog: 12\n",
      "16 Epochs:\tVal Loss: 0.092554,\tBest_Loss:  0.058670,\tAcc:  98.358%\tNo_Prog: 13\n",
      "17 Epochs:\tVal Loss: 0.117367,\tBest_Loss:  0.058670,\tAcc:  98.475%\tNo_Prog: 14\n",
      "18 Epochs:\tVal Loss: 0.130293,\tBest_Loss:  0.058670,\tAcc:  98.436%\tNo_Prog: 15\n",
      "19 Epochs:\tVal Loss: 0.125441,\tBest_Loss:  0.058670,\tAcc:  98.319%\tNo_Prog: 16\n",
      "20 Epochs:\tVal Loss: 0.183592,\tBest_Loss:  0.058670,\tAcc:  98.475%\tNo_Prog: 17\n",
      "21 Epochs:\tVal Loss: 0.125816,\tBest_Loss:  0.058670,\tAcc:  98.788%\tNo_Prog: 18\n",
      "22 Epochs:\tVal Loss: 0.158309,\tBest_Loss:  0.058670,\tAcc:  98.475%\tNo_Prog: 19\n",
      "23 Epochs:\tVal Loss: 0.232476,\tBest_Loss:  0.058670,\tAcc:  98.397%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 24 epochs and 4650 steps\n",
      "Best Validation Accuracy: 98.4754%\n",
      "[CV]  n_neurons=50, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198ACE85620>, total=  10.1s\n",
      "[CV] n_neurons=50, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198ACE85620> \n",
      "Training DNN: \n",
      "layers=5, neurons=[50 50 50 50 50], epochs=100,  batch_size=100, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.077969,\tBest_Loss:  0.077969,\tAcc:  97.459%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.061167,\tBest_Loss:  0.061167,\tAcc:  98.319%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.045975,\tBest_Loss:  0.045975,\tAcc:  98.593%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.115641,\tBest_Loss:  0.045975,\tAcc:  97.146%\tNo_Prog: 1\n",
      "4 Epochs:\tVal Loss: 0.073646,\tBest_Loss:  0.045975,\tAcc:  98.202%\tNo_Prog: 2\n",
      "5 Epochs:\tVal Loss: 0.057802,\tBest_Loss:  0.045975,\tAcc:  98.436%\tNo_Prog: 3\n",
      "6 Epochs:\tVal Loss: 0.081594,\tBest_Loss:  0.045975,\tAcc:  98.241%\tNo_Prog: 4\n",
      "7 Epochs:\tVal Loss: 0.096405,\tBest_Loss:  0.045975,\tAcc:  98.475%\tNo_Prog: 5\n",
      "8 Epochs:\tVal Loss: 0.060276,\tBest_Loss:  0.045975,\tAcc:  98.866%\tNo_Prog: 6\n",
      "9 Epochs:\tVal Loss: 0.124242,\tBest_Loss:  0.045975,\tAcc:  98.084%\tNo_Prog: 7\n",
      "10 Epochs:\tVal Loss: 0.090404,\tBest_Loss:  0.045975,\tAcc:  98.397%\tNo_Prog: 8\n",
      "11 Epochs:\tVal Loss: 0.074992,\tBest_Loss:  0.045975,\tAcc:  98.710%\tNo_Prog: 9\n",
      "12 Epochs:\tVal Loss: 0.069456,\tBest_Loss:  0.045975,\tAcc:  98.671%\tNo_Prog: 10\n",
      "13 Epochs:\tVal Loss: 0.087545,\tBest_Loss:  0.045975,\tAcc:  98.827%\tNo_Prog: 11\n",
      "14 Epochs:\tVal Loss: 0.102936,\tBest_Loss:  0.045975,\tAcc:  98.554%\tNo_Prog: 12\n",
      "15 Epochs:\tVal Loss: 0.073883,\tBest_Loss:  0.045975,\tAcc:  98.749%\tNo_Prog: 13\n",
      "16 Epochs:\tVal Loss: 0.133357,\tBest_Loss:  0.045975,\tAcc:  98.554%\tNo_Prog: 14\n",
      "17 Epochs:\tVal Loss: 0.095101,\tBest_Loss:  0.045975,\tAcc:  98.866%\tNo_Prog: 15\n",
      "18 Epochs:\tVal Loss: 0.130466,\tBest_Loss:  0.045975,\tAcc:  98.006%\tNo_Prog: 16\n",
      "19 Epochs:\tVal Loss: 0.110632,\tBest_Loss:  0.045975,\tAcc:  98.202%\tNo_Prog: 17\n",
      "20 Epochs:\tVal Loss: 0.113825,\tBest_Loss:  0.045975,\tAcc:  98.436%\tNo_Prog: 18\n",
      "21 Epochs:\tVal Loss: 0.123926,\tBest_Loss:  0.045975,\tAcc:  98.554%\tNo_Prog: 19\n",
      "22 Epochs:\tVal Loss: 0.155686,\tBest_Loss:  0.045975,\tAcc:  98.554%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 23 epochs and 4464 steps\n",
      "Best Validation Accuracy: 98.5927%\n",
      "[CV]  n_neurons=50, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198ACE85620>, total=  10.6s\n",
      "[CV] n_neurons=100, n_hidden_layers=5, learning_rate=0.01, batch_size=500, activation_function=<function relu at 0x000001989B4C6268> \n",
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=500, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.091795,\tBest_Loss:  0.091795,\tAcc:  97.068%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.066237,\tBest_Loss:  0.066237,\tAcc:  98.045%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.052250,\tBest_Loss:  0.052250,\tAcc:  98.202%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.045660,\tBest_Loss:  0.045660,\tAcc:  98.671%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.054406,\tBest_Loss:  0.045660,\tAcc:  98.475%\tNo_Prog: 1\n",
      "5 Epochs:\tVal Loss: 0.040873,\tBest_Loss:  0.040873,\tAcc:  98.905%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.039379,\tBest_Loss:  0.039379,\tAcc:  98.984%\tNo_Prog: 0\n",
      "7 Epochs:\tVal Loss: 0.036104,\tBest_Loss:  0.036104,\tAcc:  98.984%\tNo_Prog: 0\n",
      "8 Epochs:\tVal Loss: 0.045943,\tBest_Loss:  0.036104,\tAcc:  98.788%\tNo_Prog: 1\n",
      "9 Epochs:\tVal Loss: 0.042583,\tBest_Loss:  0.036104,\tAcc:  98.944%\tNo_Prog: 2\n",
      "10 Epochs:\tVal Loss: 0.044317,\tBest_Loss:  0.036104,\tAcc:  98.944%\tNo_Prog: 3\n",
      "11 Epochs:\tVal Loss: 0.039146,\tBest_Loss:  0.036104,\tAcc:  99.101%\tNo_Prog: 4\n",
      "12 Epochs:\tVal Loss: 0.039144,\tBest_Loss:  0.036104,\tAcc:  99.218%\tNo_Prog: 5\n",
      "13 Epochs:\tVal Loss: 0.035978,\tBest_Loss:  0.035978,\tAcc:  99.218%\tNo_Prog: 0\n",
      "14 Epochs:\tVal Loss: 0.037258,\tBest_Loss:  0.035978,\tAcc:  99.257%\tNo_Prog: 1\n",
      "15 Epochs:\tVal Loss: 0.037300,\tBest_Loss:  0.035978,\tAcc:  99.179%\tNo_Prog: 2\n",
      "16 Epochs:\tVal Loss: 0.037795,\tBest_Loss:  0.035978,\tAcc:  99.218%\tNo_Prog: 3\n",
      "17 Epochs:\tVal Loss: 0.038176,\tBest_Loss:  0.035978,\tAcc:  99.101%\tNo_Prog: 4\n",
      "18 Epochs:\tVal Loss: 0.038016,\tBest_Loss:  0.035978,\tAcc:  99.101%\tNo_Prog: 5\n",
      "19 Epochs:\tVal Loss: 0.038379,\tBest_Loss:  0.035978,\tAcc:  99.140%\tNo_Prog: 6\n",
      "20 Epochs:\tVal Loss: 0.038691,\tBest_Loss:  0.035978,\tAcc:  99.179%\tNo_Prog: 7\n",
      "21 Epochs:\tVal Loss: 0.038703,\tBest_Loss:  0.035978,\tAcc:  99.179%\tNo_Prog: 8\n",
      "22 Epochs:\tVal Loss: 0.038997,\tBest_Loss:  0.035978,\tAcc:  99.218%\tNo_Prog: 9\n",
      "23 Epochs:\tVal Loss: 0.039183,\tBest_Loss:  0.035978,\tAcc:  99.140%\tNo_Prog: 10\n",
      "24 Epochs:\tVal Loss: 0.039505,\tBest_Loss:  0.035978,\tAcc:  99.218%\tNo_Prog: 11\n",
      "25 Epochs:\tVal Loss: 0.039404,\tBest_Loss:  0.035978,\tAcc:  99.140%\tNo_Prog: 12\n",
      "26 Epochs:\tVal Loss: 0.039555,\tBest_Loss:  0.035978,\tAcc:  99.140%\tNo_Prog: 13\n",
      "27 Epochs:\tVal Loss: 0.039747,\tBest_Loss:  0.035978,\tAcc:  99.101%\tNo_Prog: 14\n",
      "28 Epochs:\tVal Loss: 0.039896,\tBest_Loss:  0.035978,\tAcc:  99.101%\tNo_Prog: 15\n",
      "29 Epochs:\tVal Loss: 0.040138,\tBest_Loss:  0.035978,\tAcc:  99.101%\tNo_Prog: 16\n",
      "30 Epochs:\tVal Loss: 0.040307,\tBest_Loss:  0.035978,\tAcc:  99.140%\tNo_Prog: 17\n",
      "31 Epochs:\tVal Loss: 0.040436,\tBest_Loss:  0.035978,\tAcc:  99.101%\tNo_Prog: 18\n",
      "32 Epochs:\tVal Loss: 0.040520,\tBest_Loss:  0.035978,\tAcc:  99.140%\tNo_Prog: 19\n",
      "33 Epochs:\tVal Loss: 0.040831,\tBest_Loss:  0.035978,\tAcc:  99.101%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 34 epochs and 1295 steps\n",
      "Best Validation Accuracy: 99.2181%\n",
      "[CV]  n_neurons=100, n_hidden_layers=5, learning_rate=0.01, batch_size=500, activation_function=<function relu at 0x000001989B4C6268>, total=  17.7s\n",
      "[CV] n_neurons=100, n_hidden_layers=5, learning_rate=0.01, batch_size=500, activation_function=<function relu at 0x000001989B4C6268> \n",
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=500, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.090091,\tBest_Loss:  0.090091,\tAcc:  97.654%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.063800,\tBest_Loss:  0.063800,\tAcc:  98.202%\tNo_Prog: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Epochs:\tVal Loss: 0.053580,\tBest_Loss:  0.053580,\tAcc:  98.593%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.042977,\tBest_Loss:  0.042977,\tAcc:  98.397%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.042300,\tBest_Loss:  0.042300,\tAcc:  98.514%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.041023,\tBest_Loss:  0.041023,\tAcc:  98.632%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.036701,\tBest_Loss:  0.036701,\tAcc:  98.827%\tNo_Prog: 0\n",
      "7 Epochs:\tVal Loss: 0.034645,\tBest_Loss:  0.034645,\tAcc:  98.788%\tNo_Prog: 0\n",
      "8 Epochs:\tVal Loss: 0.035368,\tBest_Loss:  0.034645,\tAcc:  98.866%\tNo_Prog: 1\n",
      "9 Epochs:\tVal Loss: 0.034106,\tBest_Loss:  0.034106,\tAcc:  98.788%\tNo_Prog: 0\n",
      "10 Epochs:\tVal Loss: 0.037430,\tBest_Loss:  0.034106,\tAcc:  98.554%\tNo_Prog: 1\n",
      "11 Epochs:\tVal Loss: 0.035471,\tBest_Loss:  0.034106,\tAcc:  98.827%\tNo_Prog: 2\n",
      "12 Epochs:\tVal Loss: 0.034830,\tBest_Loss:  0.034106,\tAcc:  98.984%\tNo_Prog: 3\n",
      "13 Epochs:\tVal Loss: 0.034424,\tBest_Loss:  0.034106,\tAcc:  99.023%\tNo_Prog: 4\n",
      "14 Epochs:\tVal Loss: 0.034895,\tBest_Loss:  0.034106,\tAcc:  99.023%\tNo_Prog: 5\n",
      "15 Epochs:\tVal Loss: 0.034768,\tBest_Loss:  0.034106,\tAcc:  99.062%\tNo_Prog: 6\n",
      "16 Epochs:\tVal Loss: 0.035212,\tBest_Loss:  0.034106,\tAcc:  99.101%\tNo_Prog: 7\n",
      "17 Epochs:\tVal Loss: 0.035564,\tBest_Loss:  0.034106,\tAcc:  98.984%\tNo_Prog: 8\n",
      "18 Epochs:\tVal Loss: 0.035359,\tBest_Loss:  0.034106,\tAcc:  98.944%\tNo_Prog: 9\n",
      "19 Epochs:\tVal Loss: 0.035679,\tBest_Loss:  0.034106,\tAcc:  99.062%\tNo_Prog: 10\n",
      "20 Epochs:\tVal Loss: 0.035860,\tBest_Loss:  0.034106,\tAcc:  99.101%\tNo_Prog: 11\n",
      "21 Epochs:\tVal Loss: 0.036146,\tBest_Loss:  0.034106,\tAcc:  99.062%\tNo_Prog: 12\n",
      "22 Epochs:\tVal Loss: 0.036201,\tBest_Loss:  0.034106,\tAcc:  99.062%\tNo_Prog: 13\n",
      "23 Epochs:\tVal Loss: 0.037074,\tBest_Loss:  0.034106,\tAcc:  99.023%\tNo_Prog: 14\n",
      "24 Epochs:\tVal Loss: 0.036322,\tBest_Loss:  0.034106,\tAcc:  99.101%\tNo_Prog: 15\n",
      "25 Epochs:\tVal Loss: 0.036947,\tBest_Loss:  0.034106,\tAcc:  99.062%\tNo_Prog: 16\n",
      "26 Epochs:\tVal Loss: 0.037241,\tBest_Loss:  0.034106,\tAcc:  98.984%\tNo_Prog: 17\n",
      "27 Epochs:\tVal Loss: 0.037251,\tBest_Loss:  0.034106,\tAcc:  99.023%\tNo_Prog: 18\n",
      "28 Epochs:\tVal Loss: 0.037592,\tBest_Loss:  0.034106,\tAcc:  99.023%\tNo_Prog: 19\n",
      "29 Epochs:\tVal Loss: 0.037642,\tBest_Loss:  0.034106,\tAcc:  99.023%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 30 epochs and 1147 steps\n",
      "Best Validation Accuracy: 98.7881%\n",
      "[CV]  n_neurons=100, n_hidden_layers=5, learning_rate=0.01, batch_size=500, activation_function=<function relu at 0x000001989B4C6268>, total=  15.4s\n",
      "[CV] n_neurons=100, n_hidden_layers=5, learning_rate=0.01, batch_size=500, activation_function=<function relu at 0x000001989B4C6268> \n",
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=500, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.089419,\tBest_Loss:  0.089419,\tAcc:  97.420%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.060344,\tBest_Loss:  0.060344,\tAcc:  98.124%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.044873,\tBest_Loss:  0.044873,\tAcc:  98.788%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.041088,\tBest_Loss:  0.041088,\tAcc:  98.593%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.040069,\tBest_Loss:  0.040069,\tAcc:  98.593%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.035777,\tBest_Loss:  0.035777,\tAcc:  98.866%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.036143,\tBest_Loss:  0.035777,\tAcc:  98.984%\tNo_Prog: 1\n",
      "7 Epochs:\tVal Loss: 0.035786,\tBest_Loss:  0.035777,\tAcc:  98.749%\tNo_Prog: 2\n",
      "8 Epochs:\tVal Loss: 0.029762,\tBest_Loss:  0.029762,\tAcc:  99.218%\tNo_Prog: 0\n",
      "9 Epochs:\tVal Loss: 0.032527,\tBest_Loss:  0.029762,\tAcc:  98.944%\tNo_Prog: 1\n",
      "10 Epochs:\tVal Loss: 0.030527,\tBest_Loss:  0.029762,\tAcc:  99.062%\tNo_Prog: 2\n",
      "11 Epochs:\tVal Loss: 0.031803,\tBest_Loss:  0.029762,\tAcc:  99.023%\tNo_Prog: 3\n",
      "12 Epochs:\tVal Loss: 0.031350,\tBest_Loss:  0.029762,\tAcc:  99.101%\tNo_Prog: 4\n",
      "13 Epochs:\tVal Loss: 0.033045,\tBest_Loss:  0.029762,\tAcc:  99.023%\tNo_Prog: 5\n",
      "14 Epochs:\tVal Loss: 0.033330,\tBest_Loss:  0.029762,\tAcc:  98.944%\tNo_Prog: 6\n",
      "15 Epochs:\tVal Loss: 0.033107,\tBest_Loss:  0.029762,\tAcc:  99.062%\tNo_Prog: 7\n",
      "16 Epochs:\tVal Loss: 0.032515,\tBest_Loss:  0.029762,\tAcc:  99.101%\tNo_Prog: 8\n",
      "17 Epochs:\tVal Loss: 0.033543,\tBest_Loss:  0.029762,\tAcc:  99.023%\tNo_Prog: 9\n",
      "18 Epochs:\tVal Loss: 0.033939,\tBest_Loss:  0.029762,\tAcc:  99.023%\tNo_Prog: 10\n",
      "19 Epochs:\tVal Loss: 0.033807,\tBest_Loss:  0.029762,\tAcc:  99.101%\tNo_Prog: 11\n",
      "20 Epochs:\tVal Loss: 0.034399,\tBest_Loss:  0.029762,\tAcc:  99.101%\tNo_Prog: 12\n",
      "21 Epochs:\tVal Loss: 0.034926,\tBest_Loss:  0.029762,\tAcc:  99.023%\tNo_Prog: 13\n",
      "22 Epochs:\tVal Loss: 0.035251,\tBest_Loss:  0.029762,\tAcc:  99.062%\tNo_Prog: 14\n",
      "23 Epochs:\tVal Loss: 0.035492,\tBest_Loss:  0.029762,\tAcc:  99.023%\tNo_Prog: 15\n",
      "24 Epochs:\tVal Loss: 0.036179,\tBest_Loss:  0.029762,\tAcc:  99.101%\tNo_Prog: 16\n",
      "25 Epochs:\tVal Loss: 0.036360,\tBest_Loss:  0.029762,\tAcc:  99.101%\tNo_Prog: 17\n",
      "26 Epochs:\tVal Loss: 0.036219,\tBest_Loss:  0.029762,\tAcc:  99.101%\tNo_Prog: 18\n",
      "27 Epochs:\tVal Loss: 0.035988,\tBest_Loss:  0.029762,\tAcc:  99.101%\tNo_Prog: 19\n",
      "28 Epochs:\tVal Loss: 0.036597,\tBest_Loss:  0.029762,\tAcc:  99.101%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 29 epochs and 1110 steps\n",
      "Best Validation Accuracy: 99.2181%\n",
      "[CV]  n_neurons=100, n_hidden_layers=5, learning_rate=0.01, batch_size=500, activation_function=<function relu at 0x000001989B4C6268>, total=  14.3s\n",
      "[CV] n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198A58DEF28> \n",
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=100, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.092386,\tBest_Loss:  0.092386,\tAcc:  97.146%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.100097,\tBest_Loss:  0.092386,\tAcc:  97.381%\tNo_Prog: 1\n",
      "2 Epochs:\tVal Loss: 0.063651,\tBest_Loss:  0.063651,\tAcc:  98.593%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.117068,\tBest_Loss:  0.063651,\tAcc:  97.576%\tNo_Prog: 1\n",
      "4 Epochs:\tVal Loss: 0.064257,\tBest_Loss:  0.063651,\tAcc:  98.554%\tNo_Prog: 2\n",
      "5 Epochs:\tVal Loss: 0.092135,\tBest_Loss:  0.063651,\tAcc:  98.671%\tNo_Prog: 3\n",
      "6 Epochs:\tVal Loss: 0.072489,\tBest_Loss:  0.063651,\tAcc:  98.319%\tNo_Prog: 4\n",
      "7 Epochs:\tVal Loss: 0.078258,\tBest_Loss:  0.063651,\tAcc:  98.202%\tNo_Prog: 5\n",
      "8 Epochs:\tVal Loss: 0.088682,\tBest_Loss:  0.063651,\tAcc:  98.710%\tNo_Prog: 6\n",
      "9 Epochs:\tVal Loss: 0.105995,\tBest_Loss:  0.063651,\tAcc:  98.436%\tNo_Prog: 7\n",
      "10 Epochs:\tVal Loss: 0.105020,\tBest_Loss:  0.063651,\tAcc:  98.397%\tNo_Prog: 8\n",
      "11 Epochs:\tVal Loss: 0.179480,\tBest_Loss:  0.063651,\tAcc:  97.889%\tNo_Prog: 9\n",
      "12 Epochs:\tVal Loss: 0.147218,\tBest_Loss:  0.063651,\tAcc:  98.319%\tNo_Prog: 10\n",
      "13 Epochs:\tVal Loss: 0.121600,\tBest_Loss:  0.063651,\tAcc:  98.397%\tNo_Prog: 11\n",
      "14 Epochs:\tVal Loss: 0.172781,\tBest_Loss:  0.063651,\tAcc:  98.124%\tNo_Prog: 12\n",
      "15 Epochs:\tVal Loss: 0.140924,\tBest_Loss:  0.063651,\tAcc:  98.944%\tNo_Prog: 13\n",
      "16 Epochs:\tVal Loss: 0.118689,\tBest_Loss:  0.063651,\tAcc:  98.749%\tNo_Prog: 14\n",
      "17 Epochs:\tVal Loss: 0.179909,\tBest_Loss:  0.063651,\tAcc:  98.514%\tNo_Prog: 15\n",
      "18 Epochs:\tVal Loss: 0.168687,\tBest_Loss:  0.063651,\tAcc:  98.749%\tNo_Prog: 16\n",
      "19 Epochs:\tVal Loss: 0.193618,\tBest_Loss:  0.063651,\tAcc:  98.475%\tNo_Prog: 17\n",
      "20 Epochs:\tVal Loss: 0.318421,\tBest_Loss:  0.063651,\tAcc:  97.928%\tNo_Prog: 18\n",
      "21 Epochs:\tVal Loss: 0.279759,\tBest_Loss:  0.063651,\tAcc:  97.654%\tNo_Prog: 19\n",
      "22 Epochs:\tVal Loss: 0.218363,\tBest_Loss:  0.063651,\tAcc:  98.827%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 23 epochs and 4464 steps\n",
      "Best Validation Accuracy: 98.5927%\n",
      "[CV]  n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198A58DEF28>, total=  14.7s\n",
      "[CV] n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198A58DEF28> \n",
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=100, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.076457,\tBest_Loss:  0.076457,\tAcc:  97.615%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.055282,\tBest_Loss:  0.055282,\tAcc:  98.514%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.070288,\tBest_Loss:  0.055282,\tAcc:  98.514%\tNo_Prog: 1\n",
      "3 Epochs:\tVal Loss: 0.074329,\tBest_Loss:  0.055282,\tAcc:  97.889%\tNo_Prog: 2\n",
      "4 Epochs:\tVal Loss: 0.065496,\tBest_Loss:  0.055282,\tAcc:  98.280%\tNo_Prog: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Epochs:\tVal Loss: 0.073874,\tBest_Loss:  0.055282,\tAcc:  98.319%\tNo_Prog: 4\n",
      "6 Epochs:\tVal Loss: 0.113012,\tBest_Loss:  0.055282,\tAcc:  97.654%\tNo_Prog: 5\n",
      "7 Epochs:\tVal Loss: 0.111854,\tBest_Loss:  0.055282,\tAcc:  98.241%\tNo_Prog: 6\n",
      "8 Epochs:\tVal Loss: 0.072985,\tBest_Loss:  0.055282,\tAcc:  98.358%\tNo_Prog: 7\n",
      "9 Epochs:\tVal Loss: 0.070998,\tBest_Loss:  0.055282,\tAcc:  98.475%\tNo_Prog: 8\n",
      "10 Epochs:\tVal Loss: 0.096926,\tBest_Loss:  0.055282,\tAcc:  98.280%\tNo_Prog: 9\n",
      "11 Epochs:\tVal Loss: 0.102898,\tBest_Loss:  0.055282,\tAcc:  98.436%\tNo_Prog: 10\n",
      "12 Epochs:\tVal Loss: 0.109449,\tBest_Loss:  0.055282,\tAcc:  98.319%\tNo_Prog: 11\n",
      "13 Epochs:\tVal Loss: 0.195180,\tBest_Loss:  0.055282,\tAcc:  97.811%\tNo_Prog: 12\n",
      "14 Epochs:\tVal Loss: 0.109879,\tBest_Loss:  0.055282,\tAcc:  98.554%\tNo_Prog: 13\n",
      "15 Epochs:\tVal Loss: 0.096795,\tBest_Loss:  0.055282,\tAcc:  98.593%\tNo_Prog: 14\n",
      "16 Epochs:\tVal Loss: 0.135313,\tBest_Loss:  0.055282,\tAcc:  98.514%\tNo_Prog: 15\n",
      "17 Epochs:\tVal Loss: 0.158006,\tBest_Loss:  0.055282,\tAcc:  98.241%\tNo_Prog: 16\n",
      "18 Epochs:\tVal Loss: 0.114913,\tBest_Loss:  0.055282,\tAcc:  98.593%\tNo_Prog: 17\n",
      "19 Epochs:\tVal Loss: 0.121134,\tBest_Loss:  0.055282,\tAcc:  98.436%\tNo_Prog: 18\n",
      "20 Epochs:\tVal Loss: 0.125632,\tBest_Loss:  0.055282,\tAcc:  98.554%\tNo_Prog: 19\n",
      "21 Epochs:\tVal Loss: 0.136554,\tBest_Loss:  0.055282,\tAcc:  98.671%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 22 epochs and 4278 steps\n",
      "Best Validation Accuracy: 98.5145%\n",
      "[CV]  n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198A58DEF28>, total=  14.4s\n",
      "[CV] n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198A58DEF28> \n",
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=100, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.054173,\tBest_Loss:  0.054173,\tAcc:  98.436%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.081466,\tBest_Loss:  0.054173,\tAcc:  97.811%\tNo_Prog: 1\n",
      "2 Epochs:\tVal Loss: 0.102204,\tBest_Loss:  0.054173,\tAcc:  97.342%\tNo_Prog: 2\n",
      "3 Epochs:\tVal Loss: 0.059648,\tBest_Loss:  0.054173,\tAcc:  98.241%\tNo_Prog: 3\n",
      "4 Epochs:\tVal Loss: 0.092690,\tBest_Loss:  0.054173,\tAcc:  97.889%\tNo_Prog: 4\n",
      "5 Epochs:\tVal Loss: 0.059168,\tBest_Loss:  0.054173,\tAcc:  98.514%\tNo_Prog: 5\n",
      "6 Epochs:\tVal Loss: 0.041876,\tBest_Loss:  0.041876,\tAcc:  98.905%\tNo_Prog: 0\n",
      "7 Epochs:\tVal Loss: 0.057381,\tBest_Loss:  0.041876,\tAcc:  98.671%\tNo_Prog: 1\n",
      "8 Epochs:\tVal Loss: 0.062839,\tBest_Loss:  0.041876,\tAcc:  98.632%\tNo_Prog: 2\n",
      "9 Epochs:\tVal Loss: 0.084203,\tBest_Loss:  0.041876,\tAcc:  98.749%\tNo_Prog: 3\n",
      "10 Epochs:\tVal Loss: 0.148017,\tBest_Loss:  0.041876,\tAcc:  97.811%\tNo_Prog: 4\n",
      "11 Epochs:\tVal Loss: 0.086935,\tBest_Loss:  0.041876,\tAcc:  98.632%\tNo_Prog: 5\n",
      "12 Epochs:\tVal Loss: 0.089835,\tBest_Loss:  0.041876,\tAcc:  98.397%\tNo_Prog: 6\n",
      "13 Epochs:\tVal Loss: 0.185065,\tBest_Loss:  0.041876,\tAcc:  97.615%\tNo_Prog: 7\n",
      "14 Epochs:\tVal Loss: 0.181193,\tBest_Loss:  0.041876,\tAcc:  98.045%\tNo_Prog: 8\n",
      "15 Epochs:\tVal Loss: 0.154601,\tBest_Loss:  0.041876,\tAcc:  98.202%\tNo_Prog: 9\n",
      "16 Epochs:\tVal Loss: 0.181751,\tBest_Loss:  0.041876,\tAcc:  98.358%\tNo_Prog: 10\n",
      "17 Epochs:\tVal Loss: 0.141191,\tBest_Loss:  0.041876,\tAcc:  98.436%\tNo_Prog: 11\n",
      "18 Epochs:\tVal Loss: 0.110100,\tBest_Loss:  0.041876,\tAcc:  98.788%\tNo_Prog: 12\n",
      "19 Epochs:\tVal Loss: 0.137267,\tBest_Loss:  0.041876,\tAcc:  98.710%\tNo_Prog: 13\n",
      "20 Epochs:\tVal Loss: 0.201020,\tBest_Loss:  0.041876,\tAcc:  97.615%\tNo_Prog: 14\n",
      "21 Epochs:\tVal Loss: 0.201118,\tBest_Loss:  0.041876,\tAcc:  98.514%\tNo_Prog: 15\n",
      "22 Epochs:\tVal Loss: 0.241831,\tBest_Loss:  0.041876,\tAcc:  98.397%\tNo_Prog: 16\n",
      "23 Epochs:\tVal Loss: 0.236063,\tBest_Loss:  0.041876,\tAcc:  98.084%\tNo_Prog: 17\n",
      "24 Epochs:\tVal Loss: 0.151163,\tBest_Loss:  0.041876,\tAcc:  99.023%\tNo_Prog: 18\n",
      "25 Epochs:\tVal Loss: 0.199027,\tBest_Loss:  0.041876,\tAcc:  98.827%\tNo_Prog: 19\n",
      "26 Epochs:\tVal Loss: 0.168424,\tBest_Loss:  0.041876,\tAcc:  98.788%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 27 epochs and 5208 steps\n",
      "Best Validation Accuracy: 98.9054%\n",
      "[CV]  n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198A58DEF28>, total=  18.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=500, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "0 Epochs:\tVal Loss: 0.061344,\tBest_Loss:  0.061344,\tAcc:  98.045%\tNo_Prog: 0\n",
      "1 Epochs:\tVal Loss: 0.050911,\tBest_Loss:  0.050911,\tAcc:  98.397%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.033912,\tBest_Loss:  0.033912,\tAcc:  98.827%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.035496,\tBest_Loss:  0.033912,\tAcc:  98.788%\tNo_Prog: 1\n",
      "4 Epochs:\tVal Loss: 0.028169,\tBest_Loss:  0.028169,\tAcc:  98.905%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.030837,\tBest_Loss:  0.028169,\tAcc:  99.140%\tNo_Prog: 1\n",
      "6 Epochs:\tVal Loss: 0.027609,\tBest_Loss:  0.027609,\tAcc:  98.905%\tNo_Prog: 0\n",
      "7 Epochs:\tVal Loss: 0.026978,\tBest_Loss:  0.026978,\tAcc:  99.101%\tNo_Prog: 0\n",
      "8 Epochs:\tVal Loss: 0.030228,\tBest_Loss:  0.026978,\tAcc:  98.944%\tNo_Prog: 1\n",
      "9 Epochs:\tVal Loss: 0.025018,\tBest_Loss:  0.025018,\tAcc:  99.101%\tNo_Prog: 0\n",
      "10 Epochs:\tVal Loss: 0.026011,\tBest_Loss:  0.025018,\tAcc:  99.179%\tNo_Prog: 1\n",
      "11 Epochs:\tVal Loss: 0.028117,\tBest_Loss:  0.025018,\tAcc:  99.140%\tNo_Prog: 2\n",
      "12 Epochs:\tVal Loss: 0.024500,\tBest_Loss:  0.024500,\tAcc:  99.257%\tNo_Prog: 0\n",
      "13 Epochs:\tVal Loss: 0.024301,\tBest_Loss:  0.024301,\tAcc:  99.179%\tNo_Prog: 0\n",
      "14 Epochs:\tVal Loss: 0.024154,\tBest_Loss:  0.024154,\tAcc:  99.179%\tNo_Prog: 0\n",
      "15 Epochs:\tVal Loss: 0.023790,\tBest_Loss:  0.023790,\tAcc:  99.218%\tNo_Prog: 0\n",
      "16 Epochs:\tVal Loss: 0.024665,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 1\n",
      "17 Epochs:\tVal Loss: 0.024610,\tBest_Loss:  0.023790,\tAcc:  99.218%\tNo_Prog: 2\n",
      "18 Epochs:\tVal Loss: 0.024697,\tBest_Loss:  0.023790,\tAcc:  99.218%\tNo_Prog: 3\n",
      "19 Epochs:\tVal Loss: 0.024419,\tBest_Loss:  0.023790,\tAcc:  99.218%\tNo_Prog: 4\n",
      "20 Epochs:\tVal Loss: 0.024964,\tBest_Loss:  0.023790,\tAcc:  99.179%\tNo_Prog: 5\n",
      "21 Epochs:\tVal Loss: 0.025303,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 6\n",
      "22 Epochs:\tVal Loss: 0.024835,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 7\n",
      "23 Epochs:\tVal Loss: 0.025059,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 8\n",
      "24 Epochs:\tVal Loss: 0.025045,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 9\n",
      "25 Epochs:\tVal Loss: 0.025217,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 10\n",
      "26 Epochs:\tVal Loss: 0.025174,\tBest_Loss:  0.023790,\tAcc:  99.296%\tNo_Prog: 11\n",
      "27 Epochs:\tVal Loss: 0.024953,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 12\n",
      "28 Epochs:\tVal Loss: 0.025371,\tBest_Loss:  0.023790,\tAcc:  99.296%\tNo_Prog: 13\n",
      "29 Epochs:\tVal Loss: 0.025285,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 14\n",
      "30 Epochs:\tVal Loss: 0.025685,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 15\n",
      "31 Epochs:\tVal Loss: 0.025608,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 16\n",
      "32 Epochs:\tVal Loss: 0.025630,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 17\n",
      "33 Epochs:\tVal Loss: 0.025806,\tBest_Loss:  0.023790,\tAcc:  99.296%\tNo_Prog: 18\n",
      "34 Epochs:\tVal Loss: 0.025804,\tBest_Loss:  0.023790,\tAcc:  99.296%\tNo_Prog: 19\n",
      "35 Epochs:\tVal Loss: 0.025843,\tBest_Loss:  0.023790,\tAcc:  99.257%\tNo_Prog: 20\n",
      "\n",
      "Model stopped EARLY at 36 epochs and 2072 steps\n",
      "Best Validation Accuracy: 99.2181%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=MikesGloriousDNNClassifier(activation_function=<function elu at 0x000001989B4C40D0>,\n",
       "              batch_norm_momentum_decay=None, batch_size=50,\n",
       "              dropout_rate=None,\n",
       "              initializer=<function variance_scaling_initializer.<locals>._initializer at 0x00000198A51552F0>,\n",
       "...izer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=None),\n",
       "          fit_params={'X_valid': array([[ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.]], dtype=float32), 'y_valid': array([0, 4, ..., 1, 2], dtype=uint8), 'n_epochs': 100},\n",
       "          iid=True, n_iter=3, n_jobs=1,\n",
       "          param_distributions={'n_hidden_layers': [5, 10, 15], 'n_neurons': [50, 100, 150], 'learning_rate': [0.01, 0.02], 'batch_size': [100, 500], 'activation_function': [<function relu at 0x000001989B4C6268>, <function elu at 0x000001989B4C40D0>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198A58DEF28>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000198ACE85620>]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#define leaky relu funciton... because for some reason its not build into TF!\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha*z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "params = {\n",
    "    \"n_hidden_layers\":[5,10,15],\n",
    "    \"n_neurons\":[50,100,150],\n",
    "    \"learning_rate\":[0.01,0.02],\n",
    "    \"batch_size\": [100, 500],\n",
    "    \"activation_function\":[tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01),leaky_relu(alpha=0.02)]\n",
    "}\n",
    "rand_search = RandomizedSearchCV(MikesGloriousDNNClassifier(), params, n_iter=3, random_state=42, verbose=2, fit_params={\"X_valid\":X_valid1, \"y_valid\":y_valid1, \"n_epochs\":100})\n",
    "rand_search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neurons': 100, 'n_hidden_layers': 5, 'learning_rate': 0.01, 'batch_size': 500, 'activation_function': <function relu at 0x000001989B4C6268>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99318933644677954"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what are the best params\n",
    "print(rand_search.best_params_)\n",
    "\n",
    "#print accruacy\n",
    "y_pred = rand_search.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NO WAY!!! Leaky Relu actually trained the best model... thats pretty unexpected. But just goes to show how its important to test a bunch of different hyperparams to find the best result. \n",
    "\n",
    "Our increase in accuracy of 1% means that our error rate when from roughly 2% to 1%... **which is a 50% reduction in errors!!!!**\n",
    "\n",
    "#### Let's save this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_search.best_estimator_.save(\"./tf_logs/my_best_DNN_0_to_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce 5 better model?\n",
    "\n",
    "Great. Now lets train this awesome model, and then train it WITH batch normalizaiton, and see if it converges faster. Use tensor board to look at training speeds!\n",
    "\n",
    "First rerun the best model using the best_params from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=1000,  batch_size=500, early_stop:True, batch_norm:False, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.072347,\tBest_Loss:  0.072347,\tAcc:  97.772%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.058847,\tBest_Loss:  0.058847,\tAcc:  98.358%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.043963,\tBest_Loss:  0.043963,\tAcc:  98.710%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.044730,\tBest_Loss:  0.043963,\tAcc:  98.593%\tNo_Prog: 1\n",
      "5 Epochs:\tVal Loss: 0.047267,\tBest_Loss:  0.043963,\tAcc:  98.632%\tNo_Prog: 2\n",
      "6 Epochs:\tVal Loss: 0.037500,\tBest_Loss:  0.037500,\tAcc:  99.023%\tNo_Prog: 0\n",
      "7 Epochs:\tVal Loss: 0.040106,\tBest_Loss:  0.037500,\tAcc:  98.827%\tNo_Prog: 1\n",
      "8 Epochs:\tVal Loss: 0.037894,\tBest_Loss:  0.037500,\tAcc:  99.140%\tNo_Prog: 2\n",
      "9 Epochs:\tVal Loss: 0.042804,\tBest_Loss:  0.037500,\tAcc:  98.632%\tNo_Prog: 3\n",
      "10 Epochs:\tVal Loss: 0.041971,\tBest_Loss:  0.037500,\tAcc:  98.866%\tNo_Prog: 4\n",
      "11 Epochs:\tVal Loss: 0.038344,\tBest_Loss:  0.037500,\tAcc:  98.905%\tNo_Prog: 5\n",
      "12 Epochs:\tVal Loss: 0.031193,\tBest_Loss:  0.031193,\tAcc:  99.257%\tNo_Prog: 0\n",
      "13 Epochs:\tVal Loss: 0.051970,\tBest_Loss:  0.031193,\tAcc:  98.984%\tNo_Prog: 1\n",
      "14 Epochs:\tVal Loss: 0.062431,\tBest_Loss:  0.031193,\tAcc:  98.436%\tNo_Prog: 2\n",
      "15 Epochs:\tVal Loss: 0.054478,\tBest_Loss:  0.031193,\tAcc:  99.023%\tNo_Prog: 3\n",
      "16 Epochs:\tVal Loss: 0.039100,\tBest_Loss:  0.031193,\tAcc:  99.179%\tNo_Prog: 4\n",
      "17 Epochs:\tVal Loss: 0.038451,\tBest_Loss:  0.031193,\tAcc:  99.140%\tNo_Prog: 5\n",
      "18 Epochs:\tVal Loss: 0.045334,\tBest_Loss:  0.031193,\tAcc:  98.984%\tNo_Prog: 6\n",
      "19 Epochs:\tVal Loss: 0.039331,\tBest_Loss:  0.031193,\tAcc:  98.984%\tNo_Prog: 7\n",
      "20 Epochs:\tVal Loss: 0.045153,\tBest_Loss:  0.031193,\tAcc:  98.984%\tNo_Prog: 8\n",
      "21 Epochs:\tVal Loss: 0.044470,\tBest_Loss:  0.031193,\tAcc:  99.023%\tNo_Prog: 9\n",
      "22 Epochs:\tVal Loss: 0.033368,\tBest_Loss:  0.031193,\tAcc:  99.257%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 23 epochs and 1288 steps\n",
      "Best Validation Accuracy: 99.2572%\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "model1 = MikesGloriousDNNClassifier(activation_function=leaky_relu(alpha=0.01),\n",
    "                                     n_neurons=100,\n",
    "                                     n_hidden_layers=5,\n",
    "                                     learning_rate=0.01,\n",
    "                                     batch_size=500,\n",
    "                                     tb_model_name='model1')\n",
    " \n",
    "model1.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)\n",
    "\n",
    "model1.save(\"./tf_logs/MODEL1_0_to_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK so we seemed to have reached out optimal model at epoch 9. Thats pretty good I guess... Lets check the accuracy, see if its still as good as before... it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99241097489784003"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print accruacy\n",
    "y_pred = model1.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dope its still performing well on the test set. Now lets add in some Batch Normalization and see if the mean_loss minimizes faster... lets hand it over to TB (assuming our TensorBoard code tweaks worked...)!\n",
    "\n",
    "    (C:\\Users\\mciniello\\AppData\\Local\\Continuum\\anaconda3) C:\\Users\\mciniello\\Desktop\\Python\\Updated projects>python -m tensorflow.tensorboard --logdir DNN1_LOGS\\\n",
    "    Starting TensorBoard b'54' at http://CA47496-MCINI05:6006\n",
    "    (Press CTRL+C to quit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=1000,  batch_size=500, early_stop:True, batch_norm:True, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.086293,\tBest_Loss:  0.086293,\tAcc:  97.342%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.053649,\tBest_Loss:  0.053649,\tAcc:  98.358%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.051087,\tBest_Loss:  0.051087,\tAcc:  98.280%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.041546,\tBest_Loss:  0.041546,\tAcc:  98.593%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.040599,\tBest_Loss:  0.040599,\tAcc:  98.514%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.032538,\tBest_Loss:  0.032538,\tAcc:  98.710%\tNo_Prog: 0\n",
      "7 Epochs:\tVal Loss: 0.036113,\tBest_Loss:  0.032538,\tAcc:  98.554%\tNo_Prog: 1\n",
      "8 Epochs:\tVal Loss: 0.033160,\tBest_Loss:  0.032538,\tAcc:  98.905%\tNo_Prog: 2\n",
      "9 Epochs:\tVal Loss: 0.030912,\tBest_Loss:  0.030912,\tAcc:  99.023%\tNo_Prog: 0\n",
      "10 Epochs:\tVal Loss: 0.030655,\tBest_Loss:  0.030655,\tAcc:  99.218%\tNo_Prog: 0\n",
      "11 Epochs:\tVal Loss: 0.030828,\tBest_Loss:  0.030655,\tAcc:  99.062%\tNo_Prog: 1\n",
      "12 Epochs:\tVal Loss: 0.030889,\tBest_Loss:  0.030655,\tAcc:  98.905%\tNo_Prog: 2\n",
      "13 Epochs:\tVal Loss: 0.030679,\tBest_Loss:  0.030655,\tAcc:  98.944%\tNo_Prog: 3\n",
      "14 Epochs:\tVal Loss: 0.030346,\tBest_Loss:  0.030346,\tAcc:  99.023%\tNo_Prog: 0\n",
      "15 Epochs:\tVal Loss: 0.030214,\tBest_Loss:  0.030214,\tAcc:  98.984%\tNo_Prog: 0\n",
      "16 Epochs:\tVal Loss: 0.030426,\tBest_Loss:  0.030214,\tAcc:  98.984%\tNo_Prog: 1\n",
      "17 Epochs:\tVal Loss: 0.030856,\tBest_Loss:  0.030214,\tAcc:  98.944%\tNo_Prog: 2\n",
      "18 Epochs:\tVal Loss: 0.035016,\tBest_Loss:  0.030214,\tAcc:  98.866%\tNo_Prog: 3\n",
      "19 Epochs:\tVal Loss: 0.031233,\tBest_Loss:  0.030214,\tAcc:  99.023%\tNo_Prog: 4\n",
      "20 Epochs:\tVal Loss: 0.032311,\tBest_Loss:  0.030214,\tAcc:  98.944%\tNo_Prog: 5\n",
      "21 Epochs:\tVal Loss: 0.031723,\tBest_Loss:  0.030214,\tAcc:  99.023%\tNo_Prog: 6\n",
      "22 Epochs:\tVal Loss: 0.030732,\tBest_Loss:  0.030214,\tAcc:  99.101%\tNo_Prog: 7\n",
      "23 Epochs:\tVal Loss: 0.031632,\tBest_Loss:  0.030214,\tAcc:  99.023%\tNo_Prog: 8\n",
      "24 Epochs:\tVal Loss: 0.032417,\tBest_Loss:  0.030214,\tAcc:  99.023%\tNo_Prog: 9\n",
      "25 Epochs:\tVal Loss: 0.032069,\tBest_Loss:  0.030214,\tAcc:  99.062%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 26 epochs and 1456 steps\n",
      "Best Validation Accuracy: 98.9836%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MikesGloriousDNNClassifier(activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290C18CC80>,\n",
       "              batch_norm_momentum_decay=0.99, batch_size=500,\n",
       "              dropout_rate=None,\n",
       "              initializer=<function variance_scaling_initializer.<locals>._initializer at 0x000002290BFA3950>,\n",
       "              k_in_top_val=1, layer_name='hidden', learning_rate=0.01,\n",
       "              max_epochs_no_progress=10, n_hidden_layers=5,\n",
       "              n_neurons=array([100, 100, 100, 100, 100]),\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=None, tb_model_name='model1_bn',\n",
       "              tf_logs_path='./DNN1_LOGS')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "dnn_clf = MikesGloriousDNNClassifier(activation_function=leaky_relu(alpha=0.01),\n",
    "                                     n_neurons=100,\n",
    "                                     n_hidden_layers=5,\n",
    "                                     learning_rate=0.01,\n",
    "                                     batch_size=500, \n",
    "                                     batch_norm_momentum_decay = 0.99, \n",
    "                                     tb_model_name='model1_bn')\n",
    " \n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99338392683401444"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print accruacy\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hmmmm we aren't converging any faster, and our accuracy is a little bit lower when we use batch normalization on the exact same model... interesting. \n",
    "\n",
    "![](pictures/MC_Project - DNN_1.png)\n",
    "\n",
    "### Sooo lets try optimizing the params WITH batch normalization... maybe that will help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mciniello\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:584: DeprecationWarning: \"fit_params\" as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the \"fit\" method instead.\n",
      "  '\"fit\" method instead.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0> \n",
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=100, early_stop:True, batch_norm:True, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.065194,\tBest_Loss:  0.065194,\tAcc:  97.850%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.048834,\tBest_Loss:  0.048834,\tAcc:  98.475%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.073980,\tBest_Loss:  0.048834,\tAcc:  97.420%\tNo_Prog: 1\n",
      "4 Epochs:\tVal Loss: 0.055525,\tBest_Loss:  0.048834,\tAcc:  98.280%\tNo_Prog: 2\n",
      "5 Epochs:\tVal Loss: 0.050570,\tBest_Loss:  0.048834,\tAcc:  98.554%\tNo_Prog: 3\n",
      "6 Epochs:\tVal Loss: 0.050438,\tBest_Loss:  0.048834,\tAcc:  98.749%\tNo_Prog: 4\n",
      "7 Epochs:\tVal Loss: 0.037595,\tBest_Loss:  0.037595,\tAcc:  98.749%\tNo_Prog: 0\n",
      "8 Epochs:\tVal Loss: 0.047837,\tBest_Loss:  0.037595,\tAcc:  98.710%\tNo_Prog: 1\n",
      "9 Epochs:\tVal Loss: 0.162833,\tBest_Loss:  0.037595,\tAcc:  96.716%\tNo_Prog: 2\n",
      "10 Epochs:\tVal Loss: 0.059306,\tBest_Loss:  0.037595,\tAcc:  98.671%\tNo_Prog: 3\n",
      "11 Epochs:\tVal Loss: 0.047068,\tBest_Loss:  0.037595,\tAcc:  98.944%\tNo_Prog: 4\n",
      "12 Epochs:\tVal Loss: 0.049001,\tBest_Loss:  0.037595,\tAcc:  98.827%\tNo_Prog: 5\n",
      "13 Epochs:\tVal Loss: 0.045411,\tBest_Loss:  0.037595,\tAcc:  99.101%\tNo_Prog: 6\n",
      "14 Epochs:\tVal Loss: 0.035429,\tBest_Loss:  0.035429,\tAcc:  99.335%\tNo_Prog: 0\n",
      "15 Epochs:\tVal Loss: 0.042175,\tBest_Loss:  0.035429,\tAcc:  99.257%\tNo_Prog: 1\n",
      "16 Epochs:\tVal Loss: 0.097706,\tBest_Loss:  0.035429,\tAcc:  98.319%\tNo_Prog: 2\n",
      "17 Epochs:\tVal Loss: 0.051980,\tBest_Loss:  0.035429,\tAcc:  99.062%\tNo_Prog: 3\n",
      "18 Epochs:\tVal Loss: 0.049132,\tBest_Loss:  0.035429,\tAcc:  99.062%\tNo_Prog: 4\n",
      "19 Epochs:\tVal Loss: 0.062329,\tBest_Loss:  0.035429,\tAcc:  98.827%\tNo_Prog: 5\n",
      "20 Epochs:\tVal Loss: 0.045038,\tBest_Loss:  0.035429,\tAcc:  98.749%\tNo_Prog: 6\n",
      "21 Epochs:\tVal Loss: 0.039533,\tBest_Loss:  0.035429,\tAcc:  99.335%\tNo_Prog: 7\n",
      "22 Epochs:\tVal Loss: 0.048658,\tBest_Loss:  0.035429,\tAcc:  99.140%\tNo_Prog: 8\n",
      "23 Epochs:\tVal Loss: 0.040779,\tBest_Loss:  0.035429,\tAcc:  99.218%\tNo_Prog: 9\n",
      "24 Epochs:\tVal Loss: 0.045738,\tBest_Loss:  0.035429,\tAcc:  99.023%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 25 epochs and 4650 steps\n",
      "Best Validation Accuracy: 99.3354%\n",
      "[CV]  n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0>, total=  28.2s\n",
      "[CV] n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   28.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=100, early_stop:True, batch_norm:True, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.056916,\tBest_Loss:  0.056916,\tAcc:  98.084%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.056654,\tBest_Loss:  0.056654,\tAcc:  98.241%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.064256,\tBest_Loss:  0.056654,\tAcc:  98.163%\tNo_Prog: 1\n",
      "4 Epochs:\tVal Loss: 0.052294,\tBest_Loss:  0.052294,\tAcc:  98.593%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.043086,\tBest_Loss:  0.043086,\tAcc:  98.749%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.049133,\tBest_Loss:  0.043086,\tAcc:  98.749%\tNo_Prog: 1\n",
      "7 Epochs:\tVal Loss: 0.057457,\tBest_Loss:  0.043086,\tAcc:  98.710%\tNo_Prog: 2\n",
      "8 Epochs:\tVal Loss: 0.041383,\tBest_Loss:  0.041383,\tAcc:  98.788%\tNo_Prog: 0\n",
      "9 Epochs:\tVal Loss: 0.074417,\tBest_Loss:  0.041383,\tAcc:  98.358%\tNo_Prog: 1\n",
      "10 Epochs:\tVal Loss: 0.056540,\tBest_Loss:  0.041383,\tAcc:  98.827%\tNo_Prog: 2\n",
      "11 Epochs:\tVal Loss: 0.074298,\tBest_Loss:  0.041383,\tAcc:  98.593%\tNo_Prog: 3\n",
      "12 Epochs:\tVal Loss: 0.039476,\tBest_Loss:  0.039476,\tAcc:  98.905%\tNo_Prog: 0\n",
      "13 Epochs:\tVal Loss: 0.041497,\tBest_Loss:  0.039476,\tAcc:  98.827%\tNo_Prog: 1\n",
      "14 Epochs:\tVal Loss: 0.055765,\tBest_Loss:  0.039476,\tAcc:  98.866%\tNo_Prog: 2\n",
      "15 Epochs:\tVal Loss: 0.054812,\tBest_Loss:  0.039476,\tAcc:  98.788%\tNo_Prog: 3\n",
      "16 Epochs:\tVal Loss: 0.086275,\tBest_Loss:  0.039476,\tAcc:  98.319%\tNo_Prog: 4\n",
      "17 Epochs:\tVal Loss: 0.072674,\tBest_Loss:  0.039476,\tAcc:  98.749%\tNo_Prog: 5\n",
      "18 Epochs:\tVal Loss: 0.042575,\tBest_Loss:  0.039476,\tAcc:  99.023%\tNo_Prog: 6\n",
      "19 Epochs:\tVal Loss: 0.042604,\tBest_Loss:  0.039476,\tAcc:  99.218%\tNo_Prog: 7\n",
      "20 Epochs:\tVal Loss: 0.051921,\tBest_Loss:  0.039476,\tAcc:  98.984%\tNo_Prog: 8\n",
      "21 Epochs:\tVal Loss: 0.036944,\tBest_Loss:  0.036944,\tAcc:  99.023%\tNo_Prog: 0\n",
      "22 Epochs:\tVal Loss: 0.050807,\tBest_Loss:  0.036944,\tAcc:  99.023%\tNo_Prog: 1\n",
      "23 Epochs:\tVal Loss: 0.053639,\tBest_Loss:  0.036944,\tAcc:  99.140%\tNo_Prog: 2\n",
      "24 Epochs:\tVal Loss: 0.065019,\tBest_Loss:  0.036944,\tAcc:  98.749%\tNo_Prog: 3\n",
      "25 Epochs:\tVal Loss: 0.084231,\tBest_Loss:  0.036944,\tAcc:  98.632%\tNo_Prog: 4\n",
      "26 Epochs:\tVal Loss: 0.064214,\tBest_Loss:  0.036944,\tAcc:  98.593%\tNo_Prog: 5\n",
      "27 Epochs:\tVal Loss: 0.068906,\tBest_Loss:  0.036944,\tAcc:  98.827%\tNo_Prog: 6\n",
      "28 Epochs:\tVal Loss: 0.052979,\tBest_Loss:  0.036944,\tAcc:  98.905%\tNo_Prog: 7\n",
      "29 Epochs:\tVal Loss: 0.053369,\tBest_Loss:  0.036944,\tAcc:  99.023%\tNo_Prog: 8\n",
      "30 Epochs:\tVal Loss: 0.044253,\tBest_Loss:  0.036944,\tAcc:  99.101%\tNo_Prog: 9\n",
      "31 Epochs:\tVal Loss: 0.052699,\tBest_Loss:  0.036944,\tAcc:  99.140%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 32 epochs and 5952 steps\n",
      "Best Validation Accuracy: 99.0227%\n",
      "[CV]  n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0>, total=  33.9s\n",
      "[CV] n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0> \n",
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=100,  batch_size=100, early_stop:True, batch_norm:True, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.059249,\tBest_Loss:  0.059249,\tAcc:  98.319%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.047391,\tBest_Loss:  0.047391,\tAcc:  98.514%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.059710,\tBest_Loss:  0.047391,\tAcc:  98.280%\tNo_Prog: 1\n",
      "4 Epochs:\tVal Loss: 0.040510,\tBest_Loss:  0.040510,\tAcc:  98.632%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.054746,\tBest_Loss:  0.040510,\tAcc:  98.436%\tNo_Prog: 1\n",
      "6 Epochs:\tVal Loss: 0.070338,\tBest_Loss:  0.040510,\tAcc:  98.475%\tNo_Prog: 2\n",
      "7 Epochs:\tVal Loss: 0.050477,\tBest_Loss:  0.040510,\tAcc:  98.554%\tNo_Prog: 3\n",
      "8 Epochs:\tVal Loss: 0.044742,\tBest_Loss:  0.040510,\tAcc:  98.866%\tNo_Prog: 4\n",
      "9 Epochs:\tVal Loss: 0.049164,\tBest_Loss:  0.040510,\tAcc:  98.905%\tNo_Prog: 5\n",
      "10 Epochs:\tVal Loss: 0.053272,\tBest_Loss:  0.040510,\tAcc:  98.866%\tNo_Prog: 6\n",
      "11 Epochs:\tVal Loss: 0.040253,\tBest_Loss:  0.040253,\tAcc:  98.944%\tNo_Prog: 0\n",
      "12 Epochs:\tVal Loss: 0.062892,\tBest_Loss:  0.040253,\tAcc:  98.593%\tNo_Prog: 1\n",
      "13 Epochs:\tVal Loss: 0.054343,\tBest_Loss:  0.040253,\tAcc:  98.827%\tNo_Prog: 2\n",
      "14 Epochs:\tVal Loss: 0.086220,\tBest_Loss:  0.040253,\tAcc:  98.475%\tNo_Prog: 3\n",
      "15 Epochs:\tVal Loss: 0.051816,\tBest_Loss:  0.040253,\tAcc:  99.101%\tNo_Prog: 4\n",
      "16 Epochs:\tVal Loss: 0.071631,\tBest_Loss:  0.040253,\tAcc:  98.944%\tNo_Prog: 5\n",
      "17 Epochs:\tVal Loss: 0.080062,\tBest_Loss:  0.040253,\tAcc:  98.514%\tNo_Prog: 6\n",
      "18 Epochs:\tVal Loss: 0.059896,\tBest_Loss:  0.040253,\tAcc:  98.905%\tNo_Prog: 7\n",
      "19 Epochs:\tVal Loss: 0.047018,\tBest_Loss:  0.040253,\tAcc:  98.866%\tNo_Prog: 8\n",
      "20 Epochs:\tVal Loss: 0.063933,\tBest_Loss:  0.040253,\tAcc:  98.827%\tNo_Prog: 9\n",
      "21 Epochs:\tVal Loss: 0.069350,\tBest_Loss:  0.040253,\tAcc:  98.749%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 22 epochs and 4092 steps\n",
      "Best Validation Accuracy: 98.9445%\n",
      "[CV]  n_neurons=100, n_hidden_layers=5, learning_rate=0.02, batch_size=100, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0>, total=  25.5s\n",
      "[CV] n_neurons=100, n_hidden_layers=10, learning_rate=0.01, batch_size=200, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0> \n",
      "Training DNN: \n",
      "layers=10, neurons=[100 100 100 100 100 100 100 100 100 100], epochs=100,  batch_size=200, early_stop:True, batch_norm:True, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.065768,\tBest_Loss:  0.065768,\tAcc:  98.124%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.070516,\tBest_Loss:  0.065768,\tAcc:  97.694%\tNo_Prog: 1\n",
      "3 Epochs:\tVal Loss: 0.051901,\tBest_Loss:  0.051901,\tAcc:  98.202%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.073980,\tBest_Loss:  0.051901,\tAcc:  98.280%\tNo_Prog: 1\n",
      "5 Epochs:\tVal Loss: 0.040883,\tBest_Loss:  0.040883,\tAcc:  98.788%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.037189,\tBest_Loss:  0.037189,\tAcc:  98.866%\tNo_Prog: 0\n",
      "7 Epochs:\tVal Loss: 0.035593,\tBest_Loss:  0.035593,\tAcc:  98.866%\tNo_Prog: 0\n",
      "8 Epochs:\tVal Loss: 0.049625,\tBest_Loss:  0.035593,\tAcc:  98.514%\tNo_Prog: 1\n",
      "9 Epochs:\tVal Loss: 0.050221,\tBest_Loss:  0.035593,\tAcc:  98.710%\tNo_Prog: 2\n",
      "10 Epochs:\tVal Loss: 0.046281,\tBest_Loss:  0.035593,\tAcc:  98.827%\tNo_Prog: 3\n",
      "11 Epochs:\tVal Loss: 0.045490,\tBest_Loss:  0.035593,\tAcc:  98.827%\tNo_Prog: 4\n",
      "12 Epochs:\tVal Loss: 0.043046,\tBest_Loss:  0.035593,\tAcc:  98.984%\tNo_Prog: 5\n",
      "13 Epochs:\tVal Loss: 0.035446,\tBest_Loss:  0.035446,\tAcc:  98.905%\tNo_Prog: 0\n",
      "14 Epochs:\tVal Loss: 0.041877,\tBest_Loss:  0.035446,\tAcc:  98.944%\tNo_Prog: 1\n",
      "15 Epochs:\tVal Loss: 0.040041,\tBest_Loss:  0.035446,\tAcc:  99.062%\tNo_Prog: 2\n",
      "16 Epochs:\tVal Loss: 0.041005,\tBest_Loss:  0.035446,\tAcc:  98.905%\tNo_Prog: 3\n",
      "17 Epochs:\tVal Loss: 0.040064,\tBest_Loss:  0.035446,\tAcc:  99.062%\tNo_Prog: 4\n",
      "18 Epochs:\tVal Loss: 0.037439,\tBest_Loss:  0.035446,\tAcc:  99.101%\tNo_Prog: 5\n",
      "19 Epochs:\tVal Loss: 0.037616,\tBest_Loss:  0.035446,\tAcc:  99.140%\tNo_Prog: 6\n",
      "20 Epochs:\tVal Loss: 0.036261,\tBest_Loss:  0.035446,\tAcc:  99.257%\tNo_Prog: 7\n",
      "21 Epochs:\tVal Loss: 0.036690,\tBest_Loss:  0.035446,\tAcc:  99.257%\tNo_Prog: 8\n",
      "22 Epochs:\tVal Loss: 0.037295,\tBest_Loss:  0.035446,\tAcc:  99.257%\tNo_Prog: 9\n",
      "23 Epochs:\tVal Loss: 0.036618,\tBest_Loss:  0.035446,\tAcc:  99.296%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 24 epochs and 2232 steps\n",
      "Best Validation Accuracy: 98.9054%\n",
      "[CV]  n_neurons=100, n_hidden_layers=10, learning_rate=0.01, batch_size=200, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0>, total=  19.9s\n",
      "[CV] n_neurons=100, n_hidden_layers=10, learning_rate=0.01, batch_size=200, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0> \n",
      "Training DNN: \n",
      "layers=10, neurons=[100 100 100 100 100 100 100 100 100 100], epochs=100,  batch_size=200, early_stop:True, batch_norm:True, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.064257,\tBest_Loss:  0.064257,\tAcc:  97.967%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.049072,\tBest_Loss:  0.049072,\tAcc:  98.554%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.043448,\tBest_Loss:  0.043448,\tAcc:  98.593%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.036973,\tBest_Loss:  0.036973,\tAcc:  98.554%\tNo_Prog: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Epochs:\tVal Loss: 0.070502,\tBest_Loss:  0.036973,\tAcc:  97.772%\tNo_Prog: 1\n",
      "6 Epochs:\tVal Loss: 0.039767,\tBest_Loss:  0.036973,\tAcc:  98.710%\tNo_Prog: 2\n",
      "7 Epochs:\tVal Loss: 0.049275,\tBest_Loss:  0.036973,\tAcc:  98.397%\tNo_Prog: 3\n",
      "8 Epochs:\tVal Loss: 0.054889,\tBest_Loss:  0.036973,\tAcc:  98.788%\tNo_Prog: 4\n",
      "9 Epochs:\tVal Loss: 0.040472,\tBest_Loss:  0.036973,\tAcc:  98.788%\tNo_Prog: 5\n",
      "10 Epochs:\tVal Loss: 0.037554,\tBest_Loss:  0.036973,\tAcc:  98.905%\tNo_Prog: 6\n",
      "11 Epochs:\tVal Loss: 0.034787,\tBest_Loss:  0.034787,\tAcc:  98.944%\tNo_Prog: 0\n",
      "12 Epochs:\tVal Loss: 0.039284,\tBest_Loss:  0.034787,\tAcc:  99.062%\tNo_Prog: 1\n",
      "13 Epochs:\tVal Loss: 0.034158,\tBest_Loss:  0.034158,\tAcc:  98.984%\tNo_Prog: 0\n",
      "14 Epochs:\tVal Loss: 0.031744,\tBest_Loss:  0.031744,\tAcc:  99.062%\tNo_Prog: 0\n",
      "15 Epochs:\tVal Loss: 0.033740,\tBest_Loss:  0.031744,\tAcc:  99.023%\tNo_Prog: 1\n",
      "16 Epochs:\tVal Loss: 0.035849,\tBest_Loss:  0.031744,\tAcc:  99.062%\tNo_Prog: 2\n",
      "17 Epochs:\tVal Loss: 0.035560,\tBest_Loss:  0.031744,\tAcc:  98.984%\tNo_Prog: 3\n",
      "18 Epochs:\tVal Loss: 0.034938,\tBest_Loss:  0.031744,\tAcc:  98.984%\tNo_Prog: 4\n",
      "19 Epochs:\tVal Loss: 0.036064,\tBest_Loss:  0.031744,\tAcc:  98.984%\tNo_Prog: 5\n",
      "20 Epochs:\tVal Loss: 0.035951,\tBest_Loss:  0.031744,\tAcc:  98.905%\tNo_Prog: 6\n",
      "21 Epochs:\tVal Loss: 0.034355,\tBest_Loss:  0.031744,\tAcc:  99.062%\tNo_Prog: 7\n",
      "22 Epochs:\tVal Loss: 0.045557,\tBest_Loss:  0.031744,\tAcc:  98.866%\tNo_Prog: 8\n",
      "23 Epochs:\tVal Loss: 0.034816,\tBest_Loss:  0.031744,\tAcc:  98.944%\tNo_Prog: 9\n",
      "24 Epochs:\tVal Loss: 0.035718,\tBest_Loss:  0.031744,\tAcc:  98.944%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 25 epochs and 2325 steps\n",
      "Best Validation Accuracy: 99.0618%\n",
      "[CV]  n_neurons=100, n_hidden_layers=10, learning_rate=0.01, batch_size=200, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0>, total=  20.9s\n",
      "[CV] n_neurons=100, n_hidden_layers=10, learning_rate=0.01, batch_size=200, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0> \n",
      "Training DNN: \n",
      "layers=10, neurons=[100 100 100 100 100 100 100 100 100 100], epochs=100,  batch_size=200, early_stop:True, batch_norm:True, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.074342,\tBest_Loss:  0.074342,\tAcc:  97.967%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.048995,\tBest_Loss:  0.048995,\tAcc:  98.397%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.048573,\tBest_Loss:  0.048573,\tAcc:  98.554%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.042583,\tBest_Loss:  0.042583,\tAcc:  98.436%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.037949,\tBest_Loss:  0.037949,\tAcc:  98.632%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.042263,\tBest_Loss:  0.037949,\tAcc:  98.827%\tNo_Prog: 1\n",
      "7 Epochs:\tVal Loss: 0.037690,\tBest_Loss:  0.037690,\tAcc:  98.788%\tNo_Prog: 0\n",
      "8 Epochs:\tVal Loss: 0.041229,\tBest_Loss:  0.037690,\tAcc:  98.827%\tNo_Prog: 1\n",
      "9 Epochs:\tVal Loss: 0.052054,\tBest_Loss:  0.037690,\tAcc:  98.632%\tNo_Prog: 2\n",
      "10 Epochs:\tVal Loss: 0.040186,\tBest_Loss:  0.037690,\tAcc:  98.827%\tNo_Prog: 3\n",
      "11 Epochs:\tVal Loss: 0.039831,\tBest_Loss:  0.037690,\tAcc:  98.866%\tNo_Prog: 4\n",
      "12 Epochs:\tVal Loss: 0.031494,\tBest_Loss:  0.031494,\tAcc:  98.905%\tNo_Prog: 0\n",
      "13 Epochs:\tVal Loss: 0.028397,\tBest_Loss:  0.028397,\tAcc:  98.944%\tNo_Prog: 0\n",
      "14 Epochs:\tVal Loss: 0.028479,\tBest_Loss:  0.028397,\tAcc:  99.023%\tNo_Prog: 1\n",
      "15 Epochs:\tVal Loss: 0.037754,\tBest_Loss:  0.028397,\tAcc:  98.866%\tNo_Prog: 2\n",
      "16 Epochs:\tVal Loss: 0.032909,\tBest_Loss:  0.028397,\tAcc:  98.944%\tNo_Prog: 3\n",
      "17 Epochs:\tVal Loss: 0.029942,\tBest_Loss:  0.028397,\tAcc:  99.023%\tNo_Prog: 4\n",
      "18 Epochs:\tVal Loss: 0.032137,\tBest_Loss:  0.028397,\tAcc:  99.023%\tNo_Prog: 5\n",
      "19 Epochs:\tVal Loss: 0.030450,\tBest_Loss:  0.028397,\tAcc:  98.984%\tNo_Prog: 6\n",
      "20 Epochs:\tVal Loss: 0.032317,\tBest_Loss:  0.028397,\tAcc:  98.866%\tNo_Prog: 7\n",
      "21 Epochs:\tVal Loss: 0.031874,\tBest_Loss:  0.028397,\tAcc:  98.984%\tNo_Prog: 8\n",
      "22 Epochs:\tVal Loss: 0.032363,\tBest_Loss:  0.028397,\tAcc:  99.023%\tNo_Prog: 9\n",
      "23 Epochs:\tVal Loss: 0.033779,\tBest_Loss:  0.028397,\tAcc:  98.944%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 24 epochs and 2232 steps\n",
      "Best Validation Accuracy: 98.9445%\n",
      "[CV]  n_neurons=100, n_hidden_layers=10, learning_rate=0.01, batch_size=200, batch_norm_momentum_decay=0.98, activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0>, total=  20.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN: \n",
      "layers=10, neurons=[100 100 100 100 100 100 100 100 100 100], epochs=100,  batch_size=200, early_stop:True, batch_norm:True, dropout:False\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.059300,\tBest_Loss:  0.059300,\tAcc:  98.280%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.038813,\tBest_Loss:  0.038813,\tAcc:  98.866%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.041340,\tBest_Loss:  0.038813,\tAcc:  98.554%\tNo_Prog: 1\n",
      "4 Epochs:\tVal Loss: 0.034030,\tBest_Loss:  0.034030,\tAcc:  98.827%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.033679,\tBest_Loss:  0.033679,\tAcc:  98.866%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.048677,\tBest_Loss:  0.033679,\tAcc:  98.514%\tNo_Prog: 1\n",
      "7 Epochs:\tVal Loss: 0.032463,\tBest_Loss:  0.032463,\tAcc:  98.984%\tNo_Prog: 0\n",
      "8 Epochs:\tVal Loss: 0.030773,\tBest_Loss:  0.030773,\tAcc:  99.062%\tNo_Prog: 0\n",
      "9 Epochs:\tVal Loss: 0.025762,\tBest_Loss:  0.025762,\tAcc:  99.062%\tNo_Prog: 0\n",
      "10 Epochs:\tVal Loss: 0.028641,\tBest_Loss:  0.025762,\tAcc:  99.140%\tNo_Prog: 1\n",
      "11 Epochs:\tVal Loss: 0.040044,\tBest_Loss:  0.025762,\tAcc:  98.827%\tNo_Prog: 2\n",
      "12 Epochs:\tVal Loss: 0.034177,\tBest_Loss:  0.025762,\tAcc:  98.984%\tNo_Prog: 3\n",
      "13 Epochs:\tVal Loss: 0.051568,\tBest_Loss:  0.025762,\tAcc:  98.710%\tNo_Prog: 4\n",
      "14 Epochs:\tVal Loss: 0.046689,\tBest_Loss:  0.025762,\tAcc:  98.827%\tNo_Prog: 5\n",
      "15 Epochs:\tVal Loss: 0.052811,\tBest_Loss:  0.025762,\tAcc:  98.984%\tNo_Prog: 6\n",
      "16 Epochs:\tVal Loss: 0.037333,\tBest_Loss:  0.025762,\tAcc:  98.944%\tNo_Prog: 7\n",
      "17 Epochs:\tVal Loss: 0.037145,\tBest_Loss:  0.025762,\tAcc:  99.179%\tNo_Prog: 8\n",
      "18 Epochs:\tVal Loss: 0.036350,\tBest_Loss:  0.025762,\tAcc:  99.062%\tNo_Prog: 9\n",
      "19 Epochs:\tVal Loss: 0.028715,\tBest_Loss:  0.025762,\tAcc:  99.296%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 20 epochs and 2800 steps\n",
      "Best Validation Accuracy: 99.0618%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=MikesGloriousDNNClassifier(activation_function=<function elu at 0x000002297AEA8B70>,\n",
       "              batch_norm_momentum_decay=None, batch_size=50,\n",
       "              dropout_rate=None,\n",
       "              initializer=<function variance_scaling_initializer.<locals>._initializer at 0x000002290EB6EF28>,\n",
       "...er'>,\n",
       "              random_state=None, tb_model_name=None,\n",
       "              tf_logs_path='./DNN1_LOGS'),\n",
       "          fit_params={'X_valid': array([[ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.]], dtype=float32), 'y_valid': array([0, 4, ..., 1, 2], dtype=uint8), 'n_epochs': 100},\n",
       "          iid=True, n_iter=2, n_jobs=1,\n",
       "          param_distributions={'n_hidden_layers': [5, 10], 'n_neurons': [100, 160], 'learning_rate': [0.01, 0.02], 'batch_size': [100, 200], 'batch_norm_momentum_decay': [0.98], 'activation_function': [<function relu at 0x000002297AED3D08>, <function elu at 0x000002297AEA8B70>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F268>, <function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0>]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_hidden_layers\":[5,10],\n",
    "    \"n_neurons\":[100,160],\n",
    "    \"learning_rate\":[0.01,0.02],\n",
    "    \"batch_size\": [100, 200],\n",
    "    \"batch_norm_momentum_decay\":[0.98],\n",
    "    \"activation_function\":[tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01),leaky_relu(alpha=0.02)]\n",
    "}\n",
    "rand_search = RandomizedSearchCV(MikesGloriousDNNClassifier(), params, n_iter=2, random_state=42, verbose=2, fit_params={\"X_valid\":X_valid1, \"y_valid\":y_valid1, \"n_epochs\":100})\n",
    "rand_search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neurons': 100, 'n_hidden_layers': 10, 'learning_rate': 0.01, 'batch_size': 200, 'batch_norm_momentum_decay': 0.98, 'activation_function': <function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002290EF5F2F0>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99182720373613542"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what are the best params\n",
    "print(rand_search.best_params_)\n",
    "\n",
    "#print accruacy\n",
    "y_pred = rand_search.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meh. Only 0.1% better... not that great. Lets try retraining with dropout!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
    "\n",
    "First lets go back to our original best model ('Model1') and see if it is overfitting the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  0.999536343534\n",
      "test data:  0.992021794123\n"
     ]
    }
   ],
   "source": [
    "#print accruacy on trianing data\n",
    "y_pred = model1.predict(X_train1)\n",
    "print('train data: ',accuracy_score(y_train1, y_pred))\n",
    "\n",
    "#print accruacy on test data\n",
    "y_pred = model1.predict(X_test1)\n",
    "print('test data: ',accuracy_score(y_test1, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN: \n",
      "layers=5, neurons=[100 100 100 100 100], epochs=1000,  batch_size=500, early_stop:True, batch_norm:True, dropout:True\n",
      "\n",
      "1 Epochs:\tVal Loss: 0.553163,\tBest_Loss:  0.553163,\tAcc:  95.661%\tNo_Prog: 0\n",
      "2 Epochs:\tVal Loss: 0.140643,\tBest_Loss:  0.140643,\tAcc:  97.146%\tNo_Prog: 0\n",
      "3 Epochs:\tVal Loss: 0.090050,\tBest_Loss:  0.090050,\tAcc:  97.772%\tNo_Prog: 0\n",
      "4 Epochs:\tVal Loss: 0.073070,\tBest_Loss:  0.073070,\tAcc:  98.163%\tNo_Prog: 0\n",
      "5 Epochs:\tVal Loss: 0.069932,\tBest_Loss:  0.069932,\tAcc:  97.928%\tNo_Prog: 0\n",
      "6 Epochs:\tVal Loss: 0.059706,\tBest_Loss:  0.059706,\tAcc:  98.475%\tNo_Prog: 0\n",
      "7 Epochs:\tVal Loss: 0.055358,\tBest_Loss:  0.055358,\tAcc:  98.593%\tNo_Prog: 0\n",
      "8 Epochs:\tVal Loss: 0.053248,\tBest_Loss:  0.053248,\tAcc:  98.593%\tNo_Prog: 0\n",
      "9 Epochs:\tVal Loss: 0.050808,\tBest_Loss:  0.050808,\tAcc:  98.749%\tNo_Prog: 0\n",
      "10 Epochs:\tVal Loss: 0.050649,\tBest_Loss:  0.050649,\tAcc:  98.632%\tNo_Prog: 0\n",
      "11 Epochs:\tVal Loss: 0.046101,\tBest_Loss:  0.046101,\tAcc:  98.788%\tNo_Prog: 0\n",
      "12 Epochs:\tVal Loss: 0.048800,\tBest_Loss:  0.046101,\tAcc:  98.632%\tNo_Prog: 1\n",
      "13 Epochs:\tVal Loss: 0.051326,\tBest_Loss:  0.046101,\tAcc:  98.475%\tNo_Prog: 2\n",
      "14 Epochs:\tVal Loss: 0.050256,\tBest_Loss:  0.046101,\tAcc:  98.671%\tNo_Prog: 3\n",
      "15 Epochs:\tVal Loss: 0.050067,\tBest_Loss:  0.046101,\tAcc:  98.554%\tNo_Prog: 4\n",
      "16 Epochs:\tVal Loss: 0.045831,\tBest_Loss:  0.045831,\tAcc:  98.593%\tNo_Prog: 0\n",
      "17 Epochs:\tVal Loss: 0.046075,\tBest_Loss:  0.045831,\tAcc:  98.632%\tNo_Prog: 1\n",
      "18 Epochs:\tVal Loss: 0.046774,\tBest_Loss:  0.045831,\tAcc:  98.710%\tNo_Prog: 2\n",
      "19 Epochs:\tVal Loss: 0.046821,\tBest_Loss:  0.045831,\tAcc:  98.632%\tNo_Prog: 3\n",
      "20 Epochs:\tVal Loss: 0.047815,\tBest_Loss:  0.045831,\tAcc:  98.632%\tNo_Prog: 4\n",
      "21 Epochs:\tVal Loss: 0.042937,\tBest_Loss:  0.042937,\tAcc:  98.944%\tNo_Prog: 0\n",
      "22 Epochs:\tVal Loss: 0.045541,\tBest_Loss:  0.042937,\tAcc:  98.749%\tNo_Prog: 1\n",
      "23 Epochs:\tVal Loss: 0.047789,\tBest_Loss:  0.042937,\tAcc:  98.671%\tNo_Prog: 2\n",
      "24 Epochs:\tVal Loss: 0.043522,\tBest_Loss:  0.042937,\tAcc:  98.827%\tNo_Prog: 3\n",
      "25 Epochs:\tVal Loss: 0.040987,\tBest_Loss:  0.040987,\tAcc:  98.944%\tNo_Prog: 0\n",
      "26 Epochs:\tVal Loss: 0.050666,\tBest_Loss:  0.040987,\tAcc:  98.632%\tNo_Prog: 1\n",
      "27 Epochs:\tVal Loss: 0.052121,\tBest_Loss:  0.040987,\tAcc:  98.749%\tNo_Prog: 2\n",
      "28 Epochs:\tVal Loss: 0.042862,\tBest_Loss:  0.040987,\tAcc:  98.827%\tNo_Prog: 3\n",
      "29 Epochs:\tVal Loss: 0.048544,\tBest_Loss:  0.040987,\tAcc:  98.671%\tNo_Prog: 4\n",
      "30 Epochs:\tVal Loss: 0.044231,\tBest_Loss:  0.040987,\tAcc:  98.749%\tNo_Prog: 5\n",
      "31 Epochs:\tVal Loss: 0.039942,\tBest_Loss:  0.039942,\tAcc:  98.944%\tNo_Prog: 0\n",
      "32 Epochs:\tVal Loss: 0.042115,\tBest_Loss:  0.039942,\tAcc:  98.866%\tNo_Prog: 1\n",
      "33 Epochs:\tVal Loss: 0.044493,\tBest_Loss:  0.039942,\tAcc:  98.827%\tNo_Prog: 2\n",
      "34 Epochs:\tVal Loss: 0.043157,\tBest_Loss:  0.039942,\tAcc:  98.944%\tNo_Prog: 3\n",
      "35 Epochs:\tVal Loss: 0.040946,\tBest_Loss:  0.039942,\tAcc:  98.827%\tNo_Prog: 4\n",
      "36 Epochs:\tVal Loss: 0.040292,\tBest_Loss:  0.039942,\tAcc:  98.905%\tNo_Prog: 5\n",
      "37 Epochs:\tVal Loss: 0.044116,\tBest_Loss:  0.039942,\tAcc:  98.749%\tNo_Prog: 6\n",
      "38 Epochs:\tVal Loss: 0.039824,\tBest_Loss:  0.039824,\tAcc:  98.866%\tNo_Prog: 0\n",
      "39 Epochs:\tVal Loss: 0.041022,\tBest_Loss:  0.039824,\tAcc:  98.866%\tNo_Prog: 1\n",
      "40 Epochs:\tVal Loss: 0.040447,\tBest_Loss:  0.039824,\tAcc:  98.827%\tNo_Prog: 2\n",
      "41 Epochs:\tVal Loss: 0.038832,\tBest_Loss:  0.038832,\tAcc:  98.749%\tNo_Prog: 0\n",
      "42 Epochs:\tVal Loss: 0.036877,\tBest_Loss:  0.036877,\tAcc:  98.827%\tNo_Prog: 0\n",
      "43 Epochs:\tVal Loss: 0.040193,\tBest_Loss:  0.036877,\tAcc:  98.788%\tNo_Prog: 1\n",
      "44 Epochs:\tVal Loss: 0.037363,\tBest_Loss:  0.036877,\tAcc:  98.788%\tNo_Prog: 2\n",
      "45 Epochs:\tVal Loss: 0.037545,\tBest_Loss:  0.036877,\tAcc:  98.905%\tNo_Prog: 3\n",
      "46 Epochs:\tVal Loss: 0.038068,\tBest_Loss:  0.036877,\tAcc:  98.984%\tNo_Prog: 4\n",
      "47 Epochs:\tVal Loss: 0.041706,\tBest_Loss:  0.036877,\tAcc:  98.827%\tNo_Prog: 5\n",
      "48 Epochs:\tVal Loss: 0.036730,\tBest_Loss:  0.036730,\tAcc:  98.866%\tNo_Prog: 0\n",
      "49 Epochs:\tVal Loss: 0.034750,\tBest_Loss:  0.034750,\tAcc:  98.905%\tNo_Prog: 0\n",
      "50 Epochs:\tVal Loss: 0.037759,\tBest_Loss:  0.034750,\tAcc:  98.944%\tNo_Prog: 1\n",
      "51 Epochs:\tVal Loss: 0.040464,\tBest_Loss:  0.034750,\tAcc:  98.710%\tNo_Prog: 2\n",
      "52 Epochs:\tVal Loss: 0.041338,\tBest_Loss:  0.034750,\tAcc:  98.827%\tNo_Prog: 3\n",
      "53 Epochs:\tVal Loss: 0.038027,\tBest_Loss:  0.034750,\tAcc:  98.944%\tNo_Prog: 4\n",
      "54 Epochs:\tVal Loss: 0.036116,\tBest_Loss:  0.034750,\tAcc:  98.984%\tNo_Prog: 5\n",
      "55 Epochs:\tVal Loss: 0.038457,\tBest_Loss:  0.034750,\tAcc:  98.749%\tNo_Prog: 6\n",
      "56 Epochs:\tVal Loss: 0.036344,\tBest_Loss:  0.034750,\tAcc:  98.866%\tNo_Prog: 7\n",
      "57 Epochs:\tVal Loss: 0.037505,\tBest_Loss:  0.034750,\tAcc:  98.905%\tNo_Prog: 8\n",
      "58 Epochs:\tVal Loss: 0.036071,\tBest_Loss:  0.034750,\tAcc:  98.984%\tNo_Prog: 9\n",
      "59 Epochs:\tVal Loss: 0.036394,\tBest_Loss:  0.034750,\tAcc:  99.140%\tNo_Prog: 10\n",
      "\n",
      "Model stopped EARLY at 60 epochs and 3360 steps\n",
      "Best Validation Accuracy: 98.9054%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MikesGloriousDNNClassifier(activation_function=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x000002291C43C9D8>,\n",
       "              batch_norm_momentum_decay=0.99, batch_size=500,\n",
       "              dropout_rate=0.5,\n",
       "              initializer=<function variance_scaling_initializer.<locals>._initializer at 0x000002291B17C158>,\n",
       "              k_in_top_val=1, layer_name='hidden', learning_rate=0.01,\n",
       "              max_epochs_no_progress=10, n_hidden_layers=5,\n",
       "              n_neurons=array([100, 100, 100, 100, 100]),\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=None, tb_model_name='model1_bn_dropout',\n",
       "              tf_logs_path='./DNN1_LOGS')"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there is a little overfitting. A bit of regularization will help, so lets add a dropout_rate of 50%\n",
    "reset_graph()\n",
    "model2 = MikesGloriousDNNClassifier(activation_function=leaky_relu(alpha=0.01),\n",
    "                                     n_neurons=100,\n",
    "                                     n_hidden_layers=5,\n",
    "                                     learning_rate=0.01,\n",
    "                                     batch_size=500, \n",
    "                                     batch_norm_momentum_decay = 0.99, \n",
    "                                     tb_model_name='model1_bn_dropout', \n",
    "                                     dropout_rate=0.5)\n",
    " \n",
    "model2.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  0.993223482417\n",
      "test data:  0.992216384511\n"
     ]
    }
   ],
   "source": [
    "#print accruacy on trianing data\n",
    "y_pred = model2.predict(X_train1)\n",
    "print('train data: ',accuracy_score(y_train1, y_pred))\n",
    "\n",
    "#print accruacy on test data\n",
    "y_pred = model2.predict(X_test1)\n",
    "print('test data: ',accuracy_score(y_test1, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nice! Dropout regularization helped a little as the test accuracy went up by 0.2%! HOWEVER, convergence did slow down a little. Let's save this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save(\"./DNN1_LOGS/my_best_DNN_0_to_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'X' type=Placeholder>,\n",
       " <tf.Operation 'y' type=Placeholder>,\n",
       " <tf.Operation 'inTrainingMode/input' type=Const>,\n",
       " <tf.Operation 'inTrainingMode' type=PlaceholderWithDefault>,\n",
       " <tf.Operation 'dropout/cond/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout/cond/switch_t' type=Identity>,\n",
       " <tf.Operation 'dropout/cond/switch_f' type=Identity>,\n",
       " <tf.Operation 'dropout/cond/pred_id' type=Identity>,\n",
       " <tf.Operation 'dropout/cond/dropout/keep_prob' type=Const>,\n",
       " <tf.Operation 'dropout/cond/dropout/Shape/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout/cond/dropout/Shape' type=Shape>,\n",
       " <tf.Operation 'dropout/cond/dropout/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dropout/cond/dropout/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dropout/cond/dropout/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dropout/cond/dropout/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dropout/cond/dropout/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dropout/cond/dropout/random_uniform' type=Add>,\n",
       " <tf.Operation 'dropout/cond/dropout/add' type=Add>,\n",
       " <tf.Operation 'dropout/cond/dropout/Floor' type=Floor>,\n",
       " <tf.Operation 'dropout/cond/dropout/div' type=RealDiv>,\n",
       " <tf.Operation 'dropout/cond/dropout/mul' type=Mul>,\n",
       " <tf.Operation 'dropout/cond/Identity/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout/cond/Identity' type=Identity>,\n",
       " <tf.Operation 'dropout/cond/Merge' type=Merge>,\n",
       " <tf.Operation 'hidden_1/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'hidden_1/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'hidden_1/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'hidden_1/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'hidden_1/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'hidden_1/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'hidden_1/kernel' type=VariableV2>,\n",
       " <tf.Operation 'hidden_1/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_1/kernel/read' type=Identity>,\n",
       " <tf.Operation 'hidden_1/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'hidden_1/bias' type=VariableV2>,\n",
       " <tf.Operation 'hidden_1/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_1/bias/read' type=Identity>,\n",
       " <tf.Operation 'hidden_1/MatMul' type=MatMul>,\n",
       " <tf.Operation 'hidden_1/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'batch_normalization/beta/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization/beta' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization/beta/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization/beta/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization/gamma/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization/gamma' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization/gamma/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization/gamma/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization/moving_mean/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization/moving_mean' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization/moving_mean/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization/moving_variance/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization/moving_variance' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization/moving_variance/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization/moments/Mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization/moments/Mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization/moments/StopGradient' type=StopGradient>,\n",
       " <tf.Operation 'batch_normalization/moments/Sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization/moments/shifted_mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization/moments/shifted_mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization/moments/SquaredDifference' type=SquaredDifference>,\n",
       " <tf.Operation 'batch_normalization/moments/Mean_1/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization/moments/Mean_1' type=Mean>,\n",
       " <tf.Operation 'batch_normalization/moments/Square' type=Square>,\n",
       " <tf.Operation 'batch_normalization/moments/variance' type=Sub>,\n",
       " <tf.Operation 'batch_normalization/moments/mean' type=Add>,\n",
       " <tf.Operation 'batch_normalization/moments/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization/moments/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_1/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_1' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization/Reshape' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization/Select' type=Select>,\n",
       " <tf.Operation 'batch_normalization/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_2/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_2' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_3/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_3' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization/Reshape_1/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization/Select_1' type=Select>,\n",
       " <tf.Operation 'batch_normalization/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_4/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_4/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_4' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_5/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_5/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization/ExpandDims_5' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization/Reshape_2/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization/Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization/Select_2' type=Select>,\n",
       " <tf.Operation 'batch_normalization/Squeeze_2' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg_1/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg_1/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg_1/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg_1/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'batch_normalization/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'batch_normalization/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'batch_normalization/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'batch_normalization/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'batch_normalization/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'mul/x' type=Const>,\n",
       " <tf.Operation 'mul' type=Mul>,\n",
       " <tf.Operation 'hidden_1_ACTIVATED' type=Maximum>,\n",
       " <tf.Operation 'dropout_2/cond/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_2/cond/switch_t' type=Identity>,\n",
       " <tf.Operation 'dropout_2/cond/switch_f' type=Identity>,\n",
       " <tf.Operation 'dropout_2/cond/pred_id' type=Identity>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/keep_prob' type=Const>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/Shape/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/Shape' type=Shape>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/random_uniform' type=Add>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/add' type=Add>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/Floor' type=Floor>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/div' type=RealDiv>,\n",
       " <tf.Operation 'dropout_2/cond/dropout/mul' type=Mul>,\n",
       " <tf.Operation 'dropout_2/cond/Identity/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_2/cond/Identity' type=Identity>,\n",
       " <tf.Operation 'dropout_2/cond/Merge' type=Merge>,\n",
       " <tf.Operation 'hidden_2/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'hidden_2/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'hidden_2/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'hidden_2/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'hidden_2/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'hidden_2/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'hidden_2/kernel' type=VariableV2>,\n",
       " <tf.Operation 'hidden_2/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_2/kernel/read' type=Identity>,\n",
       " <tf.Operation 'hidden_2/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'hidden_2/bias' type=VariableV2>,\n",
       " <tf.Operation 'hidden_2/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_2/bias/read' type=Identity>,\n",
       " <tf.Operation 'hidden_2/MatMul' type=MatMul>,\n",
       " <tf.Operation 'hidden_2/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'batch_normalization_1/beta/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization_1/beta' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_1/beta/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_1/beta/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_1/gamma/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization_1/gamma' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_1/gamma/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_1/gamma/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_1/moving_mean/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization_1/moving_mean' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_1/moving_mean/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_1/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_1/moving_variance/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization_1/moving_variance' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_1/moving_variance/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_1/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_2/moments/Mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/moments/Mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_2/moments/StopGradient' type=StopGradient>,\n",
       " <tf.Operation 'batch_normalization_2/moments/Sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_2/moments/shifted_mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/moments/shifted_mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_2/moments/SquaredDifference' type=SquaredDifference>,\n",
       " <tf.Operation 'batch_normalization_2/moments/Mean_1/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/moments/Mean_1' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_2/moments/Square' type=Square>,\n",
       " <tf.Operation 'batch_normalization_2/moments/variance' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_2/moments/mean' type=Add>,\n",
       " <tf.Operation 'batch_normalization_2/moments/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_2/moments/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_1/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_1' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_2/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/Reshape' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_2/Select' type=Select>,\n",
       " <tf.Operation 'batch_normalization_2/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_2/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_2' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_3/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_3' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_2/Reshape_1/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_2/Select_1' type=Select>,\n",
       " <tf.Operation 'batch_normalization_2/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_4/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_4/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_4' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_5/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_5/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/ExpandDims_5' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_2/Reshape_2/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_2/Select_2' type=Select>,\n",
       " <tf.Operation 'batch_normalization_2/Squeeze_2' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg_1/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg_1/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg_1/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg_1/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_2/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization_2/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'batch_normalization_2/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'batch_normalization_2/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_2/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_2/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_2/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_2/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'mul_1/x' type=Const>,\n",
       " <tf.Operation 'mul_1' type=Mul>,\n",
       " <tf.Operation 'hidden_2_ACTIVATED' type=Maximum>,\n",
       " <tf.Operation 'dropout_3/cond/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_3/cond/switch_t' type=Identity>,\n",
       " <tf.Operation 'dropout_3/cond/switch_f' type=Identity>,\n",
       " <tf.Operation 'dropout_3/cond/pred_id' type=Identity>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/keep_prob' type=Const>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/Shape/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/Shape' type=Shape>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/random_uniform' type=Add>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/add' type=Add>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/Floor' type=Floor>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/div' type=RealDiv>,\n",
       " <tf.Operation 'dropout_3/cond/dropout/mul' type=Mul>,\n",
       " <tf.Operation 'dropout_3/cond/Identity/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_3/cond/Identity' type=Identity>,\n",
       " <tf.Operation 'dropout_3/cond/Merge' type=Merge>,\n",
       " <tf.Operation 'hidden_3/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'hidden_3/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'hidden_3/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'hidden_3/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'hidden_3/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'hidden_3/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'hidden_3/kernel' type=VariableV2>,\n",
       " <tf.Operation 'hidden_3/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_3/kernel/read' type=Identity>,\n",
       " <tf.Operation 'hidden_3/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'hidden_3/bias' type=VariableV2>,\n",
       " <tf.Operation 'hidden_3/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_3/bias/read' type=Identity>,\n",
       " <tf.Operation 'hidden_3/MatMul' type=MatMul>,\n",
       " <tf.Operation 'hidden_3/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'batch_normalization_2/beta/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/beta' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_2/beta/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_2/beta/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_2/gamma/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/gamma' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_2/gamma/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_2/gamma/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_2/moving_mean/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/moving_mean' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_2/moving_mean/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_2/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_2/moving_variance/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization_2/moving_variance' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_2/moving_variance/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_2/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_3/moments/Mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/moments/Mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_3/moments/StopGradient' type=StopGradient>,\n",
       " <tf.Operation 'batch_normalization_3/moments/Sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_3/moments/shifted_mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/moments/shifted_mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_3/moments/SquaredDifference' type=SquaredDifference>,\n",
       " <tf.Operation 'batch_normalization_3/moments/Mean_1/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/moments/Mean_1' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_3/moments/Square' type=Square>,\n",
       " <tf.Operation 'batch_normalization_3/moments/variance' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_3/moments/mean' type=Add>,\n",
       " <tf.Operation 'batch_normalization_3/moments/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_3/moments/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_1/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_1' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_3/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/Reshape' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_3/Select' type=Select>,\n",
       " <tf.Operation 'batch_normalization_3/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_2/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_2' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_3/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_3' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_3/Reshape_1/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_3/Select_1' type=Select>,\n",
       " <tf.Operation 'batch_normalization_3/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_4/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_4/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_4' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_5/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_5/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/ExpandDims_5' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_3/Reshape_2/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_3/Select_2' type=Select>,\n",
       " <tf.Operation 'batch_normalization_3/Squeeze_2' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg_1/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg_1/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg_1/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg_1/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_3/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization_3/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'batch_normalization_3/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'batch_normalization_3/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_3/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_3/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_3/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_3/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'mul_2/x' type=Const>,\n",
       " <tf.Operation 'mul_2' type=Mul>,\n",
       " <tf.Operation 'hidden_3_ACTIVATED' type=Maximum>,\n",
       " <tf.Operation 'dropout_4/cond/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_4/cond/switch_t' type=Identity>,\n",
       " <tf.Operation 'dropout_4/cond/switch_f' type=Identity>,\n",
       " <tf.Operation 'dropout_4/cond/pred_id' type=Identity>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/keep_prob' type=Const>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/Shape/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/Shape' type=Shape>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/random_uniform' type=Add>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/add' type=Add>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/Floor' type=Floor>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/div' type=RealDiv>,\n",
       " <tf.Operation 'dropout_4/cond/dropout/mul' type=Mul>,\n",
       " <tf.Operation 'dropout_4/cond/Identity/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_4/cond/Identity' type=Identity>,\n",
       " <tf.Operation 'dropout_4/cond/Merge' type=Merge>,\n",
       " <tf.Operation 'hidden_4/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'hidden_4/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'hidden_4/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'hidden_4/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'hidden_4/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'hidden_4/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'hidden_4/kernel' type=VariableV2>,\n",
       " <tf.Operation 'hidden_4/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_4/kernel/read' type=Identity>,\n",
       " <tf.Operation 'hidden_4/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'hidden_4/bias' type=VariableV2>,\n",
       " <tf.Operation 'hidden_4/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_4/bias/read' type=Identity>,\n",
       " <tf.Operation 'hidden_4/MatMul' type=MatMul>,\n",
       " <tf.Operation 'hidden_4/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'batch_normalization_3/beta/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/beta' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_3/beta/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_3/beta/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_3/gamma/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/gamma' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_3/gamma/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_3/gamma/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_3/moving_mean/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/moving_mean' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_3/moving_mean/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_3/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_3/moving_variance/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization_3/moving_variance' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_3/moving_variance/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_3/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_4/moments/Mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/moments/Mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_4/moments/StopGradient' type=StopGradient>,\n",
       " <tf.Operation 'batch_normalization_4/moments/Sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_4/moments/shifted_mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/moments/shifted_mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_4/moments/SquaredDifference' type=SquaredDifference>,\n",
       " <tf.Operation 'batch_normalization_4/moments/Mean_1/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/moments/Mean_1' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_4/moments/Square' type=Square>,\n",
       " <tf.Operation 'batch_normalization_4/moments/variance' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_4/moments/mean' type=Add>,\n",
       " <tf.Operation 'batch_normalization_4/moments/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_4/moments/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_1/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_1' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_4/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/Reshape' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_4/Select' type=Select>,\n",
       " <tf.Operation 'batch_normalization_4/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_2/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_2' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_3/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_3' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_4/Reshape_1/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_4/Select_1' type=Select>,\n",
       " <tf.Operation 'batch_normalization_4/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_4/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_4/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_4' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_5/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_5/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/ExpandDims_5' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_4/Reshape_2/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_4/Select_2' type=Select>,\n",
       " <tf.Operation 'batch_normalization_4/Squeeze_2' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg_1/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg_1/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg_1/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg_1/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_4/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization_4/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'batch_normalization_4/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'batch_normalization_4/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_4/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_4/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_4/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_4/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'mul_3/x' type=Const>,\n",
       " <tf.Operation 'mul_3' type=Mul>,\n",
       " <tf.Operation 'hidden_4_ACTIVATED' type=Maximum>,\n",
       " <tf.Operation 'dropout_5/cond/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_5/cond/switch_t' type=Identity>,\n",
       " <tf.Operation 'dropout_5/cond/switch_f' type=Identity>,\n",
       " <tf.Operation 'dropout_5/cond/pred_id' type=Identity>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/keep_prob' type=Const>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/Shape/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/Shape' type=Shape>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/random_uniform' type=Add>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/add' type=Add>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/Floor' type=Floor>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/div' type=RealDiv>,\n",
       " <tf.Operation 'dropout_5/cond/dropout/mul' type=Mul>,\n",
       " <tf.Operation 'dropout_5/cond/Identity/Switch' type=Switch>,\n",
       " <tf.Operation 'dropout_5/cond/Identity' type=Identity>,\n",
       " <tf.Operation 'dropout_5/cond/Merge' type=Merge>,\n",
       " <tf.Operation 'hidden_5/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'hidden_5/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'hidden_5/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'hidden_5/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'hidden_5/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'hidden_5/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'hidden_5/kernel' type=VariableV2>,\n",
       " <tf.Operation 'hidden_5/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_5/kernel/read' type=Identity>,\n",
       " <tf.Operation 'hidden_5/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'hidden_5/bias' type=VariableV2>,\n",
       " <tf.Operation 'hidden_5/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'hidden_5/bias/read' type=Identity>,\n",
       " <tf.Operation 'hidden_5/MatMul' type=MatMul>,\n",
       " <tf.Operation 'hidden_5/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'batch_normalization_4/beta/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/beta' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_4/beta/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_4/beta/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_4/gamma/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/gamma' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_4/gamma/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_4/gamma/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_4/moving_mean/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/moving_mean' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_4/moving_mean/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_4/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_4/moving_variance/Initializer/ones' type=Const>,\n",
       " <tf.Operation 'batch_normalization_4/moving_variance' type=VariableV2>,\n",
       " <tf.Operation 'batch_normalization_4/moving_variance/Assign' type=Assign>,\n",
       " <tf.Operation 'batch_normalization_4/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'batch_normalization_5/moments/Mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/moments/Mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_5/moments/StopGradient' type=StopGradient>,\n",
       " <tf.Operation 'batch_normalization_5/moments/Sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_5/moments/shifted_mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/moments/shifted_mean' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_5/moments/SquaredDifference' type=SquaredDifference>,\n",
       " <tf.Operation 'batch_normalization_5/moments/Mean_1/reduction_indices' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/moments/Mean_1' type=Mean>,\n",
       " <tf.Operation 'batch_normalization_5/moments/Square' type=Square>,\n",
       " <tf.Operation 'batch_normalization_5/moments/variance' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_5/moments/mean' type=Add>,\n",
       " <tf.Operation 'batch_normalization_5/moments/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_5/moments/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_1/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_1' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_5/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/Reshape' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_5/Select' type=Select>,\n",
       " <tf.Operation 'batch_normalization_5/Squeeze' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_2/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_2' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_3/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_3' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_5/Reshape_1/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_5/Select_1' type=Select>,\n",
       " <tf.Operation 'batch_normalization_5/Squeeze_1' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_4/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_4/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_4' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_5/input' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_5/dim' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/ExpandDims_5' type=ExpandDims>,\n",
       " <tf.Operation 'batch_normalization_5/Reshape_2/shape' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'batch_normalization_5/Select_2' type=Select>,\n",
       " <tf.Operation 'batch_normalization_5/Squeeze_2' type=Squeeze>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg_1/sub/x' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg_1/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg_1/sub_1' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg_1/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_5/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization_5/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'batch_normalization_5/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'batch_normalization_5/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'batch_normalization_5/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_5/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_5/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'batch_normalization_5/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'batch_normalization_5/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'mul_4/x' type=Const>,\n",
       " <tf.Operation 'mul_4' type=Mul>,\n",
       " <tf.Operation 'hidden_5_ACTIVATED' type=Maximum>,\n",
       " <tf.Operation 'output_logits/kernel/Initializer/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'output_logits/kernel/Initializer/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'output_logits/kernel/Initializer/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'output_logits/kernel/Initializer/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'output_logits/kernel/Initializer/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'output_logits/kernel/Initializer/truncated_normal' type=Add>,\n",
       " <tf.Operation 'output_logits/kernel' type=VariableV2>,\n",
       " <tf.Operation 'output_logits/kernel/Assign' type=Assign>,\n",
       " <tf.Operation 'output_logits/kernel/read' type=Identity>,\n",
       " <tf.Operation 'output_logits/bias/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'output_logits/bias' type=VariableV2>,\n",
       " <tf.Operation 'output_logits/bias/Assign' type=Assign>,\n",
       " <tf.Operation 'output_logits/bias/read' type=Identity>,\n",
       " <tf.Operation 'output_logits/MatMul' type=MatMul>,\n",
       " <tf.Operation 'output_logits/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'y_proba' type=Softmax>,\n",
       " <tf.Operation 'SparseSoftmaxCrossEntropyWithLogits/Shape' type=Shape>,\n",
       " <tf.Operation 'SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' type=SparseSoftmaxCrossEntropyWithLogits>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'mean_loss' type=Mean>,\n",
       " <tf.Operation 'gradients/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Fill' type=Fill>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Tile' type=Tile>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Shape_2' type=Const>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Prod' type=Prod>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Const_1' type=Const>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Prod_1' type=Prod>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Maximum/y' type=Const>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Maximum' type=Maximum>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/Cast' type=Cast>,\n",
       " <tf.Operation 'gradients/mean_loss_grad/truediv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/zeros_like' type=ZerosLike>,\n",
       " <tf.Operation 'gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient' type=PreventGradient>,\n",
       " <tf.Operation 'gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/output_logits/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/output_logits/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/output_logits/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/output_logits/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/output_logits/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/output_logits/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/output_logits/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/output_logits/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/output_logits/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/Shape_2' type=Shape>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/zeros/Const' type=Const>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/zeros' type=Fill>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/GreaterEqual' type=GreaterEqual>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/Select' type=Select>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/LogicalNot' type=LogicalNot>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/Select_1' type=Select>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/hidden_5_ACTIVATED_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/mul_4_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/mul_4_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/mul_4_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/mul_4_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/mul_4_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/mul_4_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/mul_4_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/mul_4_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/mul_4_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/mul_4_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/mul_4_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/mul_4_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/AddN' type=AddN>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/Neg' type=Neg>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/sub_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_2_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Squeeze_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Squeeze_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/AddN_1' type=AddN>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/mul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_grad/zeros_like' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_grad/Select' type=Select>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_grad/Select_1' type=Select>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/Rsqrt_grad/RsqrtGrad' type=RsqrtGrad>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/ExpandDims_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/ExpandDims_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/batchnorm/add_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Squeeze_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Squeeze_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Squeeze_1_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Squeeze_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/mean_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_1_grad/zeros_like' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_1_grad/Select' type=Select>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_1_grad/Select_1' type=Select>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/Select_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/ExpandDims_2_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/ExpandDims_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Squeeze_1_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Squeeze_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/Neg' type=Neg>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/variance_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Size' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/add' type=Add>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/mod' type=FloorMod>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/range/start' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/range/delta' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/range' type=Range>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Fill/value' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Fill' type=Fill>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/DynamicStitch' type=DynamicStitch>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Maximum/y' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Maximum' type=Maximum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Tile' type=Tile>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Shape_2' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Shape_3' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Prod' type=Prod>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Const_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Prod_1' type=Prod>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Maximum_1/y' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Maximum_1' type=Maximum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/floordiv_1' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/Cast' type=Cast>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Mean_1_grad/truediv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Square_grad/mul/x' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Square_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Square_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/scalar' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/sub' type=Sub>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/Neg' type=Neg>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/SquaredDifference_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/AddN_2' type=AddN>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Size' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/add' type=Add>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/mod' type=FloorMod>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/range/start' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/range/delta' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/range' type=Range>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Fill/value' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Fill' type=Fill>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/DynamicStitch' type=DynamicStitch>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Maximum/y' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Maximum' type=Maximum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Tile' type=Tile>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Shape_2' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Shape_3' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Prod' type=Prod>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Const_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Prod_1' type=Prod>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Maximum_1/y' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Maximum_1' type=Maximum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/floordiv_1' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/Cast' type=Cast>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/shifted_mean_grad/truediv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/Neg' type=Neg>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_5/moments/Sub_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/AddN_3' type=AddN>,\n",
       " <tf.Operation 'gradients/AddN_4' type=AddN>,\n",
       " <tf.Operation 'gradients/hidden_5/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/hidden_5/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/hidden_5/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/hidden_5/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/hidden_5/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/hidden_5/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/hidden_5/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/hidden_5/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/hidden_5/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/Merge_grad/cond_grad' type=Switch>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/Merge_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/Merge_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/Merge_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/mul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/Switch' type=Switch>,\n",
       " <tf.Operation 'gradients/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/zeros/Const' type=Const>,\n",
       " <tf.Operation 'gradients/zeros' type=Fill>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/Identity/Switch_grad/cond_grad' type=Merge>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/RealDiv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/Neg' type=Neg>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/RealDiv_1' type=RealDiv>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/RealDiv_2' type=RealDiv>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/div_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/Switch_1' type=Switch>,\n",
       " <tf.Operation 'gradients/Shape_2' type=Shape>,\n",
       " <tf.Operation 'gradients/zeros_1/Const' type=Const>,\n",
       " <tf.Operation 'gradients/zeros_1' type=Fill>,\n",
       " <tf.Operation 'gradients/dropout_5/cond/dropout/Shape/Switch_grad/cond_grad' type=Merge>,\n",
       " <tf.Operation 'gradients/AddN_5' type=AddN>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/Shape_2' type=Shape>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/zeros/Const' type=Const>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/zeros' type=Fill>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/GreaterEqual' type=GreaterEqual>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/Select' type=Select>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/LogicalNot' type=LogicalNot>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/Select_1' type=Select>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/hidden_4_ACTIVATED_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/mul_3_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/mul_3_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/mul_3_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/mul_3_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/mul_3_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/mul_3_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/mul_3_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/mul_3_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/mul_3_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/mul_3_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/mul_3_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/mul_3_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/AddN_6' type=AddN>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/Neg' type=Neg>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/sub_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_2_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/Squeeze_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/Squeeze_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/AddN_7' type=AddN>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/mul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/Select_grad/zeros_like' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/Select_grad/Select' type=Select>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/Select_grad/Select_1' type=Select>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/Select_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/Select_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/Select_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/Rsqrt_grad/RsqrtGrad' type=RsqrtGrad>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/ExpandDims_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/ExpandDims_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/batch_normalization_4/batchnorm/add_grad/Sum' type=Sum>,\n",
       " ...]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the operations to make sure everything is in orer!\n",
    "model2._graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'hidden_1/kernel:0' shape=(784, 100) dtype=float32_ref>\n",
      "<tf.Variable 'hidden_1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'hidden_2/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'hidden_2/bias:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization_1/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization_1/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'hidden_3/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'hidden_3/bias:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization_2/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization_2/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'hidden_4/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'hidden_4/bias:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization_3/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization_3/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'hidden_5/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'hidden_5/bias:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization_4/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'batch_normalization_4/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'output_logits/kernel:0' shape=(100, 5) dtype=float32_ref>\n",
      "<tf.Variable 'output_logits/bias:0' shape=(5,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with model2._graph.as_default():\n",
    "    for var in tf.trainable_variables():\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a fresh new one.\n",
    "\n",
    "Let's first load in the best model's graph and get a handle on ALL of the important operations we need. Note that instead of creating a new softmax output layer, we will just reuse the existing one (since it has the same number of outputs, 5). We will reinitialize the hyperparams before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "restore_meta = tf.train.import_meta_graph(\"./DNN1_LOGS/my_best_DNN_0_to_4.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "mean_loss = tf.get_default_graph().get_tensor_by_name(\"mean_loss:0\")\n",
    "y_proba = tf.get_default_graph().get_tensor_by_name(\"y_proba:0\")\n",
    "logits = y_proba.op.inputs[0]\n",
    "mean_accuracy = tf.get_default_graph().get_tensor_by_name(\"mean_accuracy:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_accuracy:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To freeze lower layers, exclude their vars from optimizers list of trainable variables,\n",
    "# keeping only the output layers trainable variables:\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"Adam2\")\n",
    "#pass in var list to optimizer\n",
    "logit_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='output_logits')\n",
    "training_op = optimizer.minimize(mean_loss, var_list=logit_vars)\n",
    "\n",
    "#create accuracy calcs\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "\n",
    "Create the training, validation, and test sets for numbers 5 and up! We minus 5 from each set to get n_inputs-1 classes (0 to 4)... but this time tehse represent the digits 5,6,7,8,9. \n",
    "\n",
    "We will also only keep 100 instances per class (and only 30 per class in the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    #for each class, select the first n rows (the data is already randomized)\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        #append the list of classes\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    #concatenate the lists into a single list (stack the data!)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's train the model. This is the same training code as earlier, using early stopping, except for the initialization: we first initialize all the variables, then we restore the best model trained earlier (on digits 0 to 4), and finally we reinitialize the output layer variables. REBUILD the graph below for easier viewing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/MODEL1_0_to_4\n",
      "0\tValidation loss: 1.159437\tBest loss: 1.159437\tAccuracy: 50.67%\n",
      "1\tValidation loss: 1.082819\tBest loss: 1.082819\tAccuracy: 58.67%\n",
      "2\tValidation loss: 1.010775\tBest loss: 1.010775\tAccuracy: 56.00%\n",
      "3\tValidation loss: 0.964012\tBest loss: 0.964012\tAccuracy: 64.00%\n",
      "4\tValidation loss: 0.938321\tBest loss: 0.938321\tAccuracy: 66.67%\n",
      "5\tValidation loss: 0.943860\tBest loss: 0.938321\tAccuracy: 66.67%\n",
      "6\tValidation loss: 0.906695\tBest loss: 0.906695\tAccuracy: 67.33%\n",
      "7\tValidation loss: 0.894095\tBest loss: 0.894095\tAccuracy: 66.00%\n",
      "8\tValidation loss: 0.933458\tBest loss: 0.894095\tAccuracy: 65.33%\n",
      "9\tValidation loss: 0.884227\tBest loss: 0.884227\tAccuracy: 70.00%\n",
      "10\tValidation loss: 0.922858\tBest loss: 0.884227\tAccuracy: 65.33%\n",
      "11\tValidation loss: 0.957360\tBest loss: 0.884227\tAccuracy: 59.33%\n",
      "12\tValidation loss: 0.942646\tBest loss: 0.884227\tAccuracy: 64.67%\n",
      "13\tValidation loss: 0.907987\tBest loss: 0.884227\tAccuracy: 64.00%\n",
      "14\tValidation loss: 0.923313\tBest loss: 0.884227\tAccuracy: 64.00%\n",
      "15\tValidation loss: 0.925488\tBest loss: 0.884227\tAccuracy: 66.67%\n",
      "16\tValidation loss: 0.855799\tBest loss: 0.855799\tAccuracy: 70.00%\n",
      "17\tValidation loss: 0.863257\tBest loss: 0.855799\tAccuracy: 67.33%\n",
      "18\tValidation loss: 0.851412\tBest loss: 0.851412\tAccuracy: 69.33%\n",
      "19\tValidation loss: 0.822791\tBest loss: 0.822791\tAccuracy: 70.67%\n",
      "20\tValidation loss: 0.854854\tBest loss: 0.822791\tAccuracy: 68.67%\n",
      "21\tValidation loss: 0.835449\tBest loss: 0.822791\tAccuracy: 72.00%\n",
      "22\tValidation loss: 0.836574\tBest loss: 0.822791\tAccuracy: 70.67%\n",
      "23\tValidation loss: 0.829875\tBest loss: 0.822791\tAccuracy: 74.00%\n",
      "24\tValidation loss: 0.855616\tBest loss: 0.822791\tAccuracy: 70.67%\n",
      "25\tValidation loss: 0.846408\tBest loss: 0.822791\tAccuracy: 70.67%\n",
      "26\tValidation loss: 0.849811\tBest loss: 0.822791\tAccuracy: 72.00%\n",
      "27\tValidation loss: 0.846415\tBest loss: 0.822791\tAccuracy: 72.67%\n",
      "28\tValidation loss: 0.822124\tBest loss: 0.822124\tAccuracy: 71.33%\n",
      "29\tValidation loss: 0.862546\tBest loss: 0.822124\tAccuracy: 68.00%\n",
      "30\tValidation loss: 0.826026\tBest loss: 0.822124\tAccuracy: 74.67%\n",
      "31\tValidation loss: 0.851653\tBest loss: 0.822124\tAccuracy: 68.67%\n",
      "32\tValidation loss: 0.814521\tBest loss: 0.814521\tAccuracy: 75.33%\n",
      "33\tValidation loss: 0.841021\tBest loss: 0.814521\tAccuracy: 72.67%\n",
      "34\tValidation loss: 0.830823\tBest loss: 0.814521\tAccuracy: 70.67%\n",
      "35\tValidation loss: 0.832538\tBest loss: 0.814521\tAccuracy: 71.33%\n",
      "36\tValidation loss: 0.871230\tBest loss: 0.814521\tAccuracy: 71.33%\n",
      "37\tValidation loss: 0.838563\tBest loss: 0.814521\tAccuracy: 74.00%\n",
      "38\tValidation loss: 0.833198\tBest loss: 0.814521\tAccuracy: 70.67%\n",
      "39\tValidation loss: 0.829671\tBest loss: 0.814521\tAccuracy: 70.67%\n",
      "40\tValidation loss: 0.852181\tBest loss: 0.814521\tAccuracy: 72.00%\n",
      "41\tValidation loss: 0.824452\tBest loss: 0.814521\tAccuracy: 72.67%\n",
      "42\tValidation loss: 0.899435\tBest loss: 0.814521\tAccuracy: 71.33%\n",
      "Early stopping!\n",
      "Total training time: 14.2s\n",
      "INFO:tensorflow:Restoring parameters from ./tf_logs/my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 68.20%\n"
     ]
    }
   ],
   "source": [
    "#########################################################################################\n",
    "################################# CONSTRUCTION PHASE ####################################\n",
    "#########################################################################################\n",
    "\n",
    "#restore graph\n",
    "reset_graph()\n",
    "restore_meta = tf.train.import_meta_graph(\"./tf_logs/MODEL1_0_to_4.meta\") #import meta data for graph\n",
    "\n",
    "#grab variables that we need to reference\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "y_proba = tf.get_default_graph().get_tensor_by_name(\"y_proba:0\")\n",
    "logits = y_proba.op.inputs[0]\n",
    "mean_loss = tf.get_default_graph().get_tensor_by_name(\"mean_loss:0\")\n",
    "\n",
    "#first input of y_proba (the softmax of logits) is the logitss\n",
    "#this could also be found by reffering to the 'logits' layer\n",
    "#order of calcs in graph:\n",
    "    #1. logits\n",
    "    #2. y_proba (though this isnt used in calc)\n",
    "    #3. cross_entropy (though this isnt used in calc)\n",
    "    #4. mean_loss and mean_accuracy\n",
    "\n",
    "#freeze lower layers\n",
    "learning_rate = 0.01\n",
    "logit_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='output_logits')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(mean_loss, var_list=logit_vars)\n",
    "\n",
    "#create accuracy calcs\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()\n",
    "\n",
    "#########################################################################################\n",
    "################################## EVALUATION PHASE #####################################\n",
    "#########################################################################################\n",
    "\n",
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 10\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_meta.restore(sess, \"./tf_logs/MODEL1_0_to_4\")\n",
    "    \n",
    "    #this was in original code, but why do I need them?!? Doesnt the init.run() take care of this??!?!?\n",
    "    #for var in logit_vars:\n",
    "    #    var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([mean_loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./tf_logs/my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./tf_logs/my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This wasnt in original Code, but I think you could also do this:\n",
    "\n",
    "#### Get rid of the 'logits' and 'y_proba' vals, and just evaluate the 'mean_accuracy' and 'mean_loss' by calling those tensors directly... isnt that easier? I think the result should be the exact same as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/MODEL1_0_to_4\n",
      "0\tValidation loss: 1.159437\tBest loss: 1.159437\tAccuracy: 50.67%\n",
      "1\tValidation loss: 1.082819\tBest loss: 1.082819\tAccuracy: 58.67%\n",
      "2\tValidation loss: 1.010775\tBest loss: 1.010775\tAccuracy: 56.00%\n",
      "3\tValidation loss: 0.964012\tBest loss: 0.964012\tAccuracy: 64.00%\n",
      "4\tValidation loss: 0.938321\tBest loss: 0.938321\tAccuracy: 66.67%\n",
      "5\tValidation loss: 0.943860\tBest loss: 0.938321\tAccuracy: 66.67%\n",
      "6\tValidation loss: 0.906695\tBest loss: 0.906695\tAccuracy: 67.33%\n",
      "7\tValidation loss: 0.894095\tBest loss: 0.894095\tAccuracy: 66.00%\n",
      "8\tValidation loss: 0.933458\tBest loss: 0.894095\tAccuracy: 65.33%\n",
      "9\tValidation loss: 0.884227\tBest loss: 0.884227\tAccuracy: 70.00%\n",
      "10\tValidation loss: 0.922858\tBest loss: 0.884227\tAccuracy: 65.33%\n",
      "11\tValidation loss: 0.957360\tBest loss: 0.884227\tAccuracy: 59.33%\n",
      "12\tValidation loss: 0.942646\tBest loss: 0.884227\tAccuracy: 64.67%\n",
      "13\tValidation loss: 0.907987\tBest loss: 0.884227\tAccuracy: 64.00%\n",
      "14\tValidation loss: 0.923313\tBest loss: 0.884227\tAccuracy: 64.00%\n",
      "Early stopping!\n",
      "Total training time: 10.6s\n",
      "INFO:tensorflow:Restoring parameters from ./tf_logs/my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 64.82%\n"
     ]
    }
   ],
   "source": [
    "#########################################################################################\n",
    "################################# CONSTRUCTION PHASE ####################################\n",
    "#########################################################################################\n",
    "\n",
    "#restore graph\n",
    "reset_graph()\n",
    "restore_meta = tf.train.import_meta_graph(\"./tf_logs/MODEL1_0_to_4.meta\") #import meta data for graph\n",
    "\n",
    "#grab variables that we need to reference\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "mean_loss = tf.get_default_graph().get_tensor_by_name(\"mean_loss:0\")\n",
    "mean_accuracy = tf.get_default_graph().get_tensor_by_name(\"mean_accuracy:0\")\n",
    "\n",
    "#freeze lower layers\n",
    "learning_rate = 0.01\n",
    "logit_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='output_logits')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(mean_loss, var_list=logit_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()\n",
    "\n",
    "#########################################################################################\n",
    "################################## EVALUATION PHASE #####################################\n",
    "#########################################################################################\n",
    "\n",
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 5\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_meta.restore(sess, \"./tf_logs/MODEL1_0_to_4\")\n",
    "\n",
    "    t0 = time.time()  \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([mean_loss, mean_accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./tf_logs/my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./tf_logs/my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = mean_accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brutal accuracy... but thats what you expect when training only a single layer!!! However the model did run super fast which is pretty sweet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try caching the frozen layers, and train the model again: how much faster is it now?\n",
    "\n",
    "Let's start by getting a handle on the output of the last frozen layer. Then train the model using roughly the same code as earlier. The difference is that we **compute the output of the top frozen layer at the beginning (both for the training set and the validation set), and we cache it.** This makes training roughly 1.5 to 3 times faster in this example (this may vary greatly, depending on your system):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/MODEL1_0_to_4\n",
      "0\tValidation loss: 1.159436\tBest loss: 1.159436\tAccuracy: 50.67%\n",
      "1\tValidation loss: 1.082819\tBest loss: 1.082819\tAccuracy: 58.67%\n",
      "2\tValidation loss: 1.010775\tBest loss: 1.010775\tAccuracy: 56.00%\n",
      "3\tValidation loss: 0.964013\tBest loss: 0.964013\tAccuracy: 64.00%\n",
      "4\tValidation loss: 0.938321\tBest loss: 0.938321\tAccuracy: 66.67%\n",
      "5\tValidation loss: 0.943860\tBest loss: 0.938321\tAccuracy: 66.67%\n",
      "6\tValidation loss: 0.906695\tBest loss: 0.906695\tAccuracy: 67.33%\n",
      "7\tValidation loss: 0.894095\tBest loss: 0.894095\tAccuracy: 66.00%\n",
      "8\tValidation loss: 0.933458\tBest loss: 0.894095\tAccuracy: 65.33%\n",
      "9\tValidation loss: 0.884227\tBest loss: 0.884227\tAccuracy: 70.00%\n",
      "10\tValidation loss: 0.922858\tBest loss: 0.884227\tAccuracy: 65.33%\n",
      "11\tValidation loss: 0.957360\tBest loss: 0.884227\tAccuracy: 59.33%\n",
      "12\tValidation loss: 0.942646\tBest loss: 0.884227\tAccuracy: 64.67%\n",
      "13\tValidation loss: 0.907987\tBest loss: 0.884227\tAccuracy: 64.00%\n",
      "14\tValidation loss: 0.923312\tBest loss: 0.884227\tAccuracy: 64.00%\n",
      "Early stopping!\n",
      "Total training time: 10.5s\n",
      "INFO:tensorflow:Restoring parameters from ./tf_logs/my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 64.82%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#########################################################################################\n",
    "################################# CONSTRUCTION PHASE ####################################\n",
    "#########################################################################################\n",
    "\n",
    "#restore graph\n",
    "reset_graph()\n",
    "restore_meta = tf.train.import_meta_graph(\"./tf_logs/MODEL1_0_to_4.meta\") #import meta data for graph\n",
    "\n",
    "#grab variables that we need to reference\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "mean_loss = tf.get_default_graph().get_tensor_by_name(\"mean_loss:0\")\n",
    "mean_accuracy = tf.get_default_graph().get_tensor_by_name(\"mean_accuracy:0\")\n",
    "\n",
    "#freeze lower layers\n",
    "learning_rate = 0.01\n",
    "logit_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='output_logits')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(mean_loss, var_list=logit_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()\n",
    "\n",
    "#get frozen layer tensor\n",
    "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"hidden_5_ACTIVATED:0\")\n",
    "\n",
    "#########################################################################################\n",
    "################################## EVALUATION PHASE #####################################\n",
    "#########################################################################################\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = \n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_meta.restore(sess, \"./tf_logs/MODEL1_0_to_4\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    #cache hidden_5_ACTIVATED \n",
    "    hidden5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        #replace X with hidden5_out values\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h5_batch, y_batch = hidden5_train[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([mean_loss, mean_accuracy], feed_dict={hidden5_out: hidden5_valid, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./tf_logs/my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./tf_logs/my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = mean_accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK... soooo no difference in training time or accuracy... perhaps I am not running the model long enough (increase max epochs) or there is something wrong in the above code. We will have to dive into this again later, because right now we need to dive into recurrent neural nets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
