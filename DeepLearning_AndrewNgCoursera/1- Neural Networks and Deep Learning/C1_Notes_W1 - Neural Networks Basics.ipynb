{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1_Notes_W1 - Neural Networks Basics\n",
    "\n",
    "This is the first course of the deep learning specialization at [Coursera](https://www.coursera.org/specializations/deep-learning) which is moderated by [DeepLearning.ai](http://deeplearning.ai/). The course is taught by Andrew Ng.\n",
    "\n",
    "## Table of contents\n",
    "* [Course summary](#course-summary)\n",
    "* [1. Introduction to deep learning](#introduction-to-deep-learning)\n",
    "  * [1.1 What is a (Neural Network) NN?](#what-is-a-neural-network-nn)\n",
    "  * [1.2 Supervised learning with neural networks](#supervised-learning-with-neural-networks)\n",
    "  * [1.3 Why is deep learning taking off?](#why-is-deep-learning-taking-off)\n",
    "* [2. Neural Networks Basics](#neural-networks-basics)\n",
    "  * [2.1 Binary classification](#binary-classification)\n",
    "  * [2.2 Logistic regression](#logistic-regression)\n",
    "  * [2.3 Logistic regression cost function](#logistic-regression-cost-function)\n",
    "  * [2.4 Gradient Descent](#gradient-descent)\n",
    "  * [2.5 Derivatives](#derivatives)\n",
    "  * [2.6 More Derivatives examples](#more-derivatives-examples)\n",
    "  * [2.7 Computation graph](#computation-graph)\n",
    "  * [2.8 Derivatives with a Computation Graph](#derivatives-with-a-computation-graph)\n",
    "  * [2.9 Logistic Regression Gradient Descent](#logistic-regression-gradient-descent)\n",
    "  * [2.10 Gradient Descent on m Examples](#gradient-descent-on-m-examples)\n",
    "  * [2.11 Vectorization](#vectorization)\n",
    "  * [2.12 Vectorizing Logistic Regression](#vectorizing-logistic-regression)\n",
    "  * [2.13 Notes on Python and NumPy](#notes-on-python-and-numpy)\n",
    "  * [2.14 General Notes](#general-notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course summary\n",
    "\n",
    "Here are the course summary as its given on the course [link](https://www.coursera.org/learn/neural-networks-deep-learning):\n",
    "\n",
    "> If you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new \"superpower\" that will let you build AI systems that just weren't possible a few years ago. \n",
    ">\n",
    "> In this course, you will learn the foundations of deep learning. When you finish this class, you will:\n",
    "> - Understand the major technology trends driving Deep Learning\n",
    "> - Be able to build, train and apply fully connected deep neural networks \n",
    "> - Know how to implement efficient (vectorized) neural networks \n",
    "> - Understand the key parameters in a neural network's architecture \n",
    ">\n",
    "> This course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to deep learning\n",
    "\n",
    "> Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.\n",
    "\n",
    "## 1.1 What is a (Neural Network) NN?\n",
    "\n",
    "- Single neuron == linear regression\n",
    "- Simple NN graph:\n",
    "  - ![](Images/Others/01.jpg)\n",
    "  - Image taken from [tutorialspoint.com](tutorialspoint.com)\n",
    "- RELU stands for rectified linear unit is the most popular activation function right now that makes deep NNs train faster now.\n",
    "- Hidden layers predicts connection between inputs automatically, thats what deep learning is good at.\n",
    "- Deep NN consists of more hidden layers (Deeper layers)\n",
    "  - ![](Images/Others/02.png)\n",
    "  - Image taken from [opennn.net](opennn.net)\n",
    "- Each Input will be connected to the hidden layer and the NN will decide the connections.\n",
    "- Supervised learning means we have the (X,Y) and we need to get the function that maps X to Y.\n",
    "\n",
    "## 1.2 Supervised learning with neural networks\n",
    "\n",
    "- Different types of neural networks for supervised learning which includes:\n",
    "  - CNN or convolutional neural networks (Useful in computer vision)\n",
    "  - RNN or Recurrent neural networks (Useful in Speech recognition or NLP)\n",
    "  - Standard NN (Useful for Structured data)\n",
    "  - Hybrid/custom NN or a Collection of NNs types\n",
    "- Structured data is like the databases and tables.\n",
    "- Unstructured data is like images, video, audio, and text.\n",
    "- Structured data gives more money because companies relies on prediction on its big data.\n",
    "\n",
    "## 1.2 Why is deep learning taking off?\n",
    "\n",
    "- Deep learning is taking off for 3 reasons:\n",
    "  1. Data:\n",
    "     - Using this image we can conclude:\n",
    "       - ![](Images/11.png)\n",
    "     - For small data NN can perform as Linear regression or SVM (Support vector machine)\n",
    "     - For big data a small NN is better that SVM\n",
    "     - For big data a big NN is better that a medium NN is better that small NN.\n",
    "     - Hopefully we have a lot of data because the world is using the computer a little bit more\n",
    "       - Mobiles\n",
    "       - IOT (Internet of things)\n",
    "  2. Computation:\n",
    "     - GPUs.\n",
    "     - Powerful CPUs.\n",
    "     - Distributed computing.\n",
    "     - ASICs\n",
    "  3. Algorithm:\n",
    "     1. Creative algorithms has appeared that changed the way NN works.\n",
    "        - For example using RELU function is so much better than using SIGMOID function in training a NN because it helps with the vanishing gradient problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Neural Networks Basics\n",
    "\n",
    "> Learn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models.\n",
    "\n",
    "## 2.1 Binary classification\n",
    "\n",
    "- Mainly he is talking about how to do a logistic regression to make a binary classifier.\n",
    "  - ![log](Images/Others/03.png)\n",
    "  - Image taken from [3.bp.blogspot.com](http://3.bp.blogspot.com)\n",
    "- He talked about an example of knowing if the current image contains a cat or not.\n",
    "- Here are some notations:\n",
    "  - `M is the number of training vectors`\n",
    "  - `Nx is the size of the input vector`\n",
    "  - `Ny is the size of the output vector`\n",
    "  - `X(1) is the first input vector`\n",
    "  - `Y(1) is the first output vector`\n",
    "  - `X = [x(1) x(2).. x(M)]`\n",
    "  - `Y = (y(1) y(2).. y(M))`\n",
    "- We will use python in this course.\n",
    "- In NumPy we can make matrices and make operations on them in a fast and reliable time.\n",
    "# NOTE: We stack observations along the COLUMNS NOT ROWS [x.shape = (n,m) where n is num_feats and m is num_obs] ... this makes implementation for neural networks a lot easier. \n",
    "\n",
    "## 2.2 Logistic regression\n",
    "\n",
    "- Algorithm is used for classification algorithm of 2 classes.\n",
    "- Equations:\n",
    "  - Simple equation:\t`y = wx + b`\n",
    "  - If x is a vector: `y = w(transpose)x + b`\n",
    "  - If we need y to be in between 0 and 1 (probability): `y = sigmoid(w(transpose)x + b)`\n",
    "  - In some notations this might be used: `y = sigmoid(w(transpose)x)` \n",
    "    - While `b` is `w0` of `w` and we add `x0 = 1`. but we won't use this notation in the course (Andrew said that the first notation is better).\n",
    "- In binary classification `Y` has to be between `0` and `1`.\n",
    "- In the last equation `w` is a vector of `Nx` and `b` is a real number\n",
    "\n",
    "\n",
    "## 2.3 Logistic regression cost function\n",
    "\n",
    "### NON CONVEX (we wont use this):\n",
    "- First loss function would be the square root error:  `L(y',y) = 1/2 (y' - y)^2`\n",
    "  - But we won't use this notation because it leads us to optimization problem which is **non convex, means it contains local optimum points.**\n",
    "\n",
    "### CONVEX (use this one!):\n",
    "- This is the function that we will use: `L(y',y) = -(y*log(y') + (1-y)*log(1-y'))`\n",
    "- To explain the last function lets see:\n",
    "  - if `y = 1` ==> `L(y',1) = -log(y')`  ==> we want `y'` to be the largest   ==> `y`' biggest value is 1\n",
    "  - if `y = 0` ==> `L(y',0) = -log(1-y')` ==> we want `1-y'` to be the largest ==> `y'` to be smaller as possible because it can only has 1 value.\n",
    "- Then the Cost function will be: `J(w,b) = (1/m) * Sum(L(y'[i],y[i]))`\n",
    "- The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.\n",
    "\n",
    "![](images/log_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Gradient Descent\n",
    "\n",
    "- We want to predict `w` and `b` that minimize the cost function.\n",
    "- Our cost function is convex.\n",
    "- First we initialize `w` and `b` to 0,0 or initialize them to a random value in the convex function and then try to improve the values the reach minimum value.\n",
    "- In Logistic regression people always use 0,0 instead of random.\n",
    "- The gradient decent algorithm repeats: `w = w - alpha * dw`\n",
    "  where alpha is the learning rate and `dw` is the derivative of `w` (Change to `w`) \n",
    "  The derivative is also the slope of `w`\n",
    "- Looks like greedy algorithms. the derivative give us the direction to improve our parameters.\n",
    "\n",
    "\n",
    "- The actual equations we will implement:\n",
    "  - `w = w - alpha * d(J(w,b) / dw)`        (how much the function slopes in the w direction)\n",
    "  - `b = b - alpha * d(J(w,b) / db)`        (how much the function slopes in the d direction)\n",
    "\n",
    "![](images/grad_desc.png)\n",
    "\n",
    "### NOTE: If J (cost function) is a function of 2 or more parameters (ie. J(W,b)) then instead of lowercase (d) we used that squiggly d to denote a 'partial derivative'\n",
    "## 2.5 Derivatives\n",
    "\n",
    "- We will talk about some of required calculus.\n",
    "- You don't need to be a calculus geek to master deep learning but you'll need some skills from it.\n",
    "- Derivative of a linear line is its slope.\n",
    "- ex. `f(a) = 3a`\n",
    "  - `d(f(a))/d(a) = 3`\n",
    "- if `a = 2` then `f(a) = 6`\n",
    "- if we move a a little bit `a = 2.001` then `f(a) = 6.003` means that we multiplied the derivative (Slope) to the moved area and added it to the last result.\n",
    "\n",
    "### Deriviatve = Slope: when you nudge Wa up by 2, and J goes up by 4, then the partial derivate dWa (eg 'Wa' wrt J(Wa, b)) is 2 because J went up 2x as much as Wa. \n",
    "   - **However, as we know, derrivatives are defined by the slope of a cost function at a super small amount (using limits). But the intution holds.**\n",
    "   - Also, for non-linear funcitons (which is most functions), slope is non the same everywhere! \n",
    "\n",
    "![](images/slope.png)\n",
    "## 2.6 More Derivatives examples\n",
    "\n",
    "- `f(a) = a^2`  ==> `d(f(a))/d(a) = 2a`\n",
    "  - `a = 2`  ==> `f(a) = 4`\n",
    "  - `a = 2.0001` ==> `f(a) = 4.0004` approx.\n",
    "\n",
    "**THIS JUST MEANS THAT AT ANY POINT ON THE LINE, THE SLOPE WILL BE 2x THE VALUE OF 'a'... Meaning that if we increase 'a' by 'a_delta', the value of f(a) will increase by a_delta\\*2**  \n",
    " \n",
    "- `f(a) = a^3`  ==> `d(f(a))/d(a) = 3a^2`\n",
    "- `f(a) = log(a)`  ==> `d(f(a))/d(a) = 1/a`\n",
    "\n",
    "**THIS MEANS THAT FOR ANY VALUE OF 'a' THE SLOPE WILL BE 3a^2... AND IF WE INCREASE A BY 'a_delta', THEN f(a) will increase by 3\\*a_delta^2**  \n",
    "- To conclude, Derivative is the slope and slope is different in different points in the function thats why the derivative is a function.\n",
    "\n",
    "![](images/calculus1.png)\n",
    "\n",
    "## NOTE: to get the different calculus rules that determine the derivatives for different functions, we need to open a calculus textbook! These all can obviously be proved mathermatically... but we wont do that here. Take a calculus course for that stuff!\n",
    "\n",
    "![](images/calculus2.png)\n",
    "\n",
    "\n",
    "![](images/c1w1n_notes1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Computation graph\n",
    "\n",
    "- Its a graph that organizes the computation from left to right.\n",
    "- Its pretty much breaking down the function into its component parts!\n",
    "  - ![](Images/02.png)\n",
    "\n",
    "## 2.8 Derivatives with a Computation Graph\n",
    "\n",
    "- Calculus chain rule says:\n",
    "  If `x -> y -> z`          (x effect y and y effects z)\n",
    "  Then `d(z)/d(x) = d(z)/d(y) * d(y)/d(x)`\n",
    "- The video illustrates a big example.\n",
    "  - ![](Images/03.png)\n",
    "- We compute the derivatives on a graph from right to left and it will be a lot more easier.\n",
    "- `dvar` means the derivatives of a final output variable with respect to various intermediate quantities.\n",
    "\n",
    "### NOTE: In code, 'dvar' really means dJ/dvar... Derivative of J wrt variable\n",
    "\n",
    "## 2.9 Logistic Regression Gradient Descent\n",
    "\n",
    "- In the video he discussed the derivatives of gradient decent example for one sample with two features `x1` and `x2`.\n",
    "![](images/c1w1n_logreg.png)\n",
    "![](Images/04.png)\n",
    "\n",
    "## 2.10 Gradient Descent on m Examples\n",
    "\n",
    "- Lets say we have these variables:\n",
    "\n",
    "  ```\n",
    "  \tX1\t\t\t\t\tFeature\n",
    "  \tX2                  Feature\n",
    "  \tW1                  Weight of the first feature.\n",
    "  \tW2                  Weight of the second feature.\n",
    "  \tB                   Logistic Regression parameter.\n",
    "  \tM                   Number of training examples\n",
    "  \tY(i)\t\t\t\tExpected output of i\n",
    "  ```\n",
    "\n",
    "- So we have:\n",
    "  ![](Images/09.png)\n",
    "\n",
    "- Then from right to left we will calculate derivations compared to the result:\n",
    "\n",
    "  ```\n",
    "  \td(a)  = d(l)/d(a) = -(y/a) + ((1-y)/(1-a))\n",
    "  \td(z)  = d(l)/d(z) = a - y\n",
    "  \td(W1) = X1 * d(z)\n",
    "  \td(W2) = X2 * d(z)\n",
    "  \td(B) = d(z)\n",
    "  ```\n",
    "\n",
    "- From the above we can conclude the logistic regression pseudo code:\n",
    "\n",
    "  ```\n",
    "  \tJ = 0; dw1 = 0; dw2 =0; db = 0;                 # Devs.\n",
    "  \tw1 = 0; w2 = 0; b=0;\t\t\t\t\t\t\t# Weights\n",
    "  \tfor i = 1 to m\n",
    "  \t\t# Forward pass\n",
    "  \t\tz(i) = W1*x1(i) + W2*x2(i) + b\n",
    "  \t\ta(i) = Sigmoid(z(i))\n",
    "  \t\tJ += (Y(i)*log(a(i)) + (1-Y(i))*log(1-a(i)))\n",
    "  \t\t\n",
    "  \t\t# Backward pass\n",
    "        # ADD GRADIENTS TOGETHER FROM EACH OBSERVATION\n",
    "        # The same gradients will be added for every additional feature in the logisitic regression model\n",
    "        # Two features is easy, but you will probably need another for loop to go over ALL features if n grows\n",
    "  \t\tdz(i) = a(i) - Y(i)\n",
    "  \t\tdw1 += dz(i) * x1(i)\n",
    "  \t\tdw2 += dz(i) * x2(i)\n",
    "  \t\tdb  += dz(i)\n",
    "    \n",
    "    # AVERAGE THE GRADIENTS OVER M (NUMBER OF OBSERVATIONS)\n",
    "    # THESE ARE THE GRADIENTS FOR A SINGLE STEP OF GS\n",
    "  \tJ /= m\n",
    "  \tdw1/= m\n",
    "  \tdw2/= m\n",
    "  \tdb/= m\n",
    "  \t\n",
    "  \t# Gradient descent: SINGLE STEP\n",
    "  \tw1 = w1 - alpa * dw1\n",
    "  \tw2 = w2 - alpa * dw2\n",
    "  \tb = b - alpa * db\n",
    "  ```\n",
    "\n",
    "- EVERYTHING ABOVE REPRESENTS **A SINGLE STEP OF GRADIENT DESCENT** because we are only updating the gradients once. The above code should run for some iterations to minimize error.\n",
    "\n",
    "- So there will be two inner loops to implement the logistic regression:\n",
    "    - 1. For loop to loop over EACH training example\n",
    "    - 2. For loop to do the gradient calcs for each feature weights... So with 2 weights we dont need this, but if you hvae 100 features (n_x = 100) you dont want to write a line for each of dW1, dw2... dw100... so you will need another loop!\n",
    "- **This is SUPER INEFFICIENT!!!** And you will also need another loop to go through the number of gradient descent updates you want... remember, the code above only updates the gradients ONCE!!! \n",
    "\n",
    "- Vectorization can help us solve these inneficiencies... **We will get do a single iteration of gradient descent WITHOUT ANY FOR LOOPS**. \n",
    "- Vectorization is so important on deep learning to reduce loops. In the last code we can make the whole loop in one step using vectorization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Vectorization\n",
    "\n",
    "- Deep learning shines when the dataset are big. However for loops will make you wait a lot for a result. Thats why we need vectorization to get rid of some of our for loops.\n",
    "- NumPy library (dot) function is using vectorization by default.\n",
    "- The vectorization can be done on CPU or GPU thought the SIMD operation. But its faster on GPU.\n",
    "- Whenever possible avoid for loops.\n",
    "- Most of the NumPy library methods are vectorized version.\n",
    "\n",
    "### Vectorizing (as opposed to for loops) can speed things up by a crazy amount... Below, our code runs 300x faster!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250329.512541\n",
      "Vectorized runtime (milsec): 3.503084182739258\n",
      "250329.512541\n",
      "For Loop runtime (milsec): 946.1748600006104\n"
     ]
    }
   ],
   "source": [
    "# SIMPLE MULTIPLICATION EXAMPLE\n",
    "import time\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "# RUN VECTORIZED MATRIX MULTIPLICATION\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"Vectorized runtime (milsec): {}\".format(1000*(toc-tic)))\n",
    "\n",
    "# RUN NON-VECTORIZED MATRIX MULTIPLICATION\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"For Loop runtime (milsec): {}\".format(1000*(toc-tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized runtime (milsec): 1.5192031860351562\n",
      "For Loop runtime (milsec): 45.53103446960449\n",
      "Both vectors equal:  True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "n=100000\n",
    "a = np.random.rand(n)\n",
    "\n",
    "# RUN VECTORIZED MATRIX MULTIPLICATION\n",
    "tic = time.time()\n",
    "c1 = np.exp(a)\n",
    "toc = time.time()\n",
    "print(\"Vectorized runtime (milsec): {}\".format(1000*(toc-tic)))\n",
    "\n",
    "# RUN NON-VECTORIZED MATRIX MULTIPLICATION\n",
    "c2 = np.zeros((n,))\n",
    "tic = time.time()\n",
    "for i in range(n):\n",
    "    c2[i] = math.exp(a[i])\n",
    "toc = time.time()\n",
    "print(\"For Loop runtime (milsec): {}\".format(1000*(toc-tic)))\n",
    "print(\"Both vectors equal: \",all(c1==c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Try to always use proper shapes with your vectors and matrices (ie avoid shapes like (5,).\n",
    "\n",
    "#### a.shape = (5,) is refered to as a \"rank 1 array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5, 1)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "DON'T USE 1 RANK ARRAYS U IDIOT!!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-dc964f637be7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DON'T USE 1 RANK ARRAYS U IDIOT!!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: DON'T USE 1 RANK ARRAYS U IDIOT!!"
     ]
    }
   ],
   "source": [
    "#RANK ONE ARRAY\n",
    "print(np.random.rand(5).shape)\n",
    "\n",
    "# REGULAR VECTOR\n",
    "print(np.random.rand(5,1).shape)\n",
    "\n",
    "# use assert function to check your shapes\n",
    "a = np.random.rand(5,1)\n",
    "b = np.random.rand(5,)\n",
    "assert a.shape == (5,1)\n",
    "assert b.shape == (5,1), \"DON'T USE 1 RANK ARRAYS U IDIOT!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Vectorizing Logistic Regression\n",
    "- Whenever possible, we want to avoid for loops (either using a built in function, or vectorizing)...\n",
    "    - For instance, np.log and np.exp are way faster than doing for loops.\n",
    "    - **Always look for a built in function first before doing a for loop!**\n",
    "- We will implement Logistic Regression using one for loop then without any for loop.\n",
    "- As an input we have a matrix `X` and its `[Nx, m]` and a matrix `Y` and its `[Ny, m]`.\n",
    "- We will then compute at instance `[z1,z2...zm] = W.T * X + [b,b,...b]`... \n",
    "    - As we know, we have concatenated ALL of the weights and ALL of the X features into a single matrix each. So instead of doing separate vector calcs for W1X2 and W2X2, we can simply multiply the two matrices together: W1\\*X1 + W2\\*X2 ==> W\\*X This is basic shit, but its good to review!\n",
    "\n",
    "This can be written in python as:\n",
    "            \n",
    "        # Vectorization, then broadcasting, Z shape is (1, m)\n",
    "        Z = np.dot(W.T,X) + b\n",
    "        \n",
    "        # Vectorization, A shape is (1, m)\n",
    "        # VECTORIZED SIGMOID FUNCTION\n",
    "        A = 1 / 1 + np.exp(-Z)   \n",
    "\n",
    "Vectorizing Logistic Regression's Gradient Output:\n",
    "            \n",
    "        # Vectorization, dz shape is (1, m)\n",
    "        dz = A - Y                 \n",
    "\n",
    "        # Vectorization, dw shape is (Nx, 1)\n",
    "        dw = np.dot(X, dz.T) / m\n",
    "\n",
    "        # Vectorization, dz shape is (1, 1)\n",
    "        db = dz.sum() / m    \n",
    "        \n",
    "![](images/c1w1n_npgs.png)\n",
    "![](images/c1w1n_npgs2.png)\n",
    "\n",
    "## 2.13 Notes on Python and NumPy\n",
    "\n",
    "- In NumPy, `obj.sum(axis = 0)` sums the columns while `obj.sum(axis = 1)` sums the rows.\n",
    "- In NumPy, `obj.reshape(1,4)` changes the shape of the matrix by broadcasting the values.\n",
    "- Reshape is cheap in calculations so put it everywhere you're not sure about the calculations.\n",
    "- Broadcasting works when you do a matrix operation with matrices that doesn't match for the operation, in this case NumPy automatically makes the shapes ready for the operation by broadcasting the values.\n",
    "- Some tricks to eliminate all the strange bugs in the code:\n",
    "  - If you didn't specify the shape of a vector, it will take a shape of `(m,)` and the transpose operation won't work. You have to reshape it to `(m, 1)`\n",
    "  - Try to not use the rank one matrix in ANN\n",
    "  - Don't hesitate to use `assert(a.shape == (5,1))` to check if your matrix shape is the required one.\n",
    "  - If you've found a rank one matrix try to run reshape on it.\n",
    "- Jupyter / IPython notebooks are so useful library in python that makes it easy to integrate code and document at the same time. It runs in the browser and doesn't need an IDE to run.\n",
    "  - To open Jupyter Notebook, open the command line and call: `jupyter-notebook` It should be installed to work.\n",
    "- To Compute the derivative of Sigmoid:\n",
    "\n",
    "  ```\n",
    "    # derivative  using calculus\n",
    "  \ts = sigmoid(x)\n",
    "  \tds = s * (1 - s)      \n",
    "  ```\n",
    "\n",
    "- To make an image of `(width,height,depth)` be a vector, use this:\n",
    "\n",
    "  ```\n",
    "  #reshapes the image.\n",
    "  v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)  \n",
    "  ```\n",
    "\n",
    "![](images/c1w1n_broadcast.png)\n",
    "- Gradient descent converges faster after normalization of the input matrices.\n",
    "\n",
    "## 2.14 General Notes\n",
    "\n",
    "- The main steps for building a Neural Network are:\n",
    "  - Define the model structure (such as number of input features and outputs)\n",
    "  - Initialize the model's parameters.\n",
    "  - Loop.\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "- Preprocessing the dataset is important.\n",
    "- Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm.\n",
    "- [kaggle.com](kaggle.com) is a good place for datasets and competitions.\n",
    "- [Pieter Abbeel](https://www2.eecs.berkeley.edu/Faculty/Homepages/abbeel.html) is one of the best in deep reinforcement learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA: Logistic Regression Cost Function\n",
    "![](images/c1w1n_extrtalog.png)\n",
    "![](images/c1w1n_extrtalog2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Written Notes Review\n",
    "\n",
    "![](handnotes/c1w1n_notes1.jpg)\n",
    "![](handnotes/c1w1n_notes2.jpg)\n",
    "![](handnotes/c1w1n_notes3a.jpg)\n",
    "![](handnotes/c1w1n_notes3b.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
