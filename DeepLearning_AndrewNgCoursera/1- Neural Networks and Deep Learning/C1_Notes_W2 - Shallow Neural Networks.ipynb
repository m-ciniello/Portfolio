{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1_Notes_W2 - Shallow Neural Networks\n",
    "\n",
    "> Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.\n",
    "\n",
    "## Table of contents\n",
    "  * [1. Neural Networks Overview](#neural-networks-overview)\n",
    "  * [2. Neural Network Representation](#neural-network-representation)\n",
    "  * [3. Computing a Neural Network's Output](#computing-a-neural-networks-output)\n",
    "  * [4. Vectorizing across multiple examples](#vectorizing-across-multiple-examples)\n",
    "  * [5. Activation functions](#activation-functions)\n",
    "  * [6. Why do you need non-linear activation functions?](#why-do-you-need-non-linear-activation-functions)\n",
    "  * [7. Derivatives of activation functions](#derivatives-of-activation-functions)\n",
    "  * [8. Gradient descent for Neural Networks](#gradient-descent-for-neural-networks)\n",
    "  * [9. Random Initialization](#random-initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Networks Overview \n",
    "\n",
    "- In logistic regression we had:\n",
    "\n",
    "  ```\n",
    "  X1  \\  \n",
    "  X2   ==>  z = XW + B ==> a = Sigmoid(z) ==> l(a,Y)\n",
    "  X3  /\n",
    "  ```\n",
    "\n",
    "- In neural networks with one layer we will have:\n",
    "\n",
    "  ```\n",
    "  X1  \\  \n",
    "  X2   =>  z1 = XW1 + B1 => a1 = Sigmoid(a1) => z2 = a1W2 + B2 => a2 = Sigmoid(z2) => l(a2,Y)\n",
    "  X3  /\n",
    "  ```\n",
    "\n",
    "\n",
    "- `X` is the input vector `(X1, X2, X3)`, and `Y` is the output variable `(1x1)`\n",
    "- NN is stack of logistic regression objects.\n",
    "\n",
    "![](images/c1w2n_basicnn.png)\n",
    "\n",
    "# 2. Neural Network Representation\n",
    "\n",
    "- We will define the neural networks that has one hidden layer.\n",
    "- NN contains of input layers, hidden layers, output layers.\n",
    "- Hidden layer means we cant see that layers in the training set.\n",
    "- a<sup>[0]</sup> = x (the input layer)\n",
    "- a<sup>[1]</sup> will represent the activation of the hidden neurons.\n",
    "- a<sup>[2]</sup> will represent the output layer.\n",
    "- We are talking about 2 layers NN. **The input layer isn't counted.**\n",
    "![](images/c1w2n_basicnn2.png)\n",
    "\n",
    "\n",
    "# 3. Computing a Neural Network's Output\n",
    "\n",
    "- Equations of Hidden layers:\n",
    "  ![](images/c1w2n_basicnn3.png)\n",
    "\n",
    "  - ![](Images/05.png)\n",
    "- Here is some informatios about the last image (**NOTE THESE CORRESPOND TO ONLY 1 ONSERVATION m=1)**:\n",
    "  - `noOfHiddenNeurons = 4`\n",
    "  - `Nx = 3`\n",
    "  - Shapes of the variables:\n",
    "    - `W1` is the matrix of the first hidden layer, it has a shape of `(noOfHiddenNeurons,nx)`\n",
    "    - `b1` is the matrix of the first hidden layer, it has a shape of `(noOfHiddenNeurons,1)`\n",
    "    - `z1` is the result of the equation `z1 = W1*X + b`, it has a shape of `(noOfHiddenNeurons,1)` \n",
    "    - `a1` is the result of the equation `a1 = sigmoid(z1)`, it has a shape of `(noOfHiddenNeurons,1)`\n",
    "    - `W2` is the matrix of the second hidden layer, it has a shape of `(1,noOfHiddenLayers)`\n",
    "    - `b2` is the matrix of the second hidden layer, it has a shape of `(1,1)`\n",
    "    - `z2` is the result of the equation `z2 = W2*a1 + b`, it has a shape of `(1,1)`\n",
    "    - `a2` is the result of the equation `a2 = sigmoid(z2)`, it has a shape of `(1,1)`\n",
    "        \n",
    "![](images/c1w2n_basicnn4.png)\n",
    "\n",
    "#### So to recap, we can think of each neuon as a single logistic regression, and we are just stacking them on top of one another. Each of those produces an output (a[1]i) and then those get fed into layer 2 (a[2]), which is also pretty much just another logisitic regression! (Though that obviously changes if you use a different activation function)\n",
    "\n",
    "\n",
    "# 4. Vectorizing across multiple examples\n",
    "\n",
    "- Pseudo code for forward propagation for the 2 layers NN:\n",
    "\n",
    "  ```\n",
    "  for i = 1 to m\n",
    "    # shape of z[1, i] is (noOfHiddenNeurons,1)\n",
    "    z[1, i] = W1*x[i] + b1      \n",
    "    \n",
    "    # shape of a[1, i] is (noOfHiddenNeurons,1)\n",
    "    a[1, i] = sigmoid(z[1, i])  \n",
    "    \n",
    "    # shape of z[2, i] is (1,1)\n",
    "    z[2, i] = W2*a[1, i] + b2   \n",
    "    \n",
    "    # shape of a[2, i] is (1,1)\n",
    "    a[2, i] = sigmoid(z[2, i])  \n",
    "  ```\n",
    "\n",
    "- So the new pseudo code:\n",
    "    - `X.shape = (Nx,m)`\n",
    "    - `W1.shape = (noOfHiddenNeurons, Nx)`\n",
    "    - `W2.shape = (1, noOfOutputNeurons)`\n",
    "    - `b.shape = (1, m)`\n",
    "\n",
    "  ```\n",
    "  Z1 = W1X + b1     # shape of Z1 (noOfHiddenNeurons,m)\n",
    "  A1 = sigmoid(Z1)  # shape of A1 (noOfHiddenNeurons,m)\n",
    "  Z2 = W2A1 + b2    # shape of Z2 is (1,m)\n",
    "  A2 = sigmoid(Z2)  # shape of A2 is (1,m)\n",
    "  ```\n",
    "\n",
    "- If you notice always m is the number of columns.\n",
    "- In the last example we can call `X` = `A0`. So the previous step can be rewritten as:\n",
    "\n",
    "  ```\n",
    "  Z1 = W1A0 + b1    # shape of Z1 (noOfHiddenNeurons,m)\n",
    "  A1 = sigmoid(Z1)  # shape of A1 (noOfHiddenNeurons,m)\n",
    "  Z2 = W2A1 + b2    # shape of Z2 is (1,m)\n",
    "  A2 = sigmoid(Z2)  # shape of A2 is (1,m)\n",
    "  ```\n",
    "\n",
    "![](images/c1w2n_basicnn5.png)\n",
    "![](images/c1w2n_basicnn6.png)\n",
    "\n",
    "\n",
    "# 5. Activation functions\n",
    "\n",
    "- So far we are using sigmoid, but in some cases other functions can be a lot better.\n",
    "- Sigmoid can lead us to gradient decent problem where the updates are so low... gradients get closer and closer to either zero or 1... so updates become slower and slower and gradients get smaller and smaller. \n",
    "- Sigmoid activation function range is [0,1]\n",
    "      \n",
    "      `# Where z is the input matrix\n",
    "      A = 1 / (1 + np.exp(-z))`\n",
    "      \n",
    "- Tanh activation function range is [-1,1]   (Shifted version of sigmoid function)\n",
    "  - In NumPy we can implement Tanh using one of these methods:\n",
    "    \n",
    "    `# Where z is the input matrix\n",
    "    A = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))`\n",
    "\n",
    "    Or\n",
    "    \n",
    "    `# Where z is the input matrix\n",
    "    A = np.tanh(z)   `\n",
    "    \n",
    "- It turns out that the tanh activation usually works better than sigmoid activation function for hidden units because the **mean of its output is closer to zero, and so it centers the data better for the next layer.**\n",
    "- Sigmoid or Tanh function disadvantage is that if the input is **too small or too high,** the slope will be near zero which will cause us the gradient decent problem.\n",
    "- One of the popular activation functions that solved the slow gradient decent is the RELU function.\n",
    "  \n",
    "  `# If z is negative the slope is 0`\n",
    "  `# if z is positive the slope remains linear.\n",
    "  RELU = max(0,z)`\n",
    "  \n",
    "- So here is some basic rule for choosing activation functions:\n",
    "    - if your classification is between 0 and 1, use the output activation as sigmoid and the others as RELU.\n",
    "    - Leaky RELU activation function is different from basic RELU because if the input is negative the slope will be super small. It works as well as RELU (and sometimes better) but most people uses RELU.\n",
    "\n",
    "  `#the 0.01 can be a parameter for your algorithm.\n",
    "  Leaky_RELU = max(0.01z,z)`\n",
    "  \n",
    "- In NN you will decide a lot of choices like:\n",
    "  - No of hidden layers.\n",
    "  - No of neurons in each hidden layer.\n",
    "  - **Learning rate. (The most important parameter)**\n",
    "  - Activation functions.\n",
    "  - And others..\n",
    "- It turns out there are no guide lines for that. You should try all activation functions for example.\n",
    "\n",
    "# 6. Why do you need non-linear activation functions?\n",
    "\n",
    "- If we removed the activation function from our algorithm, then we just have a linear activation function (identity functions).\n",
    "- Linear activation function will output linear activations\n",
    "  - Whatever hidden layers you add, the activation will be always linear like logistic regression (So its useless in a lot of complex problems)\n",
    "  - **IT TURNS OUT THAT A NEURAL NETWORK WITH ONLY LINEAR ACTIVATION FUNCTIONS IS USUALLY NO BETTER THAN A STRAIGHT LOGISTIC REGRESSION!!!**\n",
    "- **You might use linear activation function in one place - in the output layer if the output is real numbers (regression problem).** BUT even in this case if the output value is non-negative you could use RELU instead.\n",
    "\n",
    "# 7. Derivatives of activation functions\n",
    "\n",
    "- Derivation of Sigmoid activation function:\n",
    "\n",
    "  ```\n",
    "  g(z) = 1 / (1 + np.exp(-z))\n",
    "  g'(z) = (1 / (1 + np.exp(-z))) * (1 - (1 / (1 + np.exp(-z))))\n",
    "  g'(z) = g(z) * (1 - g(z))\n",
    "  ```\n",
    "\n",
    "- Derivation of Tanh activation function:\n",
    "\n",
    "  ```\n",
    "  g(z)  = (e^z - e^-z) / (e^z + e^-z)\n",
    "  g'(z) = 1 - np.tanh(z)^2 = 1 - g(z)^2\n",
    "  ```\n",
    "\n",
    "- Derivation of RELU activation function:\n",
    "\n",
    "  ```\n",
    "  g(z)  = np.maximum(0,z)\n",
    "  g'(z) = { 0  if z < 0\n",
    "            1  if z >= 0  }\n",
    "  ```\n",
    "\n",
    "- Derivation of leaky RELU activation function:\n",
    "\n",
    "  ```\n",
    "  g(z)  = np.maximum(0.01 * z, z)\n",
    "  g'(z) = { 0.01  if z < 0\n",
    "            1     if z >= 0   }\n",
    "  ```\n",
    "\n",
    "# 8. Gradient descent for Neural Networks\n",
    "- In this section we will have the full back propagation of the neural network (Just the equations with no explanations).\n",
    "- Gradient descent algorithm:\n",
    "  - NN parameters:\n",
    "    - `n[0] = Nx`\n",
    "    - `n[1] = NoOfHiddenNeurons`\n",
    "    - `n[2] = NoOfOutputNeurons = 1`\n",
    "    - `W1` shape is `(n[1],n[0])`\n",
    "    - `b1` shape is `(n[1],1)`\n",
    "    - `W2` shape is `(n[2],n[1])`\n",
    "    - `b2` shape is `(n[2],1)`\n",
    "  - Cost function `I =  I(W1, b1, W2, b2) = (1/m) * Sum(L(Y,A2))`\n",
    "  - Then Gradient descent:\n",
    "\n",
    "    ```\n",
    "    Repeat:\n",
    "    \t\tCompute predictions (y'[i], i = 0,...m)\n",
    "    \t\tGet derivatives: dW1, db1, dW2, db2\n",
    "    \t\tUpdate: W1 = W1 - LearningRate * dW1\n",
    "    \t\t\t\tb1 = b1 - LearningRate * db1\n",
    "    \t\t\t\tW2 = W2 - LearningRate * dW2\n",
    "    \t\t\t\tb2 = b2 - LearningRate * db2\n",
    "    ```\n",
    "\n",
    "- Forward propagation:\n",
    "\n",
    "  ```\n",
    "  Z1 = W1A0 + b1    # A0 is X\n",
    "  A1 = g1(Z1)\n",
    "  Z2 = W2A1 + b2\n",
    "  A2 = Sigmoid(Z2)      # Sigmoid because the output is between 0 and 1\n",
    "  ```\n",
    "\n",
    "- Backpropagation (derivations):   \n",
    "  ```\n",
    "  dZ2 = A2 - Y      # derivative of cost function we used * derivative of the sigmoid function\n",
    "  dW2 = (dZ2 * A1.T) / m\n",
    "  db2 = Sum(dZ2) / m\n",
    "  dZ1 = (W2.T * dZ2) * g'1(Z1)  # element wise product (*)\n",
    "  dW1 = (dZ1 * A0.T) / m   # A0 = X\n",
    "  db1 = Sum(dZ1) / m\n",
    "  # Hint there are transposes with multiplication because to keep dimensions correct\n",
    "  ```\n",
    "- How we derived the 6 equations of the backpropagation:   \n",
    "  ![](Images/06.png)\n",
    "\n",
    "# 9. Random Initialization\n",
    "\n",
    "- In logistic regression it wasn't important to initialize the weights randomly, while in NN we have to initialize them randomly.\n",
    "\n",
    "- If we initialize all the weights with zeros in NN it won't work (initializing bias with zero is OK):\n",
    "  - all hidden units will be completely identical (symmetric) - compute exactly the same function... So both hidden units will be the exact same, and we don't get any additional value from having multiple neurons!!!\n",
    "  - on each gradient descent iteration all the hidden units will always update the same\n",
    "\n",
    "![](images/c1w2n_basicnn7.png)\n",
    "- To solve this we initialize the W's with a small random numbers:\n",
    "\n",
    "  ```\n",
    "  W1 = np.random.randn((2,2)) * 0.01    # 0.01 to make it small enough\n",
    "  b1 = np.zeros((2,1))                  # its ok to have b as zero, it won't get us to the symmetry breaking problem\n",
    "  ```\n",
    "\n",
    "- We need small values because in sigmoid (or tanh), for example, if the weight is too large you are more likely to end up even at the very start of training with very large values of Z. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning. If you don't have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue.\n",
    "\n",
    "- Constant 0.01 is alright for 1 hidden layer networks, but if the NN is deep this number can be changed but it will always be a small number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Written Notes\n",
    "\n",
    "![](HandNotes/c1w2n_notes1.jpg)\n",
    "![](HandNotes/c1w2n_notes2.jpg)\n",
    "![](HandNotes/c1w2n_notes3.jpg)\n",
    "![](HandNotes/c1w2n_notes4.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
