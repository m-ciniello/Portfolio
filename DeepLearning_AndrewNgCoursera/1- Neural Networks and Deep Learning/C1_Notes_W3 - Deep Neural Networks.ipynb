{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1_Notes_W3 - Deep Neural Networks\n",
    "\n",
    "> Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision.\n",
    "\n",
    "## Table of contents\n",
    "  * [1. Deep L-layer neural network](#deep-l-layer-neural-network)\n",
    "  * [2. Forward Propagation in a Deep Network](#forward-propagation-in-a-deep-network)\n",
    "  * [3. Getting your matrix dimensions right](#getting-your-matrix-dimensions-right)\n",
    "  * [4. Why deep representations?](#why-deep-representations)\n",
    "  * [5. Building blocks of deep neural networks](#building-blocks-of-deep-neural-networks)\n",
    "  * [6. Forward and Backward Propagation](#forward-and-backward-propagation)\n",
    "  * [7. Parameters vs Hyperparameters](#parameters-vs-hyperparameters)\n",
    "  * [8. What does this have to do with the brain](#what-does-this-have-to-do-with-the-brain)\n",
    "  * [9. Extra: Ian Goodfellow interview](#extra-ian-goodfellow-interview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deep L-layer neural network\n",
    "\n",
    "- Shallow NN is a NN with one or two layers.\n",
    "- Deep NN is a NN with three or more layers.\n",
    "- We will use the notation `L` to denote the number of layers in a NN.\n",
    "- `n[l]` is the number of neurons in a specific layer `l`.\n",
    "- `n[0]` denotes the number of inputs or featyres, n_x. \n",
    "- `n[L]` denotes the number of neurons in output layer.\n",
    "- `g[l]` is the activation function for layer l\n",
    "- `a[l] = g[l](z[l])`\n",
    "- `w[l]` weights is used for `z[l]`\n",
    "- `x = a[0]`, `a[l] = y'`\n",
    "- These were the notation we will use for deep neural network.\n",
    "- So we have:\n",
    "  - A vector `n` of shape `(1, NoOfLayers+1)`\n",
    "  - A vector `g` of shape `(1, NoOfLayers)`\n",
    "  - A list of different shapes `w` based on the number of neurons on the previous and the current layer.\n",
    "  - A list of different shapes `b` based on the number of neurons on the current layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Forward Propagation in a Deep Network\n",
    "\n",
    "- Forward propagation general rule for one input:\n",
    "\n",
    "  ```\n",
    "  z[l] = W[l]a[l-1] + b[l]\n",
    "  a[l] = g[l](a[l])\n",
    "  ```\n",
    "\n",
    "- Forward propagation general rule for `m` inputs:\n",
    "\n",
    "  ```\n",
    "  Z[l] = W[l]A[l-1] + B[l]\n",
    "  A[l] = g[l](A[l])\n",
    "  ```\n",
    "\n",
    "- We can't compute the whole layers forward propagation without a for loop **so its OK to have a for loop here.**\n",
    "- The dimensions of the matrices are so important you need to figure it out.\n",
    "\n",
    "![](images/c1w2n_basicnn8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Getting your matrix dimensions right\n",
    "\n",
    "- The best way to debug your matrices dimensions is by a pencil and paper.\n",
    "- Dimension of `W` is `(n[l],n[l-1])` . Can be thought by right to left.\n",
    "- Dimension of `b` is `(n[l],1)`\n",
    "- `dw` has the same shape as `W`, while `db` is the same shape as `b`\n",
    "- VECTORIZATION: Dimension of `Z[l],` `A[l]`, `dZ[l]`, and `dA[l]`  is `(n[l],m)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Why deep representations?\n",
    "\n",
    "- Why deep NN works well, we will discuss this question in this section.\n",
    "- Deep NN makes relations with data from simpler to complex. In each layer it tries to make a relation with the previous layer. E.g.:\n",
    "  - 1) Face recognition application:\n",
    "      - Image ==> Edges ==> Face parts ==> Faces ==> desired face\n",
    "  - 2) Audio recognition application:\n",
    "      - Audio ==> Low level sound features like (sss,bb) ==> Phonemes ==> Words ==> Sentences\n",
    "![](images/c1w3n_deeprnn1.png)\n",
    "- Intuitively, you can think of the earlier layers of the neural network learning simpler functions, and the later layers buidling on these simple patterns to learn more complex functions. \n",
    "- Neural Researchers think that deep neural networks \"think\" like brains (simple ==> complex)\n",
    "- Circuit theory and deep learning:\n",
    "  - ![](Images/07.png)\n",
    "  - In the above example, you are simply trying to create an XOR gate for n inputs... so for XOR(10), you want to see if any of n1 to n10 inputs are 1 (as opposed to zero).  \n",
    "  - If you only have a single hidden layer (on the right), you will need 2^n-1 neurons to enumerate all the possible combinations in the XOR calculations... but if you have multiple layers, as we see on the left, you can parse down the tree with with fewer computations. \n",
    "- When starting on an application don't start directly by dozens of hidden layers. Try the simplest solutions (e.g. Logistic Regression), then try the shallow neural network and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Building blocks of deep neural networks\n",
    "\n",
    "- Forward and back propagation for a layer l:\n",
    "  - ![Untitled](Images/10.png)\n",
    "- One iteration of training is as follows:\n",
    "  - ![](Images/08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Forward and Backward Propagation\n",
    "\n",
    "- Pseudo code for forward propagation for layer l:\n",
    "\n",
    "  ```\n",
    "  Input  A[l-1]\n",
    "  Z[l] = W[l]A[l-1] + b[l]\n",
    "  A[l] = g[l](Z[l])\n",
    "  Output A[l], cache(Z[l])\n",
    "  ```\n",
    "\n",
    "- Pseudo  code for back propagation for layer l:\n",
    "\n",
    "  ```\n",
    "  Input da[l], Caches\n",
    "  dZ[l] = dA[l] * g'[l](Z[l])         # Multiplication here (*) is element wise multiplication! \n",
    "  dW[l] = (dZ[l]A[l-1].T) / m\n",
    "  db[l] = sum(dZ[l])/m                # Dont forget axis=1, keepdims=True\n",
    "  dA[l-1] = w[l].T * (dZ[1])          # Multiplication here (*) is dot product!\n",
    "  Output dA[l-1], dW[l], db[l]\n",
    "  ```\n",
    "- If we have used our loss function then:\n",
    "\n",
    "  ```\n",
    "  dA[L] = (-(y/a) + ((1-y)/(1-a)))\n",
    "  ```\n",
    "![](images/c1w3n_deeprnn10.png)  \n",
    "![](images/c1w3n_deeprnn9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Parameters vs Hyperparameters\n",
    "\n",
    "- Main parameters of the NN is `W` and `b`\n",
    "- Hyper parameters (parameters that control the algorithm) are like:\n",
    "  - Learning rate.\n",
    "  - Number of iteration.\n",
    "  - Number of hidden layers `L`.\n",
    "  - Number of hidden units `n`.\n",
    "  - Choice of activation functions.\n",
    "- You have to try values yourself of hyper parameters.\n",
    "- In the earlier days of DL and ML learning rate was often called a parameter, but it really is (and now everybody call it) a hyperparameter.\n",
    "- On the next course we will see how to optimize hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. What does this have to do with the brain\n",
    "\n",
    "- The analogy that \"It is like the brain\" has become really an oversimplified explanation.\n",
    "- There is a very simplistic analogy between a single logistic unit and a single neuron in the brain.\n",
    "- No human today understand how a human brain neuron works.\n",
    "- No human today know exactly how many neurons on the brain.\n",
    "- Deep learning in Andrew's opinion is very good at learning very flexible, complex functions to learn X to Y mappings, to learn input-output mappings (supervised learning).\n",
    "- The field of computer vision has taken a bit more inspiration from the human brains then other disciplines that also apply deep learning.\n",
    "- NN is a small representation of how brain work. The most near model of human brain is in the computer vision (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Extra: Ian Goodfellow interview\n",
    "\n",
    "- Ian is one of the world's most visible deep learning researchers.\n",
    "- Ian is mainly working with generative models. He is the creator of GANs.\n",
    "- We need to stabilize GANs. Stabilized GANs can become the best generative models.\n",
    "- Ian wrote the first textbook on the modern version of deep learning with Yoshua Bengio and Aaron Courville.\n",
    "- Ian worked with [OpenAI.com](OpenAI.com) and Google on ML and NN applications.\n",
    "- Ian tells all who wants to get into AI to get a Ph.D. or post your code on Github and the companies will find you.\n",
    "- Ian thinks that we need to start anticipating security problems with ML now and make sure that these algorithms are secure from the start instead of trying to patch it in retroactively years later.`m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
