{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>   Character level language model - Dinosaurus land </h1>\n",
    "\n",
    "Welcome to Dinosaurus Island! 65 million years ago, dinosaurs existed, and in this assignment they are back. You are in charge of a special task. Leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go beserk, so choose wisely! \n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "Luckily you have learned some deep learning and you will use it to save the day. Your assistant has collected a list of all the dinosaur names they could find, and compiled them into this [dataset](dinos.txt). (Feel free to take a look by clicking the previous link.) To create new dinosaur names, you will build a character level language model to generate new names. Your algorithm will learn the different name patterns, and randomly generate new names. Hopefully this algorithm will keep you and your team safe from the dinosaurs' wrath! \n",
    "\n",
    "By completing this assignment you will learn:\n",
    "\n",
    "- How to store text data for processing using an RNN \n",
    "- How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit\n",
    "- How to build a character-level text generation recurrent neural network\n",
    "- Why clipping the gradients is important\n",
    "\n",
    "We will begin by loading in some functions that we have provided for you in `rnn_utils`. Specifically, you have access to functions such as `rnn_forward` and `rnn_backward` which are equivalent to those you've implemented in the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn_utils import *\n",
    "from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Problem Statement\n",
    "\n",
    "### 1.1 - Dataset and Preprocessing\n",
    "\n",
    "Run the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16961 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "# replace crappy characters (we are not using a bad copy of the data unfortunately...)\n",
    "for char in ['»','¿','ï']:\n",
    "    data = data.replace(char, '')\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characters are a-z (26 characters) plus the \"\\n\" (or newline character), which in this assignment plays a role similar to the `<EOS>` (or \"End of sentence\") token we had discussed in lecture, only here it indicates the end of the dinosaur name rather than the end of a sentence. In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26. We also create a second python dictionary that maps each index back to the corresponding character character. This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer. Below, `char_to_ix` and `ix_to_char` are the python dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of the model\n",
    "\n",
    "Your model will have the following structure: \n",
    "\n",
    "- Initialize parameters \n",
    "- Run the optimization loop\n",
    "    - Forward propagation to compute the loss function\n",
    "    - Backward propagation to compute the gradients with respect to the loss function\n",
    "    - Clip the gradients to avoid exploding gradients\n",
    "    - Using the gradients, update your parameter with the gradient descent update rule.\n",
    "- Return the learned parameters \n",
    "    \n",
    "<img src=\"images/rnn1.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> **Figure 1**: Recurrent Neural Network, similar to what you had built in the previous notebook \"Building a RNN - Step by Step\".  </center></caption>\n",
    "\n",
    "At each time-step, the RNN tries to predict what is the next character given the previous characters. The dataset $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters in the training set, while $Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is such that at every time-step $t$, we have $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building blocks of the model\n",
    "\n",
    "In this part, you will build two important blocks of the overall model:\n",
    "- Gradient clipping: to avoid exploding gradients\n",
    "- Sampling: a technique used to generate characters\n",
    "\n",
    "You will then apply these two functions to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Clipping the gradients in the optimization loop\n",
    "\n",
    "In this section you will implement the `clip` function that you will call inside of your optimization loop. Recall that your overall loop structure usually consists of a forward pass, a cost computation, a backward pass, and a parameter update. Before updating the parameters, you will perform gradient clipping when needed to make sure that your gradients are not \"exploding,\" meaning taking on overly large values. \n",
    "\n",
    "In the exercise below, you will implement a function `clip` that takes in a dictionary of gradients and returns a clipped version of gradients if needed. There are different ways to clip gradients; we will use a simple element-wise clipping procedure, in which every element of the gradient vector is clipped to lie between some range [-N, N]. More generally, you will provide a `maxValue` (say 10). In this example, if any component of the gradient vector is greater than 10, it would be set to 10; and if any component of the gradient vector is less than -10, it would be set to -10. If it is between -10 and 10, it is left alone. \n",
    "\n",
    "<img src=\"images/clip.png\" style=\"width:400;height:150px;\">\n",
    "<caption><center> **Figure 2**: Visualization of gradient descent with and without gradient clipping, in a case where the network is running into slight \"exploding gradient\" problems. </center></caption>\n",
    "\n",
    "**Exercise**: Implement the function below to return the clipped gradients of your dictionary `gradients`. Your function takes in a maximum threshold and returns the clipped versions of your gradients. You can check out this [hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html) for examples of how to clip in numpy. You will need to use the argument `out = ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, -10, -10,   7,  -1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([1,-12,-15,7,-1])\n",
    "np.clip(test, -10,10, out=test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED FUNCTION: clip\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWaa\"][1][2] **\n",
    "    </td>\n",
    "    <td> \n",
    "    10.0\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWax\"][3][1]**\n",
    "    </td>\n",
    "    <td> \n",
    "    -10.0\n",
    "    </td>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWya\"][1][2]**\n",
    "    </td>\n",
    "    <td> \n",
    "0.29713815361\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"db\"][4]**\n",
    "    </td>\n",
    "    <td> \n",
    "[ 10.]\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dby\"][1]**\n",
    "    </td>\n",
    "    <td> \n",
    "[ 8.45833407]\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Sampling\n",
    "\n",
    "Now assume that your model is trained. You would like to generate new text (characters). The process of generation is explained in the picture below:\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 3**: In this picture, we assume the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step, and have the network then sample one character at a time. </center></caption>\n",
    "\n",
    "**Exercise**: Implement the `sample` function below to sample characters. You need to carry out 4 steps:\n",
    "\n",
    "- **Step 1**: Pass the network the first \"dummy\" input $x^{\\langle 1 \\rangle} = \\vec{0}$ (the vector of zeros). This is the default input before we've generated any characters. We also set $a^{\\langle 0 \\rangle} = \\vec{0}$\n",
    "\n",
    "- **Step 2**: Run one step of forward propagation to get $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$. Here are the equations:\n",
    "\n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
    "\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
    "\n",
    "Note that $\\hat{y}^{\\langle t+1 \\rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character.  We have provided a `softmax()` function that you can use.\n",
    "\n",
    "- **Step 3**: Carry out sampling: Pick the next character's index according to the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$. This means that if $\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$, you will pick the index \"i\" with 16% probability. To implement it, you can use [`np.random.choice`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html).\n",
    "\n",
    "Here is an example of how to use `np.random.choice()`:\n",
    "```python\n",
    "np.random.seed(0)\n",
    "p = np.array([0.1, 0.0, 0.7, 0.2])\n",
    "index = np.random.choice([0, 1, 2, 3], p = p.ravel())\n",
    "```\n",
    "This means that you will pick the `index` according to the distribution: \n",
    "$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$.\n",
    "\n",
    "- **Step 4**: The last step to implement in `sample()` is to overwrite the variable `x`, which currently stores $x^{\\langle t \\rangle }$, with the value of $x^{\\langle t + 1 \\rangle }$. You will represent $x^{\\langle t + 1 \\rangle }$ by creating a one-hot vector corresponding to the character you've chosen as your prediction. You will then forward propagate $x^{\\langle t + 1 \\rangle }$ in Step 1 and keep repeating the process until you get a \"\\n\" character, indicating you've reached the end of the dinosaur name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(0)\n",
    "p = np.array([0.1, 0.0, 0.4, 0.5]) #probability distribution\n",
    "index = np.random.choice([0, 1, 2, 3], p = p.ravel()) #choose random number based on p dist above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "# Initialize counter\n",
    "count = Counter(data)\n",
    "# Sort data\n",
    "count_sorted = dict(sorted(count.items(), key=operator.itemgetter(1), reverse=True))\n",
    "# Plot distribution\n",
    "plt.bar(count_sorted.keys(), count_sorted.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC NOTE: To be clear, the y output values (softmax(Wyaa⟨t+1⟩+by)) are a probability distribution for each letter (given a preceeding letter). To generate text, we simply run a letter through the network (starting at t0), and after each y value is calculated (the probability distribution for the given letter), we sample (using np.choice) and  feed the new letter into the next step of the network! \n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=['hello']\n",
    "x is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5,10,1,2,3,4]\n",
    "x[np.argmax(x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sample\n",
    "\n",
    "def sample(parameters, char_to_ix, seed, x=None, random_choice=True, return_probas=False):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters:\n",
    "        - Waa: hidden state weights, shape=(n_hidden_neurons, n_hidden_neurons)\n",
    "        - Wax: x[t] weights, shape=(n_hidden_neurons, n_feats)\n",
    "        - Wya: output (y) weights, shape=(n_output_neurons, n_hidden_neurons)\n",
    "        - ba: hidden state bias vector, shape=(n_hidden_neurons, 1)\n",
    "        - by: output bias vector, shape=(n_output_neurons, 1)\n",
    "        \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    # IN THIS EXAMPLE, N_OUTPUT_NEURONS WILL BE 27 (the vocab size)\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa = parameters['Waa']\n",
    "    Wax = parameters['Wax']\n",
    "    Wya = parameters['Wya']\n",
    "    ba = parameters['ba']\n",
    "    by = parameters['by']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_hidden_neurons = Waa.shape[1]\n",
    "    \n",
    "    # Check to make sure ALL match the vocab_size of 27\n",
    "    assert all([i==vocab_size for i in [Wax.shape[1], Wya.shape[0]]]), \"Vocab size params do not match!\" # MC addition\n",
    "    # Make sure rows and columns number matchs in Waa\n",
    "    assert Waa.shape[0] == Waa.shape[1], \"Waa rows and columns must be equal(n_hidden_neurons)\" # MC addition\n",
    "\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n",
    "    if x is None:\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "    a_prev = np.zeros((n_hidden_neurons, 1))\n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    # RUN x THROUGH THE RNN\n",
    "    probas = []\n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        probas.append(y)\n",
    "        \n",
    "        # for grading purposes\n",
    "        np.random.seed(counter + seed) \n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        if random_choice:\n",
    "            idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "        else:\n",
    "            idx = np.argmax(y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "#     print(\"vocab_size: \",vocab_size)\n",
    "#     print(\"X.shape: \",x.shape)\n",
    "#     print(\"Wax.shape: \",Wax.shape)\n",
    "#     print(\"a_prev.shape: \",a_prev.shape)\n",
    "#     print(\"Waa.shape: \",Waa.shape)\n",
    "#     print(\"y_output.shape: \",y.shape)\n",
    "\n",
    "    if return_probas:\n",
    "        return indices, probas\n",
    "    else:\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling:\n",
      "list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 12, 13, 0, 0]\n",
      "list of sampled characters: ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'l', 'm', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# REMEMBER: n_feats = vocab size in nlp exercises (because you have binary vectors)\n",
    "# Also vocab_size is the same as n_output because we are langauge modeling, and want a word vecotr for each output!\n",
    "np.random.seed(2)\n",
    "n_hidden_neurons = 100\n",
    "vocab_size = 27\n",
    "\n",
    "n_feats = vocab_size\n",
    "n_output_neurons = vocab_size\n",
    "\n",
    "# X.shape = (vocab_size, n_obs)\n",
    "Wax = np.random.randn(n_hidden_neurons, n_feats)\n",
    "# a.shape = (n_hidden_neurons, n_obs)\n",
    "Waa = np.random.randn(n_hidden_neurons, n_hidden_neurons)\n",
    "# y.shape = (n_output_neurons, n_obs)\n",
    "Wya = np.random.randn(n_output_neurons, n_hidden_neurons)\n",
    "ba = np.random.randn(n_hidden_neurons, 1)\n",
    "by = np.random.randn(n_feats, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"\\nSampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "<table>\n",
    "<tr>\n",
    "    <td> \n",
    "    **list of sampled indices:**\n",
    "    </td>\n",
    "    <td> \n",
    "    [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, <br>\n",
    "    7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 5, 6, 12, 25, 0, 0]\n",
    "    </td>\n",
    "    </tr><tr>\n",
    "    <td> \n",
    "    **list of sampled characters:**\n",
    "    </td>\n",
    "    <td> \n",
    "    ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', <br>\n",
    "    'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', <br>\n",
    "    'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'e', 'f', 'l', 'y', '\\n', '\\n']\n",
    "    </td>\n",
    "    \n",
    "        \n",
    "    \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building the language model \n",
    "\n",
    "It is time to build the character-level language model for text generation. \n",
    "\n",
    "\n",
    "### 3.1 - Gradient descent \n",
    "\n",
    "In this section you will implement a function performing one step of stochastic gradient descent (with clipped gradients). You will go through the training examples one at a time, so the optimization algorithm will be stochastic gradient descent. As a reminder, here are the steps of a common optimization loop for an RNN:\n",
    "\n",
    "- Forward propagate through the RNN to compute the loss\n",
    "- Backward propagate through time to compute the gradients of the loss with respect to the parameters\n",
    "- Clip the gradients if necessary \n",
    "- Update your parameters using gradient descent \n",
    "\n",
    "**Exercise**: Implement this optimization process (one step of stochastic gradient descent). \n",
    "\n",
    "We provide you with the following functions: \n",
    "\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\" Performs the forward propagation through the RNN and computes the cross-entropy loss.\n",
    "    It returns the loss' value as well as a \"cache\" storing values to be used in the backpropagation.\"\"\"\n",
    "    ....\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n",
    "    to the parameters. It returns also all the hidden states.\"\"\"\n",
    "    ...\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will rewrite the function below, just for illustration purposes!\n",
    "- Softmax: activation function\n",
    "- rnn_step_forward: feed data through RNN for a single timestep\n",
    "- rnn_forward: run rnn_step_forward for each timestep in X\n",
    "- rnn_step_backward: feed loss value and params back through network to extract gradients for a single timestep\n",
    "- rnn_backward: run rnn_step_backward for each timestep in X\n",
    "- optimize: put it all together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wax (5, 2)\n",
      "Waa (5, 5)\n",
      "Wya (2, 5)\n",
      "ba (5, 1)\n",
      "by (2, 1)\n"
     ]
    }
   ],
   "source": [
    "n_feats = 2\n",
    "n_obs = 1\n",
    "n_hidden_neurons = 5\n",
    "n_output_neurons = 2\n",
    "\n",
    "x_t = np.zeros((n_feats, n_obs))\n",
    "Wax = np.random.randn(n_hidden_neurons, n_feats)\n",
    "a_prev = np.random.randn(n_hidden_neurons, n_obs)\n",
    "Waa = np.random.randn(n_hidden_neurons, n_hidden_neurons)\n",
    "ba = np.random.randn(n_hidden_neurons, 1)\n",
    "\n",
    "Wya = np.random.randn(n_output_neurons, n_hidden_neurons)\n",
    "by = np.random.randn(n_feats, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "for key,val in parameters.items():\n",
    "    print(key, val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "softmax([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rnn_step_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last a: [[-0.5377121 ]\n",
      " [-0.9591247 ]\n",
      " [-0.45195964]\n",
      " [-0.84850277]\n",
      " [-0.60551724]]\n",
      "last y: [[0.99225591]\n",
      " [0.00774409]]\n"
     ]
    }
   ],
   "source": [
    "def rnn_step_forward(parameters, a_prev, x):    \n",
    "    # Extract params\n",
    "    Waa = parameters['Waa']\n",
    "    Wax = parameters['Wax']\n",
    "    Wya = parameters['Wya']\n",
    "    by = parameters['by'] \n",
    "    ba = parameters['ba']\n",
    "    # calculate hidden state\n",
    "    Wax_x = np.dot(Wax, x)\n",
    "    Waa_aprev = np.dot(Waa, a_prev)\n",
    "    a_next = np.tanh(Wax_x + Waa_aprev + ba)\n",
    "    # calculate y value for timestep (unnormalized log probabilities for next chars, probabilities for next chars)\n",
    "    y_t = softmax(np.dot(Wya, a_next) + by)  \n",
    "    return a_next, y_t\n",
    "\n",
    "a, y = rnn_step_forward(parameters, a_prev, x_t)\n",
    "print(\"last a:\", a)\n",
    "print(\"last y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rnn_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    \n",
    "    # initialize your loss to 0\n",
    "    loss = 0\n",
    " \n",
    "    # Iterate through each word idx, create one-hot vector representation, and store in dictionary X\n",
    "    for t in range(len(X)):\n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        x[t][X[t]] = 1\n",
    "        \n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rnn_step_backward & update_prameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    # Update gradients for a single timestep\n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['ba']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rnn_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "    \n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    \n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(ba), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Backpropagate through time\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 126.50397572165346\n",
      "gradients[\"dWaa\"][1][2] = 0.1947093153472697\n",
      "np.argmax(gradients[\"dWax\"]) = 93\n",
      "gradients[\"dWya\"][1][2] = -0.007773876032004693\n",
      "gradients[\"db\"][4] = [-0.06809825]\n",
      "gradients[\"dby\"][1] = [0.01538192]\n",
      "a_last[4] = [-1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size = 27\n",
    "n_hidden_neurons = 100\n",
    "\n",
    "n_feats = vocab_size\n",
    "n_output_neurons = vocab_size\n",
    "\n",
    "a_prev = np.random.randn(n_hidden_neurons, 1)\n",
    "Wax = np.random.randn(n_hidden_neurons, n_feats)\n",
    "Waa = np.random.randn(n_hidden_neurons, n_hidden_neurons)\n",
    "Wya = np.random.randn(n_output_neurons, n_hidden_neurons)\n",
    "ba = np.random.randn(n_hidden_neurons, 1)\n",
    "by = np.random.randn(n_output_neurons, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "X = [12,3,5,11, 22, 3]\n",
    "Y = [4,14,11,22,25, 26]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "\n",
    "<table>\n",
    "\n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **Loss **\n",
    "    </td>\n",
    "    <td> \n",
    "    126.503975722\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWaa\"][1][2]**\n",
    "    </td>\n",
    "    <td> \n",
    "    0.194709315347\n",
    "    </td>\n",
    "<tr>\n",
    "    <td> \n",
    "    **np.argmax(gradients[\"dWax\"])**\n",
    "    </td>\n",
    "    <td> 93\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWya\"][1][2]**\n",
    "    </td>\n",
    "    <td> -0.007773876032\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"db\"][4]**\n",
    "    </td>\n",
    "    <td> [-0.06809825]\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dby\"][1]**\n",
    "    </td>\n",
    "    <td>[ 0.01538192]\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **a_last[4]**\n",
    "    </td>\n",
    "    <td> [-1.]\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dataset of dinosaur names, we use each line of the dataset (one name) as one training example. Every 100 steps of stochastic gradient descent, you will sample 10 randomly chosen names to see how the algorithm is doing. Remember to shuffle the dataset, so that stochastic gradient descent visits the examples in random order. \n",
    "\n",
    "**Exercise**: Follow the instructions and implement `model()`. When `examples[index]` contains one dinosaur name (string), to create an example (X, Y), you can use this:\n",
    "```python\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "```\n",
    "Note that we use: `index= j % len(examples)`, where `j = 1....num_iterations`, to make sure that `examples[index]` is always a valid statement (`index` is smaller than `len(examples)`).\n",
    "The first entry of `X` being `None` will be interpreted by `rnn_forward()` as setting $x^{\\langle 0 \\rangle} = \\vec{0}$. Further, this ensures that `Y` is equal to `X` but shifted one step to the left, and with an additional \"\\n\" appended to signify the end of the dinosaur name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "    \n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "def initialize_parameters(n_hidden_neurons, n_feats, n_output_neurons):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_hidden_neurons, n_feats)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(n_hidden_neurons, n_hidden_neurons)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(n_output_neurons, n_hidden_neurons)*0.01 # hidden to output\n",
    "    ba = np.zeros((n_hidden_neurons, 1)) # hidden bias\n",
    "    by = np.zeros((n_output_neurons, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba,\"by\": by}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9%4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(examples, ix_to_char, char_to_ix, num_iterations = 35000, n_hidden_neurons = 50, dino_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    examples -- should be a list of dino names!\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_feats, n_output_neurons = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_hidden_neurons, n_feats, n_output_neurons)\n",
    "    for key, val in parameters.items():\n",
    "        print(key, val.shape)\n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # NOTE: because we have shitty custom data, we will just manually add it to function args as 'examples'\n",
    "    # Build list of all dinosaur names (training examples).\n",
    "#     with open(\"dinos.txt\") as f:\n",
    "#         examples = f.readlines()\n",
    "#     examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your RNN\n",
    "    a_prev = np.zeros((n_hidden_neurons, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, your model should learn to generate reasonable-looking names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wax (50, 27)\n",
      "Waa (50, 50)\n",
      "Wya (27, 50)\n",
      "ba (50, 1)\n",
      "by (27, 1)\n",
      "Iteration: 0, Loss: 23.090633\n",
      "\n",
      "----\n",
      " nkzxwtdmfqoeyhsqwasjkjvu\n",
      " \n",
      "----\n",
      "----\n",
      " kneb\n",
      " \n",
      "----\n",
      "----\n",
      " kzxwtdmfqoeyhsqwasjkjvu\n",
      " \n",
      "----\n",
      "----\n",
      " neb\n",
      " \n",
      "----\n",
      "----\n",
      " zxwtdmfqoeyhsqwasjkjvu\n",
      " \n",
      "----\n",
      "----\n",
      " eb\n",
      " \n",
      "----\n",
      "----\n",
      " xwtdmfqoeyhsqwasjkjvu\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 26.648994\n",
      "\n",
      "----\n",
      " orusaurus\n",
      " \n",
      "----\n",
      "----\n",
      " ipa\n",
      " \n",
      "----\n",
      "----\n",
      " kwusaurus\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " xus\n",
      " \n",
      "----\n",
      "----\n",
      " da\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 24.943986\n",
      "\n",
      "----\n",
      " oryus\n",
      " \n",
      "----\n",
      "----\n",
      " lon\n",
      " \n",
      "----\n",
      "----\n",
      " lytos\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " xus\n",
      " \n",
      "----\n",
      "----\n",
      " ed\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 23.567811\n",
      "\n",
      "----\n",
      " oshurus\n",
      " \n",
      "----\n",
      "----\n",
      " mon\n",
      " \n",
      "----\n",
      "----\n",
      " nytos\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " vus\n",
      " \n",
      "----\n",
      "----\n",
      " eda\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 23.151049\n",
      "\n",
      "----\n",
      " posaurus\n",
      " \n",
      "----\n",
      "----\n",
      " nnde\n",
      " \n",
      "----\n",
      "----\n",
      " osaurus\n",
      " \n",
      "----\n",
      "----\n",
      " pha\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ia\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 22.473597\n",
      "\n",
      "----\n",
      " phus\n",
      " \n",
      "----\n",
      "----\n",
      " ora\n",
      " \n",
      "----\n",
      "----\n",
      " osaurus\n",
      " \n",
      "----\n",
      "----\n",
      " pha\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ha\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 22.313896\n",
      "\n",
      "----\n",
      " osousus\n",
      " \n",
      "----\n",
      "----\n",
      " nka\n",
      " \n",
      "----\n",
      "----\n",
      " osaurus\n",
      " \n",
      "----\n",
      "----\n",
      " opa\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ha\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 21.925845\n",
      "\n",
      "----\n",
      " onus\n",
      " \n",
      "----\n",
      "----\n",
      " non\n",
      " \n",
      "----\n",
      "----\n",
      " osaurus\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " g\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 21.951665\n",
      "\n",
      "----\n",
      " ospusus\n",
      " \n",
      "----\n",
      "----\n",
      " lon\n",
      " \n",
      "----\n",
      "----\n",
      " lus\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ei\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 21.717075\n",
      "\n",
      "----\n",
      " phusaurus\n",
      " \n",
      "----\n",
      "----\n",
      " nosasaurus\n",
      " \n",
      "----\n",
      "----\n",
      " nyx\n",
      " \n",
      "----\n",
      "----\n",
      " pa\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " elamos\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 21.868586\n",
      "\n",
      "----\n",
      " optosaurus\n",
      " \n",
      "----\n",
      "----\n",
      " in\n",
      " \n",
      "----\n",
      "----\n",
      " lus\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " eh\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 21.635270\n",
      "\n",
      "----\n",
      " phyus\n",
      " \n",
      "----\n",
      "----\n",
      " osaurus\n",
      " \n",
      "----\n",
      "----\n",
      " ostosaurus\n",
      " \n",
      "----\n",
      "----\n",
      " peg\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ia\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 21.721447\n",
      "\n",
      "----\n",
      " osusaurus\n",
      " \n",
      "----\n",
      "----\n",
      " in\n",
      " \n",
      "----\n",
      "----\n",
      " lyxisaurus\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ei\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 21.439392\n",
      "\n",
      "----\n",
      " phus\n",
      " \n",
      "----\n",
      "----\n",
      " osaurus\n",
      " \n",
      "----\n",
      "----\n",
      " ostos\n",
      " \n",
      "----\n",
      "----\n",
      " qon\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ia\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 21.601360\n",
      "\n",
      "----\n",
      " ostus\n",
      " \n",
      "----\n",
      "----\n",
      " lon\n",
      " \n",
      "----\n",
      "----\n",
      " lyx\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ia\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 21.390378\n",
      "\n",
      "----\n",
      " ostrus\n",
      " \n",
      "----\n",
      "----\n",
      " jia\n",
      " \n",
      "----\n",
      "----\n",
      " lytosaurus\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ia\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 21.517885\n",
      "\n",
      "----\n",
      " ostosaurus\n",
      " \n",
      "----\n",
      "----\n",
      " lon\n",
      " \n",
      "----\n",
      "----\n",
      " lyx\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ia\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 21.231962\n",
      "\n",
      "----\n",
      " ostus\n",
      " \n",
      "----\n",
      "----\n",
      " in\n",
      " \n",
      "----\n",
      "----\n",
      " isaurus\n",
      " \n",
      "----\n",
      "----\n",
      " on\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "----\n",
      " ia\n",
      " \n",
      "----\n",
      "----\n",
      " us\n",
      " \n",
      "----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = data.split(\"\\n\")\n",
    "parameters = model(examples, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets break this down step by step (or TIMEstep by TIMEstep... a lil RNN pun for ya)\n",
    "\n",
    "When run the sample funciton with a vector of zeros as x, we effectively sample the probability of getting any first letter... you'll notice that if you do the math with the zero vector, you are left with only by (everything else gets zeroed out!). **NOTE that the probability distributions created for the first letter DO NOT equal [num_first_letters / total examples]... this is because the `ba` and `Wya` which create the first probability distribution (assuming a x vector of zeros) reflect the probability of getting a letter across ALL timesteps... I think...**\n",
    "\n",
    "```python\n",
    "# NOTE X WILL BE A VECTOR OF ZEROES FOR FIRST SAMPLE\n",
    "def rnn_step_forward(parameters, a_prev, x):    \n",
    "    # Extract trianed params\n",
    "    Waa = parameters['Waa']\n",
    "    Wax = parameters['Wax']\n",
    "    Wya = parameters['Wya']\n",
    "    by = parameters['by'] \n",
    "    ba = parameters['ba']\n",
    "    # calculate hidden state left side: SHOULD RESULT IN VECTOR OF ZEROS (because of x)\n",
    "    Wax_x = np.dot(Wax, x)\n",
    "    # calucate hidden state right side: SHOULD RESULT IN VECTOR OF ZEROS (because of initil a_prev)\n",
    "    Waa_aprev = np.dot(Waa, a_prev)\n",
    "    # calculate a_next: SHOULD REUSLT IN np.tanh('ba') because only 'ba' has non-zero values\n",
    "    a_next = np.tanh(Wax_x + Waa_aprev + ba)\n",
    "    # calculate y value for timestep (unnormalized log probabilities for next chars, probabilities for next chars)\n",
    "    y_t = softmax(np.dot(Wya, a_next) + by)  \n",
    "    return a_next, y_t\n",
    "```\n",
    "So the probability of any given first letter should equal:\n",
    "```python\n",
    "np.dot(Wya, np.tanh(ba)) + by\n",
    "```\n",
    "**We will try this with the parameters we trained above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_t shape: (27, 1)\n",
      "a_next shape: (50, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>first_y</th>\n",
       "      <th>first_y_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>u</td>\n",
       "      <td>[0.2317703881129463]</td>\n",
       "      <td>[0.2317703881129463]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i</td>\n",
       "      <td>[0.18594274101664704]</td>\n",
       "      <td>[0.18594274101664704]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>[0.16632124510202712]</td>\n",
       "      <td>[0.16632124510202712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>o</td>\n",
       "      <td>[0.14133007805189768]</td>\n",
       "      <td>[0.14133007805189768]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>e</td>\n",
       "      <td>[0.06260820046803531]</td>\n",
       "      <td>[0.06260820046803531]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>s</td>\n",
       "      <td>[0.049943376618633774]</td>\n",
       "      <td>[0.049943376618633774]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>h</td>\n",
       "      <td>[0.03542348337951588]</td>\n",
       "      <td>[0.03542348337951588]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>l</td>\n",
       "      <td>[0.03425879932578105]</td>\n",
       "      <td>[0.03425879932578105]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>r</td>\n",
       "      <td>[0.0234505766924641]</td>\n",
       "      <td>[0.0234505766924641]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>y</td>\n",
       "      <td>[0.012723458040856703]</td>\n",
       "      <td>[0.012723458040856703]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>n</td>\n",
       "      <td>[0.012430900412200444]</td>\n",
       "      <td>[0.012430900412200444]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>p</td>\n",
       "      <td>[0.008436175098268031]</td>\n",
       "      <td>[0.008436175098268031]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c</td>\n",
       "      <td>[0.007627159881924112]</td>\n",
       "      <td>[0.007627159881924112]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>t</td>\n",
       "      <td>[0.005743845914028172]</td>\n",
       "      <td>[0.005743845914028172]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n</td>\n",
       "      <td>[0.0056881961070391635]</td>\n",
       "      <td>[0.0056881961070391635]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>m</td>\n",
       "      <td>[0.004428513103593867]</td>\n",
       "      <td>[0.004428513103593867]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d</td>\n",
       "      <td>[0.0027575633558780026]</td>\n",
       "      <td>[0.0027575633558780026]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b</td>\n",
       "      <td>[0.0022115111131887273]</td>\n",
       "      <td>[0.0022115111131887273]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>g</td>\n",
       "      <td>[0.002101317898859255]</td>\n",
       "      <td>[0.002101317898859255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>k</td>\n",
       "      <td>[0.001970083805007724]</td>\n",
       "      <td>[0.001970083805007724]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>v</td>\n",
       "      <td>[0.0007616966895654066]</td>\n",
       "      <td>[0.0007616966895654066]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>x</td>\n",
       "      <td>[0.0006349815056020446]</td>\n",
       "      <td>[0.0006349815056020446]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>w</td>\n",
       "      <td>[0.000443185227217731]</td>\n",
       "      <td>[0.000443185227217731]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>q</td>\n",
       "      <td>[0.0003534454906792741]</td>\n",
       "      <td>[0.0003534454906792741]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>z</td>\n",
       "      <td>[0.0003104174299635377]</td>\n",
       "      <td>[0.0003104174299635377]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f</td>\n",
       "      <td>[0.00018335508941965292]</td>\n",
       "      <td>[0.00018335508941965292]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>j</td>\n",
       "      <td>[0.00014530506875991023]</td>\n",
       "      <td>[0.00014530506875991023]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chars                   first_y            first_y_manual\n",
       "21     u      [0.2317703881129463]      [0.2317703881129463]\n",
       "9      i     [0.18594274101664704]     [0.18594274101664704]\n",
       "1      a     [0.16632124510202712]     [0.16632124510202712]\n",
       "15     o     [0.14133007805189768]     [0.14133007805189768]\n",
       "5      e     [0.06260820046803531]     [0.06260820046803531]\n",
       "19     s    [0.049943376618633774]    [0.049943376618633774]\n",
       "8      h     [0.03542348337951588]     [0.03542348337951588]\n",
       "12     l     [0.03425879932578105]     [0.03425879932578105]\n",
       "18     r      [0.0234505766924641]      [0.0234505766924641]\n",
       "25     y    [0.012723458040856703]    [0.012723458040856703]\n",
       "14     n    [0.012430900412200444]    [0.012430900412200444]\n",
       "16     p    [0.008436175098268031]    [0.008436175098268031]\n",
       "3      c    [0.007627159881924112]    [0.007627159881924112]\n",
       "20     t    [0.005743845914028172]    [0.005743845914028172]\n",
       "0     \\n   [0.0056881961070391635]   [0.0056881961070391635]\n",
       "13     m    [0.004428513103593867]    [0.004428513103593867]\n",
       "4      d   [0.0027575633558780026]   [0.0027575633558780026]\n",
       "2      b   [0.0022115111131887273]   [0.0022115111131887273]\n",
       "7      g    [0.002101317898859255]    [0.002101317898859255]\n",
       "11     k    [0.001970083805007724]    [0.001970083805007724]\n",
       "22     v   [0.0007616966895654066]   [0.0007616966895654066]\n",
       "24     x   [0.0006349815056020446]   [0.0006349815056020446]\n",
       "23     w    [0.000443185227217731]    [0.000443185227217731]\n",
       "17     q   [0.0003534454906792741]   [0.0003534454906792741]\n",
       "26     z   [0.0003104174299635377]   [0.0003104174299635377]\n",
       "6      f  [0.00018335508941965292]  [0.00018335508941965292]\n",
       "10     j  [0.00014530506875991023]  [0.00014530506875991023]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define input 'a'\n",
    "x = np.zeros((n_feats, n_obs))\n",
    "#x[char_to_ix['a']] = 1\n",
    "n_hidden_neurons = 50\n",
    "n_feats = 27\n",
    "n_obs = 1 # n_obs is always 1 because we train one letter at a time\n",
    "a_prev = np.zeros((n_hidden_neurons, n_obs))\n",
    "\n",
    "# Run input 'a' through model and extract weights\n",
    "a_next, y_t = rnn_step_forward(parameters, a_prev, x)\n",
    "\n",
    "print(\"y_t shape:\", y_t.shape)\n",
    "print(\"a_next shape:\", a_next.shape)\n",
    "\n",
    "#pd.DataFrame({\"chars\":list(ix_to_char.values()),\"p|char\":y_t.ravel()*100}).sort_values('p|char', ascending=False)[0:5]\n",
    "test = softmax(np.dot(parameters['Wya'], np.tanh(parameters['ba']))+parameters['by'])\n",
    "pd.DataFrame({\"chars\":list(ix_to_char.values()),\n",
    "              'first_y':list(y_t),\n",
    "              'first_y_manual':list(test)},\n",
    "             index=range(0,27)).sort_values('first_y', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have selected a letter using a `np.random.choice(p=y_t)` based off y_t probability distribution (which will always remain the same for the first y_t as long as paramenters remain the same), then we take that sample, and the hidden state output (a_next), and feed it into the next timestep of the RNN. This process keeps repeating until we get either 50 characters or we hit a newline character!\n",
    "\n",
    "##### NOTE: If we dont use np.random.choice, we will ALWAYS get the same string of characters, as the paramater weights are not changing. See below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us\n",
      "\n",
      "us\n",
      "\n",
      "us\n",
      "\n",
      "us\n",
      "\n",
      "us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    idxs = sample(parameters, char_to_ix, seed=np.random.randint(1,1000), x=None, random_choice=False)\n",
    "    print(''.join([ix_to_char[i] for i in idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on\n",
      "\n",
      "odan\n",
      "\n",
      "a\n",
      "\n",
      "ia\n",
      "\n",
      "nychanus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    idxs = sample(parameters, char_to_ix, seed=np.random.randint(1,1000), x=None, random_choice=True)\n",
    "    print(''.join([ix_to_char[i] for i in idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out the probabilities distributions for each timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(27, 3)\n"
     ]
    }
   ],
   "source": [
    "# Use return_probas arg to access y (probability distribution values) for each timestep\n",
    "idxs, probas = sample(parameters, char_to_ix, seed=np.random.randint(1,1000), x=None, random_choice=True, return_probas=True)\n",
    "probas_matrix = np.hstack(probas)\n",
    "print(len(probas))\n",
    "print(probas_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 's', '\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ix_to_char[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b582016e_d969_11e8_a5df_185e0f132080row0_col2 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_b582016e_d969_11e8_a5df_185e0f132080row19_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_b582016e_d969_11e8_a5df_185e0f132080row21_col0 {\n",
       "            background-color:  yellow;\n",
       "        }</style>  \n",
       "<table id=\"T_b582016e_d969_11e8_a5df_185e0f132080\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >0</th> \n",
       "        <th class=\"col_heading level0 col1\" >1</th> \n",
       "        <th class=\"col_heading level0 col2\" >2</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row0\" class=\"row_heading level0 row0\" >\n",
       "</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row0_col0\" class=\"data row0 col0\" >0.0056882</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row0_col1\" class=\"data row0 col1\" >0.00378488</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row0_col2\" class=\"data row0 col2\" >0.838969</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row1\" class=\"row_heading level0 row1\" >a</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row1_col0\" class=\"data row1 col0\" >0.166321</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row1_col1\" class=\"data row1 col1\" >0.00415298</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row1_col2\" class=\"data row1 col2\" >0.139567</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row2\" class=\"row_heading level0 row2\" >b</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row2_col0\" class=\"data row2 col0\" >0.00221151</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row2_col1\" class=\"data row2 col1\" >0.00193776</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row2_col2\" class=\"data row2 col2\" >2.90913e-05</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row3\" class=\"row_heading level0 row3\" >c</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row3_col0\" class=\"data row3 col0\" >0.00762716</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row3_col1\" class=\"data row3 col1\" >0.00420472</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row3_col2\" class=\"data row3 col2\" >0.000584953</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row4\" class=\"row_heading level0 row4\" >d</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row4_col0\" class=\"data row4 col0\" >0.00275756</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row4_col1\" class=\"data row4 col1\" >0.0131657</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row4_col2\" class=\"data row4 col2\" >6.43881e-05</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row5\" class=\"row_heading level0 row5\" >e</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row5_col0\" class=\"data row5 col0\" >0.0626082</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row5_col1\" class=\"data row5 col1\" >0.00724906</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row5_col2\" class=\"data row5 col2\" >0.000749257</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row6\" class=\"row_heading level0 row6\" >f</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row6_col0\" class=\"data row6 col0\" >0.000183355</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row6_col1\" class=\"data row6 col1\" >0.000201253</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row6_col2\" class=\"data row6 col2\" >4.53154e-06</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row7\" class=\"row_heading level0 row7\" >g</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row7_col0\" class=\"data row7 col0\" >0.00210132</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row7_col1\" class=\"data row7 col1\" >0.00455349</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row7_col2\" class=\"data row7 col2\" >1.05742e-05</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row8\" class=\"row_heading level0 row8\" >h</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row8_col0\" class=\"data row8 col0\" >0.0354235</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row8_col1\" class=\"data row8 col1\" >0.00104666</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row8_col2\" class=\"data row8 col2\" >0.000924852</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row9\" class=\"row_heading level0 row9\" >i</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row9_col0\" class=\"data row9 col0\" >0.185943</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row9_col1\" class=\"data row9 col1\" >0.00150103</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row9_col2\" class=\"data row9 col2\" >0.00197814</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row10\" class=\"row_heading level0 row10\" >j</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row10_col0\" class=\"data row10 col0\" >0.000145305</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row10_col1\" class=\"data row10 col1\" >1.66345e-05</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row10_col2\" class=\"data row10 col2\" >1.0224e-05</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row11\" class=\"row_heading level0 row11\" >k</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row11_col0\" class=\"data row11 col0\" >0.00197008</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row11_col1\" class=\"data row11 col1\" >0.000693022</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row11_col2\" class=\"data row11 col2\" >0.000122431</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row12\" class=\"row_heading level0 row12\" >l</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row12_col0\" class=\"data row12 col0\" >0.0342588</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row12_col1\" class=\"data row12 col1\" >0.00478718</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row12_col2\" class=\"data row12 col2\" >3.63989e-05</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row13\" class=\"row_heading level0 row13\" >m</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row13_col0\" class=\"data row13 col0\" >0.00442851</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row13_col1\" class=\"data row13 col1\" >0.00311197</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row13_col2\" class=\"data row13 col2\" >0.000727306</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row14\" class=\"row_heading level0 row14\" >n</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row14_col0\" class=\"data row14 col0\" >0.0124309</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row14_col1\" class=\"data row14 col1\" >0.0893732</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row14_col2\" class=\"data row14 col2\" >9.25904e-05</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row15\" class=\"row_heading level0 row15\" >o</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row15_col0\" class=\"data row15 col0\" >0.14133</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row15_col1\" class=\"data row15 col1\" >0.000774213</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row15_col2\" class=\"data row15 col2\" >0.00301571</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row16\" class=\"row_heading level0 row16\" >p</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row16_col0\" class=\"data row16 col0\" >0.00843618</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row16_col1\" class=\"data row16 col1\" >0.0024673</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row16_col2\" class=\"data row16 col2\" >0.000469667</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row17\" class=\"row_heading level0 row17\" >q</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row17_col0\" class=\"data row17 col0\" >0.000353445</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row17_col1\" class=\"data row17 col1\" >0.000852877</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row17_col2\" class=\"data row17 col2\" >5.62521e-05</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row18\" class=\"row_heading level0 row18\" >r</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row18_col0\" class=\"data row18 col0\" >0.0234506</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row18_col1\" class=\"data row18 col1\" >0.0311372</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row18_col2\" class=\"data row18 col2\" >1.49842e-06</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row19\" class=\"row_heading level0 row19\" >s</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row19_col0\" class=\"data row19 col0\" >0.0499434</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row19_col1\" class=\"data row19 col1\" >0.811464</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row19_col2\" class=\"data row19 col2\" >0.00151568</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row20\" class=\"row_heading level0 row20\" >t</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row20_col0\" class=\"data row20 col0\" >0.00574385</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row20_col1\" class=\"data row20 col1\" >0.00360275</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row20_col2\" class=\"data row20 col2\" >0.00486212</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row21\" class=\"row_heading level0 row21\" >u</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row21_col0\" class=\"data row21 col0\" >0.23177</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row21_col1\" class=\"data row21 col1\" >0.000119767</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row21_col2\" class=\"data row21 col2\" >0.00516643</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row22\" class=\"row_heading level0 row22\" >v</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row22_col0\" class=\"data row22 col0\" >0.000761697</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row22_col1\" class=\"data row22 col1\" >0.00145855</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row22_col2\" class=\"data row22 col2\" >3.77923e-06</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row23\" class=\"row_heading level0 row23\" >w</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row23_col0\" class=\"data row23 col0\" >0.000443185</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row23_col1\" class=\"data row23 col1\" >0.000623309</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row23_col2\" class=\"data row23 col2\" >3.51041e-06</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row24\" class=\"row_heading level0 row24\" >x</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row24_col0\" class=\"data row24 col0\" >0.000634982</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row24_col1\" class=\"data row24 col1\" >0.00548222</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row24_col2\" class=\"data row24 col2\" >1.96297e-05</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row25\" class=\"row_heading level0 row25\" >y</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row25_col0\" class=\"data row25 col0\" >0.0127235</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row25_col1\" class=\"data row25 col1\" >0.00212489</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row25_col2\" class=\"data row25 col2\" >0.0010011</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_b582016e_d969_11e8_a5df_185e0f132080level0_row26\" class=\"row_heading level0 row26\" >z</th> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row26_col0\" class=\"data row26 col0\" >0.000310417</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row26_col1\" class=\"data row26 col1\" >0.000113756</td> \n",
       "        <td id=\"T_b582016e_d969_11e8_a5df_185e0f132080row26_col2\" class=\"data row26 col2\" >1.3743e-05</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f87d1b7b70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn into dataframe\n",
    "index = pd.DataFrame(probas_matrix, index=list(ix_to_char.values()))\n",
    "index.style.highlight_max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You can see that your algorithm has started to generate plausible dinosaur names towards the end of the training. At first, it was generating random characters, but towards the end you could see dinosaur names with cool endings. Feel free to run the algorithm even longer and play with hyperparameters to see if you can get even better results. Our implemetation generated some really cool names like `maconucon`, `marloralus` and `macingsersaurus`. Your model hopefully also learned that dinosaur names tend to end in `saurus`, `don`, `aura`, `tor`, etc.\n",
    "\n",
    "If your model generates some non-cool names, don't blame the model entirely--not all actual dinosaur names sound cool. (For example, `dromaeosauroides` is an actual dinosaur name and is in the training set.) But this model should give you a set of candidates from which you can pick the coolest! \n",
    "\n",
    "This assignment had used a relatively small dataset, so that you could train an RNN quickly on a CPU. Training a model of the english language requires a much bigger dataset, and usually needs much more computation, and could run for many hours on GPUs. We ran our dinosaur name for quite some time, and so far our favoriate name is the great, undefeatable, and fierce: Mangosaurus!\n",
    "\n",
    "<img src=\"images/mangosaurus.jpeg\" style=\"width:250;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Writing like Nietzsche!\n",
    "\n",
    "Ok, now we are going to create a language model using keras! This time we will use the Nietzsche corpus. This was taken from the keras website: \n",
    "https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "##### NOTE: That we are going to build  a MANY TO ONE model. So we feed in a sequence, and the next word is predicted. Then we feed that same sequence in with the next predicted word to get the next word, and so on and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, RNN\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Get data\n",
    "Should have a character length of ~600k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'preface   supposing that truth is a woman--what then? is there not ground for suspecting that all ph'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "import io\n",
    "\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "# Replace '\\n' chars with spaces\n",
    "text = text.replace(\"\\n\",\" \")\n",
    "print('corpus length:', len(text))\n",
    "text[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Get unique chars and indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 56\n",
      "{0: ' ', 1: '!', 2: '\"', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '0', 10: '1', 11: '2', 12: '3', 13: '4', 14: '5', 15: '6', 16: '7', 17: '8', 18: '9', 19: ':', 20: ';', 21: '=', 22: '?', 23: '[', 24: ']', 25: '_', 26: 'a', 27: 'b', 28: 'c', 29: 'd', 30: 'e', 31: 'f', 32: 'g', 33: 'h', 34: 'i', 35: 'j', 36: 'k', 37: 'l', 38: 'm', 39: 'n', 40: 'o', 41: 'p', 42: 'q', 43: 'r', 44: 's', 45: 't', 46: 'u', 47: 'v', 48: 'w', 49: 'x', 50: 'y', 51: 'z', 52: 'ä', 53: 'æ', 54: 'é', 55: 'ë'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Extract staggered sequences from text corpus\n",
    "- We are going to build a many to one model! \n",
    "- So we feed in a string of 40 characters, and predict the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 120171\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 5\n",
    "X_seqs = []\n",
    "Y_seqs = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    X_seqs.append(text[i: i + maxlen])\n",
    "    Y_seqs.append(text[i + maxlen])\n",
    "print('nb sequences:', len(X_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sing that truth is a woman--what then? i'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seqs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: sing that truth is a woman--what then? i\n",
      "y: s\n"
     ]
    }
   ],
   "source": [
    "# So we feed in a long string, and the model should predict the next letter... \"e\"!!!\n",
    "print(\"X:\", X_seqs[3])\n",
    "print(\"y:\", Y_seqs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Vectorize Data\n",
    "Data shape should be (num_sentences, max_sent_length, num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sents = len(X_seqs)\n",
    "num_feats = len(chars) # num_chars\n",
    "\n",
    "x = np.zeros((num_sents, maxlen, num_feats), dtype=np.bool)\n",
    "y = np.zeros((num_sents, num_feats), dtype=np.bool) # Only need on vector per sequence (only one character in y values!)\n",
    "\n",
    "# Vectorize X sequences to BINARY word embeddings\n",
    "for i, sentence in enumerate(X_seqs):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[Y_seqs[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 43, 30, 31, 26]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 word indices of first sequence\n",
    "[char_indices[i] for i in X_seqs[0]][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "43\n",
      "30\n",
      "31\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# First five vectors of first sequence\n",
    "for row in x[0][0:5]:\n",
    "    print(np.argmax(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Build the mode: a single LSTM\n",
    "\n",
    "Dense layer on top:\n",
    "- Keras LSTM neurons DO NOT automatically run the hidden state for each timestep through the softmax activation... \n",
    "- As such, to get the softmax of EACH timestep's hidden state (what we have called `y`), then we need to add a Dense layer on top. \n",
    "\n",
    "A LSTM layer operates in such a way that it accepts input in the form (number_of_timesteps, dimensions_of_each_item). A very import parameter here is the `return_sequences` arg: \n",
    "- `Dense(num_feats, activation='softmax')`\n",
    "    - return_sequences=False: the model will return only the last output in the sequence (many-to-one)\n",
    "    - return_sequences=True: the model will return the output from each timestep (many-to-many).\n",
    "\n",
    "Token from this post: https://datascience.stackexchange.com/questions/26401/how-to-implement-one-to-many-and-many-to-many-sequence-prediction-in-keras\n",
    "\n",
    "**First lets experiment with some different model architectures to get a better understanding of how keras works:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 1: Simple RNN\n",
    "- 1 SimpleRNN layer with 10 neurons\n",
    "- Input sequence length = 7\n",
    "- n_feats per sequence timestep = 5 features\n",
    "- Return ALL hidden states WITHOUT softmax\n",
    "- Number of params should be:\n",
    "    - Wax (n_feats\\*n_neurons) = 50 \n",
    "    - + Waa (n_neurons\\*n_neurons) = 100\n",
    "    - + ba (n_neurons) = 10\n",
    "    - = 160 params\n",
    "- Output shape should be:\n",
    "    - Many-to-one: (num_obs, n_neurons)\n",
    "    - Many-to-many: (num_obs, n_nuerons, num_timesteps)\n",
    "    \n",
    "#### Note the output shape will be a little arranged different (n_steps, n_neurons vs n_neurons, nsteps) compared to our manual implementation, but the numbers will be the same.\n",
    "\n",
    "<img src=\"images/rnn1.png\" style=\"width:450;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 7, 10)             160       \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 7, 10)             210       \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 7, 10)             210       \n",
      "=================================================================\n",
      "Total params: 580\n",
      "Trainable params: 580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single RNN\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tsteps = 7\n",
    "feats_per_tstep = 5\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(10, input_shape=(tsteps, feats_per_tstep), return_sequences=True))\n",
    "model.add(SimpleRNN(10, input_shape=(tsteps, feats_per_tstep), return_sequences=True))\n",
    "model.add(SimpleRNN(10, input_shape=(tsteps, feats_per_tstep), return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_21 (SimpleRNN)    (None, 10)                160       \n",
      "=================================================================\n",
      "Total params: 160\n",
      "Trainable params: 160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Turn off return sequences\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(10, input_shape=(tsteps, feats_per_tstep), return_sequences=False))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a dense layer:\n",
    "- `return_sequences = True`\n",
    "    - Should return output shape (n_obs, n_tsteps, n_output_neurons)\n",
    "    - params should be (n_hidden_neurons \\* n_output_neurons) + output_bias = 10\\*2 + 2\n",
    "        - output_bias = n_output_neurons\n",
    "        \n",
    "\n",
    "\n",
    "- `return_sequences = False`\n",
    "    - Should return output shape (n_obs, n_output_neurons)\n",
    "    - params should be (n_hidden_neurons \\* n_output_neurons) + output_bias = 10\\*2 + 2\n",
    "        - output_bias = n_output_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_31 (SimpleRNN)    (None, 7, 10)             160       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 7, 2)              22        \n",
      "=================================================================\n",
      "Total params: 182\n",
      "Trainable params: 182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add a dense layer\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=10, input_shape=(tsteps, feats_per_tstep), return_sequences=True))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2: LSTM\n",
    "- 1 LSTM layer with 128 neurons\n",
    "- Input sequence length = 7\n",
    "- n_feats per sequence timestep = 5 features\n",
    "- Return ALL hidden states WITHOUT softmax\n",
    "- Remember we stack X and a_prev and cooresponding matrices for efficiency. Here are the params below:\n",
    "    - Wf - (n_hidden_neurons, n_hidden_neurons + n_feats) = 128 \\* (128+5) = 17024 \n",
    "    - bf - (n_hidden_neurons, 1) = 128\n",
    "    - Wu - (n_hidden_neurons, n_hidden_neurons + n_feats) = 128 \\* (128+5) = 17024\n",
    "    - bu - (n_hidden_neurons, 1) = 128\n",
    "    - Wc - (n_hidden_neurons, n_hidden_neurons + n_feats) = 128 \\* (128+5) = 17024\n",
    "    - bc - (n_hidden_neurons, 1) = 128\n",
    "    - Wo - (n_hidden_neurons, n_hidden_neurons + n_feats) = 128 \\* (128+5) = 17024\n",
    "    - bo - (n_hidden_neurons, 1) = 128\n",
    "    - Wy - (n_output_neurons, n_hidden_neurons) = 2 \\* 128 = 256\n",
    "    - by - (n_output_neurons, 1) = 2\n",
    "    - TOTAL PARAMS: 68,866\n",
    "\n",
    "\n",
    "- Output shape should be:\n",
    "    - Many-to-one: (num_obs, n_neurons)\n",
    "    - Many-to-many: (num_obs, n_nuerons, num_timesteps)\n",
    "    \n",
    "![](Images/22.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_37 (LSTM)               (None, 128)               68608     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 68,866\n",
      "Trainable params: 68,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single RNN\n",
    "tsteps = 7\n",
    "feats_per_tstep = 5\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(tsteps, feats_per_tstep), return_sequences=False))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now actually build and train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, num_feats)))\n",
    "model.add(Dense(num_feats, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 3.2682\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" be promised, but not feelings, for thes\"\n",
      " be promised, but not feelings, for thesnenenenn neneeee  nenren nenn nin n entieeni eeineeenienie eeeeee ne  e e eeeneeneeneern  neennenin eee eernennneneeeneenenee enin e nneiieenneeese t neennene inn ie ee n nininee eennennnn e ee ennnenneinneeee nnineinee eeeeeei enn eienneeeeeeeeneenenn  nneneeniin  n e ennnn niee en nnee e ennenn eeienine  ieniee entieiteie enen ieeenennennnieie einneee nne t nen ineieeie n ee enninienneeee nn e i\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" be promised, but not feelings, for thes\"\n",
      " be promised, but not feelings, for thesnnn  ieeinn etde  ifn  s innee ainnn,e se nnin neiee e ie t ny inenneadain n iie rnene ianieaerieteeninrie, ieeee i i nnei i enitnrerieninenn einnn rrenen aneenieoie enurineeninns innennenenreii t hehelesnneeeenr e  i ho nin nhe eeenie oiisnnesi n  ei tinse   so tinesnnneintini ie  nyninineno eeheiereiir inin  enenienaeniieeseoe erti eeeen eeteu nnyeienenenthiio ih nihn eire ire enitcteni ee tinne\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" be promised, but not feelings, for thes\"\n",
      " be promised, but not feelings, for thes ernliiue i na r encdteenheinhia iheetesmlens ogtsniuattepsipsthyenomrlhn!ueaeosrihyocheireehnseu re t nee enut ,fnddmuwet beewueeer eoselnnnnat aswniniifuebit,es ywniiasqninptetenhei lsitsa ensledef  en ifaee uniroetf cea rinei imep ietierit eriiei tfegtnilotte.oyt  adyiofoereeeiinenonrnipraweneesi eilunnnrbtisitemauuiyheseineatn;nyteeinip n!noaatet um  e hsfnin nulenhsexinnaaoten iewleni3trplesv\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" be promised, but not feelings, for thes\"\n",
      " be promised, but not feelings, for thessoaeäorpp n=i ne eti tenht ouipmese tn oteceokte )tngy irhsei shpse- rnnssl  mmei',vsrsoeo thieén,n iiewriffutnsiiwyeo xtbeee,otwinro eitohiuuetohetwutobrirr) r smrdiinp fniineiltaetlnindconokreud  nftaëenifuirsna,a\"uoontib,tnewf.rtanhhfaerpm ae; sinih!uh wreaoetclgru oieerh  nniso . ahtsnegsnnhnfuuctth eili8iedp]e ttrhpuoohnynyisrwr luutnneerth\"-nfrtnre eneiseyrrgl6udanekseeisnemourteeteas p!rpnn\n",
      "Epoch 2/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.9473\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"h is not that sincere, austere slave-fai\"\n",
      "h is not that sincere, austere slave-fai s t  s at ss t   e s tt t  as t  sts    s  te ts  asstts t s t t t ae a s t s ae ta  t s    aat  t taa a at s as t a e e   a s n t   t  t st     tte t s s  sssst t tsa e te s nt st s ss a t  t  st a tt   us e a sse ta utts e s stt tas so t t s   t s att tt atastas  att ta tss os t s s aaas  a t as s t ts t ans t t  ts   tsa ates asa s o s  ess  ats s tee  t aate  as  s  asas t as a st t t  es   s\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"h is not that sincere, austere slave-fai\"\n",
      "h is not that sincere, austere slave-faiesg htss saaarseas eanspss,  3 n  o etesaaseaaasas ats shas tact ta  tt tot odis o nst t  ntett ed sas tesp aa  tsteut fu s sso  eotat aesae staoa bs   sses onses,srasotn eaht  rennteontra itaroled  onhesreateeacastient te  e t os es tosastsaaauus t  yseeeatannn ssla anstsas afna s nasstlne ie ouss tt au ns aeel veus ae  sst  atat tett ts as utht tn a tdfau a  ee eeysrtunhea a s nsn sa as  a otd  \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"h is not that sincere, austere slave-fai\"\n",
      "h is not that sincere, austere slave-faiq ds3cu,vhgspeoapssi, rapoesnatsea ehfhestieon uth rrf namdol,l tfegfagisth,oa sugw\";ewseen y dluhgs iulynpt sslc  f rsn gsi gtwosysuhthtasrilaauthavr eesglars aeg saaan  utntbeotef o bsu is\"eigehr,vd,ut asbsupibt  wsa e cuntda h otsoit\"luo,ssaousvomaip:d eesdtgo ,nataye u sortso i tlrueseuotfothoilthsptcitne;k buws)n ngi ho ote s tpeod ndotagd,seoneldg gri nierf 3lmaar ,esrweftoosonnasnaetogselmp\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"h is not that sincere, austere slave-fai\"\n",
      "h is not that sincere, austere slave-faihrs!ip orrna eahrdta  d  ustyst e cahvr?ahui,ret fty,  yfewtatgwn v ma efhgn  tnfgtciooull ressufssm , tge usuon  usdd atrsa wsd rensbrdnt,h susnstdn, sht ktjresstasb8gseuti bl ntstt redlh.   fd-an  sdtoortkfhh,suyne,aoa titutwdsnlbwn e uestuo aetomostahso ossotmos ut.?amt!nsd-, h, -efoynraca.ry escl rooortyi.yvgsbunea.ne,ahnnoowfuvoipnauuntaa su t cb hssi elesaseooutbsft es afofuhnanee oohevtw,ud\n",
      "Epoch 3/60\n",
      "2000/2000 [==============================] - 2s 976us/step - loss: 2.8476\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ree prospect--but looks downwards.  287.\"\n",
      "ree prospect--but looks downwards.  287. hh ahh the ihhe hh the hh the hh thithe hh thh thhh the hhithhe hh the hhithe hhe hhe hhe hh thathhthithithe hhe hh th  hh thethhhhhhh th the hhe the hh the hh the hh the hhithe hh hh th the hh thh thh the hh ahh the hhe ithithi hh ahe hh the hh the hh the hh hh thhe hhe hh the hhh  hhe hhe hh the hhh hh  hithe hhhe thi hh the hh th thh  hh whh ihh thithe hh  hhe hh  hhithithhthe thhhhthe hh thh \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ree prospect--but looks downwards.  287.\"\n",
      "ree prospect--but looks downwards.  287.n hhe ph whs hithhiooaht  hhathe th fhi ohith othirhhe hithhh  hthhh hh theahhofh thhe hi ioalheth thhhhofhihhe l  hhh   hhe ihh the hitthe o thif ehhin ahesimhh thithothhh ahhatherhiihe iairhhi o  ie hh iis the hiiihhifhho, the  ioe hhathihh thhhrith lhorhh itithhthiahhithtosathofoith hh  haaoh ith iithhthi hh thaath tiththirashh thhhthh o tha thh fhoih th whe he hthe iu thtpphhofthishevhhif irhi\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ree prospect--but looks downwards.  287.\"\n",
      "ree prospect--but looks downwards.  287.  at i, wha ie, hn ytltou iassitheahoothhdifhf pthifhinonhhinhowrewenfhefio titathi oe,  beflgwwhe hishe  hhrh ahafhhurthk howinoehith iwh?hhinsnlhtithltthiisrtrsththnsinrhhe h.ehocehswht oikphul,hh bey l, omtaee , rosy cthharheaiss horb,he shh lhh thro i p,,th a, o hhhfhitdafh,e oaef,iteciohisrhfimhh hanhisarhe, ohsrha tf _y hmhinhfeouih oghh hw ihhe, irymilfiwe,pthh wioiito forhi bf ripiirethoft\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ree prospect--but looks downwards.  287.\"\n",
      "ree prospect--but looks downwards.  287.bhhokehaireafpbv nhoa. slha,othf,u-etoe, ohhifil  oretha hrsis b irkeotthatrrtioli8ioreh,aitteors,lfmitwthatsoefwhhyeiae .s teia ths thf .utfrlanm hs tefathhttharo   bhatthesfh wwheil,afnf ore tishthehtel lhaohaeo ff \"ip\"hoih atkf s:an.ahe,hhosos shhenh hhne adm  yhewisois athimhiwiowyfodhbe horont himasrs !x,reofefibesho,, thhe woni1anfidhoeeishekltifphuismukhtorhitipe hfcner\"y,thsdokelba tmmh  t\n",
      "Epoch 4/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.6589\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"l, he would go \"inside.\" the long and se\"\n",
      "l, he would go \"inside.\" the long and se an and ans an an ans an ans an an an ans an anathand an an an are an whe an and ou  an an ans an are an an an an an an an an an ans thas ans an an as an an an  an an and araan ans an ans an annranus an an an an arans an as an an athar an an an af an an ans an an an ar an ans pe an an an an an an an pora an an and an an an an an an an an an an aus an and an and an pan an an as at an an anathans an\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"l, he would go \"inside.\" the long and se\"\n",
      "l, he would go \"inside.\" the long and se and anp an  is ahe bas pous are atharand ind an mhangeuns and ans an pheny wheas wheusatanoroan ain athare ore thanalhal  prulp sors as aarfo, , af of ponns and of ans ans ans  aounheraaspof as anup, thenaan athand in atheraid tos  an  huun anas anaf re pa puroptare annd monuas as an aiad pnanp are, ans,tansanarisunr ph soran and if ass b roan aru, an of phor, apraseathas, wh  oraru anantpanuphan\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"l, he would go \"inside.\" the long and se\"\n",
      "l, he would go \"inside.\" the long and sepst?whulf or arobuaahe minwinpar wnapenam. :a dphe,-ttift, ouscrr rfurhe po , pepef aoflthowpre.sioneopofnnppa he fasounmutuphinvnf i, whie, the \"hyn!t aos? tato \"\"tfos ore piropmai s\"wa, haoh f pnais ofs tousfo\" i\"ywartofi os ankufe helbulrasenhppraw ap wtos wteuaw wh?lwp;  foas.atu ga unatogpiwtpisflous aicheauacaahar phece ianrtt ad mrus euae artspen, alugisophr  aesu ok so8ygin aneathe duphas \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"l, he would go \"inside.\" the long and se\"\n",
      "l, he would go \"inside.\" the long and seahbcf isurevi  rasfvochasoslasvlugiebfuthuir \"pliry wsmu titdecvphafam y miph , thedurof apy peptyaaeoff,n ard ondauae natanf me w g puaie ppaeltau  aaatttheunurs, bhvp posfhor , rmtumuuoyute trpak:itpf,tirnpensppatukl tireldm ruchthofpvuf d n. rlaaasp,nno rrarh tt!esnoef, ,knresarluneann pasefopace, bntywsa az \"uruwofoci\"eduirgpial -ffpfos!tunnmtusortusis,ofe auto,aspasiurepefopaearhnktame\"yfanhs\n",
      "Epoch 5/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.4794\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"m the double benefit he owes to metaphys\"\n",
      "m the double benefit he owes to metaphys and the the the the the the the the the the the were thend ther thend the the the the ther and thend wore the the ther ind the the thens wons thend thend ther and thand wore thes ind the the thes ind thend the the the the the thand the the thes in the  the the the tind the we the re thend ther and the the wond the mans thend thend and wore the the and the the ther the the thend and the the the th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"m the double benefit he owes to metaphys\"\n",
      "m the double benefit he owes to metaphys ares thed  thenres it tind wing thend worin wen ind wonrs the thes ond ind thas st and ins thind , and wuns therins thend pe tire  sod tere angs thes then ted  of thee and nte connd ornd w res ins then ann re aans and ind wind and mrrenge rerestin nod tht wind thed the wens ind whas than wind thee thend pere perand tethe ind fore ind sond thes indemedes and thand rudus that ins thol  ond ahd theu\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"m the double benefit he owes to metaphys\"\n",
      "m the double benefit he owes to metaphysphas ings ol anrevotutevtlusndt aus-thints fvapir8 intisto\"n me mtvevewisrendwing bongais alnd  tisd ow pnamdatpcralagnsefy wusiservat or-is ftheefuis vemhn t  ore, medethe air  t tis dd tif  irus siruvnvepat thar angict its whe p y benemvqfos mrral andindcor sucsrsrlagangrd whs mhen nvere ieve fsssnsnnv rasanngsrbuten es nnvelu sosduus y dade er in dadnwersod tesore lftecnrn islevinb mingnnnn, it\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"m the double benefit he owes to metaphys\"\n",
      "m the double benefit he owes to metaphys !r inueronnthos wiw naange iffind tfsisginurins sadnsl yobe ae 3uas rs hndeslinor slsirf \"tae wecpoy heve dust tn; plusan5d wr,athenkntev\"wor ?\"refnt osam wuains waotde o ndesucpmtywtyand beg wfn es anrmand tuan dbany tirerhane s, tedp3e arlf js in  it tra sanqtgnstagasginu os dryshetfolvevd nuds v thisqisgmiciednttysmuad, ly rans   na4o.tth,uc,gypnderompdey esnspiud kemcanoce ahedktandben-ujch  \n",
      "Epoch 6/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.4063\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \", and yet we should know ourselves as an\"\n",
      ", and yet we should know ourselves as an the pe tis and the sore po s the the sons pe the pe the whe in the the pe the the pe s the the pe the pe of and so and the the pe the of en the pe pe po s in the the the pe the pe s sos in the the pe the pe of as in the the pe pe pe pe of and the the pe te the po the the the pe in  of in the s an eus in the fore the the pe the pe the pe the wo s in pe of of the pe the the pe the pe the  of phe so\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \", and yet we should know ourselves as an\"\n",
      ", and yet we should know ourselves as an th, in te po of in thy  fo ans inde uly the  of the pe whe thes is phan an e  oo is the se y ind the ing re pe the pe of tha pe the the ousore wos pe of ans ind wo he of ind the pe  ad whe se pe pe of -on as pe s th and bo pe t ar and se pe sof do s in an no ao lus so ans the the  oud the \"od se eine in mus the of the the fore ind sod and sore wous in is he of in and in and in to pily ee thed in \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \", and yet we should know ourselves as an\"\n",
      ", and yet we should know ourselves as and geroendy pes re w quly fucear pusanat es-eftoporly nd a3 ely if bo butis sonaly!, th. wheay bs the is ang phe )s tied pfof !e unly oub. is oo if reg or s  are wiphe oe tof my vee th  te sudy ir af is of re sossns; seliaa ae vas ?y ind te core ;rsof pnry nig of thechesr pe s aiglus \"sely do. ewied in wd fosre stige \"o is and pf th daalkeris ro cos ud wh wremadd oudicig- naleof io p of hho te whe \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \", and yet we should know ourselves as an\"\n",
      ", and yet we should know ourselves as andesmx ol wry egvege todvie ols inqt is  aot phaverf pas we busente, (utod bo, paide, mf t, f iud tor als ttuo orusthu, n\"ppins coed teofof da h, phe rho tkea jpd pistsges anyif bill agbs bosepind austeg sh aruten ole m lu  lsgdeftis tiangame welu s, be afs ne f. ib sf aeveth pes peso ,ed eu-gvy she, ma fat ouly whed theucome wlpsd and, wf wo dt t thuges:sf s, resedtirappbatgam- htin.th vo yphesume\n",
      "Epoch 7/60\n",
      "2000/2000 [==============================] - 2s 980us/step - loss: 2.3002\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"e has had as its aim the resolving of ev\"\n",
      "e has had as its aim the resolving of eve the the the the the the the the the the the the the sthe the the the the stee stis the the the the stis the the stee the the the the and the the the the the the the the the the stis the the the stis the the the the the stee the the the the the the the stis the the the stis the the the the the the the the the the the the and the sthe the the stins the the the stit the the the the the the the the \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"e has had as its aim the resolving of ev\"\n",
      "e has had as its aim the resolving of eve s ras of the the tes are the cast the toces al aoe the the the dere tien the werhes the mens the the the cons shes the stit as es and thes, theal st e the te and sereses an the sthe sis the  ored ande tor at ang, that te, ans the  ind the ind the thatd stees the the the the \"t on the the tes ind sting the mans and the the the ned the sind ses the tee the me he the tis st tis mend the es and thas\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"e has had as its aim the resolving of ev\"\n",
      "e has had as its aim the resolving of eveskt \"avege, apces, lo ansluopusishert), whe ridteld unsechintin as the, ind coocy undt e oot revas ghe betotetiks pest of og,s is estasss, the saan, of ateus.\"sssees, theavovisrorabe is ne athathe \"isas\"asitpeact ie s \"hestrastinty soans apisbed all, of bast red inps avs an th theg  farss wevese -bs donblentec-te, ce tee the ba trs, ams berdles re sureve t ade ged8tiat abve tes ande ko spinke is,\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"e has had as its aim the resolving of ev\"\n",
      "e has had as its aim the resolving of eve tyjod inkawtad, isat at op \"ll d.ve wirlee\" dude atli t chige ors ar tr, intemragl rheer rees s.sctit  sthhesed cea of,tor dle.ntesgy ait thy msos is the gotiy the aped-at,stred thad beehtehyda ict esdith, au sdogatsr er ngatotittane thys pasbdt hnsma, js r, auste y bcit dorp, deve, the povertarithand  condmeao oly,ans it taeo, th atrethissuthethe sied sphor.sitps isinces, mheay aoets, cinecytha\n",
      "Epoch 8/60\n",
      "2000/2000 [==============================] - 2s 944us/step - loss: 2.2198\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"e in me to believe anything definite abo\"\n",
      "e in me to believe anything definite abo the tin te the the the the tinst wond the the weve the the ten the wof wow the tof wond the tis the tind the tit wentithe the the the wint the the the wer tevest the the wond the the cond the the the tint the the wond the the wend the ten the the con the the weres whe ten wenst wes went the the the wend the the wenstit the tond the the pestins the the the wend the the the wond the te the the ten \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"e in me to believe anything definite abo\"\n",
      "e in me to believe anything definite abo fod tis we ste  the tithy fos pe thes the fon wor wes the fons the pest and mave the wind the duthe tind the do sthe the sthe the pung, wen wust futtoustt das tentithe us tusteentit the pere ble pind the ton te the is the tof it t it wore t ing res weats whe woustite\" the it the the pedes the stre and the the tos wund the the perest the the tteed whe mor pund the tonstes the wo the the tos weved \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"e in me to believe anything definite abo\"\n",
      "e in me to believe anything definite abofaenp thitt tff ntut\"t peesplastticn ttied  ty tolsstoucd the itye thaug-of  ou wd hus suthe tolastt wspeis miss rfpribv-fontawinds the aand ifasuff, nonnutdereme u, budel--thitne  so st an afunutitouugs; wy)et it the wofuthitst anwendsyntind woy gly ffd to\"n the ctotle rre med \"in the tevofessfaped, as thengl ctenhi\"hi-darid ns, wow fod moqlievmustely won pfauly -thas eagnlinhy \"ust tines majlven\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"e in me to believe anything definite abo\"\n",
      "e in me to believe anything definite abondy is tha f coe wf en \"frnde-ufleb toe y whn to.esity .)  to th, isw fo stt ae whus cf iu, dtinve\"cwe ns? ptes the malhe fardeuct heco.jtind ,as wo, son ! foznrevuw med mabered xt cudntequsslndtt-es sus wuss\"cibt ofe ond the the  wnetinkdeus;,tenscs lqteecighi t eus, tom \"ns canatutew of buts. fard inttuend butrea; wip wenesutu, aol whencavly afkegy icad, aabefo,ith, tustwausuufis yoy thy katatec\n",
      "Epoch 9/60\n",
      "2000/2000 [==============================] - 2s 948us/step - loss: 2.1550\n",
      "\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ow, the dream is a _seeking and presenti\"\n",
      "ow, the dream is a _seeking and presentind in the mand in in the the more in the it in in in it in the it in the it in the ind it in the it in it in the it in in in in it in the the it in the it ind in it in in and and the itind in in in the it in the tind ar in the it ind and in it in the tind it in the tint and it in the rere in in in in in it in in in the it in the mand in in in in it it the it in in the iting in in in in it in the w\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ow, the dream is a _seeking and presenti\"\n",
      "ow, the dream is a _seeking and presentire er ind it re is mor mall yind in in in in in in ar or iin ingireg in mare ain \"ons and las int the of ir the ris forithe of mor te ingitenithe whind phide mar in it ind sore prens in auly in thind rore atind for it tinleathe perithe rich womly ion whit it ios ly in moristint of at it ial  in the of ind and of iof of ind sant wo wire ion and sor in and hipe forly ind sond itistint of are tind in\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ow, the dream is a _seeking and presenti\"\n",
      "ow, the dream is a _seeking and presentils whitheencelial s the motichefininet mer to, whenion alialnill. anoilspoly ts, pnithe prow weitisthescir ct in tirdorking, ia d pinl moromeandwathatemas acl, rioninfand aideeve mar of .ind aid dtreeuitle ffpeactcoelly of horemotinit mistinely the  ind peressirevorfiad er ing, the wicl perkis  of mocistoms of inlulf.kly mar the henialiby, lises'ins aluippl s, wil er in, is? the wilus, in spiiw of\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ow, the dream is a _seeking and presenti\"\n",
      "ow, the dream is a _seeking and presenti-theat auly aid \"hitludruofiredtisvskinlestiskat ofuthitswem-thvy porocosulustt and weeh ti. sele coinig fdrins  wheel-ienteal yala neek!ludcall aspy tinegrostivnr, whicl, helatidoritislteall, --wreny ig epruqige, iens eat robucillb iciluevemsithus, thit itiuntplires, whe , ml is py-to\"dthung 3ousof-alv ertigem\"theld it ofi dl; itmath; rensends is hheandy mo cias lofohoelis.ipusthe timl at, inn ir\n",
      "Epoch 10/60\n",
      "2000/2000 [==============================] - 2s 904us/step - loss: 2.0449\n",
      "\n",
      "----- Generating text after Epoch: 9\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"rness for knowledge (or through sin, as \"\n",
      "rness for knowledge (or through sin, as ins ins the we ins the whing ins in the phithe the mand the it of the whing of and the whit the sthe the more of as ins the pestinst whing the sthe the whing the mand st inge as and the mand and the the of is of sthe the sthe the mand and and and the sthe the sthe the for whing and the sting whing st whing the sthe the stingenst we the the sthe the pe inge of as ins the whenst whing the stins lost\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"rness for knowledge (or through sin, as \"\n",
      "rness for knowledge (or through sin, as and the what te for sonliss of whist we it wems chithous of the for mand and stinst mand btensthust we the the mand, whing whe montinst pe hande the weskis, thand the wond st of the meding the be cos the rand st mo s\" fon \"ins and ses and the besting fand the peshinstis the of the mand, the sthe whan the forestill the whing it the coms hind stros thithe stings whing ins ald ind ins af is dosthe re\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"rness for knowledge (or through sin, as \"\n",
      "rness for knowledge (or through sin, as ef of is nes sodrtitit winte wing, ans secuos of and mandy chonde sofor mornde thely aistendindt in tis thonons)d of we at and, wrounnnd ass onssillyy thand splheansoes spl des, whinst furts th. woccndly and thions and bechich, ind\", in wher the the rolomensisdis ahwerhes fovemtynd, is focs, when the lutemcons af ald thventeng-andy yores fitistsins; rosalf nos theiw, of the st ins be, ew of the no\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"rness for knowledge (or through sin, as \"\n",
      "rness for knowledge (or through sin, as hmvingwhotysorfy ind), tusod cherhebhisg-th ngweqilforatinqd abeusy bo fe deresteavh ith ghand stinolmestyes hiocosy iglenory and \"oluteam.meguhs-thenbd, icev\"s-the mons, unos sintabvsn lnst co\"ly itang-the tens-tiont whe?, a ssonsa instossssains nislaves irs genlre tnentes g id pup ha euco sty, whinsoncy wh ngrly qpaths dy solegrucich whinslu \"int it hanaw efenote, ins itve y the doche monngh, .h\n",
      "Epoch 11/60\n",
      "2000/2000 [==============================] - 2s 956us/step - loss: 1.9453\n",
      "\n",
      "----- Generating text after Epoch: 10\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"artness,\" and berlin wit and sand. it is\"\n",
      "artness,\" and berlin wit and sand. it is rererengendes and and the mand and the cor the mones and the cor the cor the weres and the mand and the cor the cor the mand and the cor the rerestithe cor the mand and the cond and the the mand and the rence mor the reand and the of and the restingthe cor the of are cor the cor the of and the reand the rathe sores and the werestingthe cor the mand and and the rente bere mare and the reres and th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"artness,\" and berlin wit and sand. it is\"\n",
      "artness,\" and berlin wit and sand. it is lustiand cor and and and the ain the rendestinntthe mand at the the the weve herenof the por-mor of che nithe of are sthe mand the for and whe of and the winond serhind ef the ous and the hing, whe the vene cor as the cor whe non nous? and the as, the docones and and the remrienginger and the reof the rore fove of chat the werithe sthande sperterenithe cor the rand and sor and phe beselfen and ma\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"artness,\" and berlin wit and sand. it is\"\n",
      "artness,\" and berlin wit and sand. it is dientingabf 3eatinone inm. ire tor renbe smeans, ase can theslp.ousquleem ons as  mar dyqunot of dateand the \"one af the busplye hitis, reas bfibungle, ivelyasuof tfmarhand- may ma d ar aliss wopuleyw and goreslbolis gred sand of ofandy wigh, whand bareingutorer theack mo ctisrer eo, dostacathe rins the cov aod te statevera d marevinem arl ane and lyerponde bethedrhmaghysoniss at alentiche \"s-lic\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"artness,\" and berlin wit and sand. it is\"\n",
      "artness,\" and berlin wit and sand. it is teobe ousmastod, oceretingef-ecs end th meee the ve, ures phs insed to lantisws, tivensthirey rud sibeseer vas, the pes.emabuvy nevir thes; orsmoreanderfirknd  as ou and ite cat ofaauads, the rinmasth)l murielreremsirolf king,all dre momengretierns fof )farero,ucevar:s, buskand dlesely, rua st, the gat in ronemsocberastore for te.rtoustecve wostyulvglly muretian of rthe bncoufnaarda eabidlyervit,\n",
      "Epoch 12/60\n",
      "2000/2000 [==============================] - 2s 870us/step - loss: 1.8420\n",
      "\n",
      "----- Generating text after Epoch: 11\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" such feelings--the person betrays himse\"\n",
      " such feelings--the person betrays himseathe athe atithe of atiat and and aod atithe and and arat and and aod atititithatheareand atitithe itithe and athe athe athe of are aod athe of are areatiand areatiand and athe of athe athe athe atitithe of arat and and aod athe athe athe of athe athe athe and and atithe and and apeato and are poreand and and atithathe the athe of atiat and areatiand and atithathatheatiand areatiand atithe of atit\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" such feelings--the person betrays himse\"\n",
      " such feelings--the person betrays himseat at the opeatith are wo and and and to itat the hiand, and at athe iand the poat and areatatouco are of it eonithe aod arat aod as atheandispreabdeatatitithe poaly io doraplocorithe paraloso arealastiengenocosaidathe athe raad ateateations ape pand are to it at that the ounseaintind and and ahe atatiand and ane beareabdentithoreporaint that it the bave of ahat atithaal at and arithatitis at the \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" such feelings--the person betrays himse\"\n",
      " such feelings--the person betrays himsearain iof asos coat ardtmave forito lope touthy pave colpeathe diats, olhe caind aneibianpaand thearyeict, iss ropustirisperolusy arit fopions of char aon whah tomprated andt as akathave baou foe pore, the atathous ofos ale of \"hy it thive toitheaysache ceandubeogierpaed tiom arat what the haid amattoomrandact poreniblaanat oce itho as ate aod pheirewoaly hovessteryancaeecarathe tpetitoad the iave\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" such feelings--the person betrays himse\"\n",
      " such feelings--the person betrays himseahey pandiay aousions aot iob herhiavvaseicaedico sthatrorsaensf oc, mop hardypuavlaalial; stododuailobyaneaiapsaad io iat ousthoverousellaponoperoal., upreerute uaoday ias huonstyos, reome, thestaaas ous atyerhed thangthiandorusoroua ueat aor it toaint asemva\"seof, to koratdvofonaad ioad the ispea thatutael, the prewenitppero, woam.breol now iol wo\"of epospand- and oo tre. it rocoputolaall aeudai\n",
      "Epoch 13/60\n",
      "2000/2000 [==============================] - 2s 884us/step - loss: 1.7420\n",
      "\n",
      "----- Generating text after Epoch: 12\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ulace got the upper hand in greece, fear\"\n",
      "ulace got the upper hand in greece, feartingthe rore bered the rederichende phathe the rence phistrores porel the renst the preastinded the renot phicond stre the docusthe the cor phenst and streand bereand buscondens and arle the rendes perhally ind ard ard arle prathe bus, whe phenstenten ins lores of the rerestionst of prrelver and arle procond stre the beres of phines phestreand burely ind the ropustinst perhenst perhand streathe ph\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ulace got the upper hand in greece, fear\"\n",
      "ulace got the upper hand in greece, fears, ind perhand stineng the busselons nor isply the renot pred of whinh pheselyd and a hens, the docisgreare breablat perhich, ind precint stind dire berenof phinenes of sthe of ard bustingthe bald arlated and the herestit strusklf the worustireng chat the morersol cochersplest us, whet the precoresoreresfincter the malustitithe it of whind berutherstice for cor perhand sthe it hend ins athe sthe b\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ulace got the upper hand in greece, fear\"\n",
      "ulace got the upper hand in greece, fearheresol-sthe it oudesuristome pald sthoffyrictoundenofaksteertuce orsthecs, wrenquberstrrous-lustael-steves, whir \"bumesps? ind vimereustlly sastingthuthulsquaceans, thirh tineasuskered, the mand, and bricis? demndectiblcond phast womusencand \"phersilf prathe sphiche beaqustrertor gheunted tiond the derind \"splectiss for wirn of onsthesthe ngirm aphathall entenderenotsas, 3idsbedleesurren asd lijt\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ulace got the upper hand in greece, fear\"\n",
      "ulace got the upper hand in greece, fear, the nthunecupcetstickluered acat precf.kemf-fond, ltrencteus inde of ar of andy, raed pusst wtrnostitotinopgres ay s, and biallably lundergpo hersterss klqeuscl, enge wieskl, the .sonion inpal rplediof puits rotfstreencinin finh, \"trestncy uskouspestedrhend the ms hu.spduchaeufoplinkarst mosndebuspeico er3iigessritg of of arot dosph the sounrepleastitot phr,ons tcp stolitr. pretsot trendasllyorl\n",
      "Epoch 14/60\n",
      "2000/2000 [==============================] - 2s 914us/step - loss: 1.6495\n",
      "\n",
      "----- Generating text after Epoch: 13\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"and mentored in an un-german school! thi\"\n",
      "and mentored in an un-german school! thin the rall the rerist went in the ralustinins in the rinl, thin the rand the rous in the rerhe renge in the rous the rall the rinl, whin the rand the rand the rall the rere prat wand arl are phin the ring, in the rins of thin the rous have berhe thin the rous the rand the mand and the ring, the rinn the rall the rous thing the rall the rall the rall the ring, in the rand the rall the rous the reni\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"and mentored in an un-german school! thi\"\n",
      "and mentored in an un-german school! thin the ring, in the res in the rerven whan arat the have of the ralustinly pritins corhe thin the ralustiren and the rust the cor pirh are sorithinst the wast have ithh are for tonive of it hand itintir that whinn the rous, thin at and aro aro ing of the ring of ar al the rand thit wain bean and and and the hir of it have bain the mand and berhal thin the rous and the rous thin -that beruin for it \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"and mentored in an un-german school! thi\"\n",
      "and mentored in an un-german school! thin no labed thins tha peatiant the vand chit pine, ins in the hir mage mand ins imon whon whir moll, aphinughiende\" thins. inathy ron itne, vo iay in now puignd-thanl chen soris! nave fore, that ias of dico itro and biol bere prainstrian ingee conitirintruve -that thinertray wain minninne prihntor as it, the nopl the  prast. of ithirt, whinnsall the \"tial sing nnt pliciwttionty and buesimal jhih \"t\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"and mentored in an un-german school! thi\"\n",
      "and mentored in an un-german school! thin of ar oucq davl acially-thy cuan , whe, as l. r\"phirentelve vraikusitk be wain it bion the  trilv mucn, fonst las pealade maninouns lajl buthined porha lf huwisal ty muntit poal ar-yur8uther onolly spiiqlr san ewy in doceand, that it pp fopeanty laytuog-nly us iaved thinhe sole ins \"ind lat of hind ine of beve of ahe theun at ea ofitichevipf ca.c eeeam and ins it hine, have osand and thial nows,\n",
      "Epoch 15/60\n",
      "2000/2000 [==============================] - 2s 920us/step - loss: 1.5044\n",
      "\n",
      "----- Generating text after Epoch: 14\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ing, reason, spirituality--anything what\"\n",
      "ing, reason, spirituality--anything what the marealy in the the mareang at all the dociand and and and and and and and and and and the mare of ithe the magisolustioreno areatitesorear and the rall the rall the rocaint the mareand and and and the mareand and and and are the mareand and the mareand and and and and and the ralustionen and and the mares of the mareane in the arlatite it it the of ingelyely inathe the canded the mare of ithe\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ing, reason, spirituality--anything what\"\n",
      "ing, reason, spirituality--anything whatistidatintteathearusioustirent the berithe ins at all the matimatipat inst are or allely inatialet the maticat of araing, the mamatiterithe it at ald the have arialusolustiaint that all the aimatives, in the of arlatentitico atiitingy ititial the rerestitithe ins at the magive the ainely ions, the candy innely ions, the docaist and cinsteions and the rociatientely all to ins labe for to aply horet\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ing, reason, spirituality--anything what\"\n",
      "ing, reason, spirituality--anything whatisolutitiugalf hame, reanuante corchirnlf ioniss of anheamemcouchiconistle torles?eattelyely ymatiaingy, icis. dors, the hiyeinstlly-tuatlopoce amateof itoly how trath it ions ior to thithtiach boreted radly canntiterstatingitirelyica\" payted aed morl. tha ded bithion wewrioed ibr aow titiole sorato-tourealy iist qeaskly-toaigale of ahlalestyatite, tha maglpe paramay alosoineriastiogouonidaluaid a\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ing, reason, spirituality--anything what\"\n",
      "ing, reason, spirituality--anything whatde, arithesinedereome lativalusompecaina es ianistieatinitt.y intfo-enkwed ousdioeen. phacatidotititet ieneeesodibal, is ia duveoustoustieenifrmatute soritine, torevaoustaushededieonevinopuagl, to the horatoveatithe sonrthaal areatowacat veariand ours, indt ood, e.nilo\"splv, inrs at\"tiof \"liatikis ucif peatutemareofi\"s, theagadomorirelul skeius, tevs ot sprelves uor phe iallverrerceasto. in any lu\n",
      "Epoch 16/60\n",
      "2000/2000 [==============================] - 2s 796us/step - loss: 1.4224\n",
      "\n",
      "----- Generating text after Epoch: 15\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ranite for being granite. in our own bra\"\n",
      "ranite for being granite. in our own bratinges of athian for poriply heresperesperesply the dasingen it he destrustin the resperesply in the magly in the wopl the rall the rtoustingthe respertingthe rthe tos prisple the desinging ard and the rasply the dasingen it he destion the resperstor and and and as in the magly in the resperstor phatiof proussof it mate proplest and the magipe poriplest ave pare porisol phistelve prathe sthe the d\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ranite for being granite. in our own bra\"\n",
      "ranite for being granite. in our own bratistrores and are phepresprerten the raspertingthing sor prathe priand insely the dasistinest in the mas poruplesos atrat the mabl goristingthe rtoustingthe reprenow phesosperist now phesperith phithe phiss, whing mor pais of arl arl arousmor the resplodesply instio mor pursple, whind sthe magly reat womussloresuros and ins wo lestives and and the vacuatingthe the mast and sore splres on sply the \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ranite for being granite. in our own bra\"\n",
      "ranite for being granite. in our own brabl bunlass, whes ipntelty ald cos peruandy sondemprico, in. it leapusoucismay the ithuugre, theshirvtl. of oplly have aspespemoply in ffirhernder tan psoichinctengtios anme ins ape silestironty pussl. as ro migt eestol that tor pe pplasting whirrursphes vand the strumest iingiag it , whes tirlf ind moripenticl sopirouns, as to dpsispre, ins \"verw puision \"t in \"prens, we. whinestof tin weming all \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ranite for being granite. in our own bra\"\n",
      "ranite for being granite. in our own branine sporkinghy athichinntingimorf;riavt ieg:prentekt ay es hugelutis al -tfive sperienmn es! have uthiplusuchesvelor tirow io phisonst mod bues of bers sas persallly o8 upsilegten tfore pralled woallnes ia ntemed dopry reons far py perreodms?,pin whand atleely rotites ar and giaspy \"sthire the gahiptecomas of buprelofis of arke co is leoussplast ing the womustoupea, toriconst inghhes if co temati\n",
      "Epoch 17/60\n",
      "2000/2000 [==============================] - 2s 794us/step - loss: 1.2980 0s - loss: 1.29\n",
      "\n",
      "----- Generating text after Epoch: 16\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"t we succeeded in explaining our entire \"\n",
      "t we succeeded in explaining our entire or arl arl arl are the dopus, we mand are the magioussoness and sore mand and and may ouss and sore proussolest and arl are are are proussolest and the resorean aspeatitingenst of phis of ins and sore proussonst and and and may pe the sore prous, and the renoply rest of proussolest are proussolest and sore of arly the sope panded of arl arle the mose perhally sprestoustingensely phis of arl are pr\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t we succeeded in explaining our entire \"\n",
      "t we succeeded in explaining our entire bere mopus ins sore solly lose of it aisenor prathe renos and asainousoreandist of aslly the qusesingens as the respodesole the rand arouthe relye of arl the sore proussungens of arl arl arl arl arl the rerospenof prousply the vasally s and sore tre the rask woskin the stor sprealne sporesson the gas, the desope prousss of arlea of the womas of arlube of arl. buegus, wente the mone sole prraloness\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t we succeeded in explaining our entire \"\n",
      "t we succeeded in explaining our entire splro splyengengall; now mersnot premoney oustap renas wospereem the stours of tro phist onstof thatpienon ariansing ionalesole mass onens loee bapbas wous, werus-emacly, the  wode, the maslly us, word dand a lastrodeshis more sblisuls pore ohy not pes?edesforegmanof aihe bone now peatinense fomem pe be pl. in the madesphas sore the mandestror nors, to destroinsprand weiness am. mephigosprousstous\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"t we succeeded in explaining our entire \"\n",
      "t we succeeded in explaining our entire phand by magemors, rf  s re whs perhuphils-sonruskr, and brie pforespeevrofispiess! phes nengterfof asl alugitet\"trenot anne the unse ossemon nopuims, wecho usalonere bend, we.rat tfomathsor sobdaliverleind af are pratinos bapd nad spor non pousally now ua dodyqiossolve perousple, edoelwhy the struarsplres trat vand bean ut.ed strt morly noceroplath. ar yoma sposkinverson ba of hin pome corsomatev\n",
      "Epoch 18/60\n",
      "2000/2000 [==============================] - 2s 796us/step - loss: 1.1524\n",
      "\n",
      "----- Generating text after Epoch: 17\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"g impossible as regards the christianity\"\n",
      "g impossible as regards the christianity in the athall, that ithe the mage arl al. the magusting the magusof phisally that the damand and and the magusof phisinot perhall-thand the resken and and and the mage arl are \"the arl are the magusting the magus the valustingthe athearst the damand and and whing the dand and and the magusof phisally that ithe the damand and and the mage of arl are the magus the vely the magusof phisally that ith\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"g impossible as regards the christianity\"\n",
      "g impossible as regards the christianity and the rall the mage of arl that ithe the maguskengens and aly the mar aluttingen athat the remand and and the \"thear uskemat all to it mage bere askelf the mage of arl the perisklf the magusous if aint, that ithe for to the gall the remast and the danest and the magine for all the dage atheared and and and the asmalle phall, the mate arl and the arhelven the .oruske the rall the ate asd of itl \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"g impossible as regards the christianity\"\n",
      "g impossible as regards the christianity it it af ite phedebus!ilosf asolace chmadait perhirnod rsmail dfouinial phaid the dasence, thitheruso aus of itherathaply and \"s of the vtou spratist ind mam of arl ale toning, that it mowupe mseave been, of sor as luest that arine ty it alr to.us, wine-ffabus, ffre usaly the prath, the hopf wohuareus!elay unidu-tmat the palusty haid aid sure 3npe ainestrofrtirly in mor praceat daghusican ly, ind\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"g impossible as regards the christianity\"\n",
      "g impossible as regards the christianity at  tere yes but eubhe asl it at thes ahealy  3at it there lade arild, and can wal sersullf thaid timevite pertion epbutemencd sunes weit deneabeandy phfiio\"- \"la toucheardomadubyyss kavd arlutf or alattallverutitavespered thite mand been there praty, thand inaeais lamen in! it huseceed of arounsterof afhe sence qas looualy al thingonda sthiren pla ay coct al, t nes the of arl of whir mabu fislue\n",
      "Epoch 19/60\n",
      "2000/2000 [==============================] - 2s 988us/step - loss: 1.0465\n",
      "\n",
      "----- Generating text after Epoch: 18\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"o well that as a discerner, one who has \"\n",
      "o well that as a discerner, one who has in the arl are whis sore cand at it the perial  porely t and the rous the \"speat danins an a aig of the whis sore candisting- lantieg ins lf the the rroust as tores in the arl ard \"s? whing heve it we whis sore candisken the mar as the rere prath, the quss and the arl the mar to the sole t and the rery whing st ave bere co phy and sore prath, the \"splat whing pe sply the strear the spre, the magup\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"o well that as a discerner, one who has \"\n",
      "o well that as a discerner, one who has in the whas sunting cand atial bus, and \"s? wenter as th whing st as t re of tha streat new plathy revand the riskstintthe as alost as, wow phisk phy is, wow in whis de in the as ally t we desiciand st aid cand and wo de is late aid aid ous \"ppe ard whit we phisill thir oust as lust westit the the prousstin the mars and grous \"s apd the struthe whis dand as lly the for to pry, whing cand as in the\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"o well that as a discerner, one who has \"\n",
      "o well that as a discerner, one who has to upridy un-tt, that it may poris? of nor wo pert ainng phy and tsoust the pryeis, w whve st and cand why ingence chat now is t \"miticanct ad a wo.us in as? w s. whis  a d :es in that valuty wht sing, dererulist abd ties iply of apl the now phich, ask mofuge fuch, whing  tode is,ols thinh--thatrtar ous sp. wfithe buins undidodedisingingind fo iloci, wims of ss\"lven it hiwtince us? whad ard alr so\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"o well that as a discerner, one who has \"\n",
      "o well that as a discerner, one who has dow phiskrecoiupd apmowisinich -thing, wha d ir. iny whed asd caw dins of a demy andtsofatspoust ard us! ptithr perhivas imp ica ma llatha ty ds this  fr. pusty, the ?iwiins so isk lude fphihincidfy ed the poreply custhr styhinst the \"ssalest the vally  undalf tho sporeenbn : puins--tpriss, wtrererelyed  tort ro tial ss co spelf ing of so upcratg, tha mar iun \"s rpom, the dust trat in rt it \"s of \n",
      "Epoch 20/60\n",
      "2000/2000 [==============================] - 2s 844us/step - loss: 0.9377\n",
      "\n",
      "----- Generating text after Epoch: 19\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"le standard before we undertake to estab\"\n",
      "le standard before we undertake to estably in the athat the of and and and a pespleat and and and and and and the maghe that the maguon phitiof bere the mamand anod of ain al. in the athearst and the maghe the mand, the qups of the maguon phait re us! and the maghe that the docans and the of and the maghe the mand and and and and the maghe that the of inn \"s an the aroust and the roplating co more porelube of and and and the maghe that \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"le standard before we undertake to estab\"\n",
      "le standard before we undertake to estably ror the roousplating--thoreas on the ropurs, thithes of the formall-tor tolusilinging, and the athearennte quastilathousplating, ins have ore porell the rerisll the mast and the of aslly the quve purhle the \"touch and have arativenow peacant-thavenof and the rall the hion somarlably that all, tor ald the of ople for to phicat of procist and aspeatitl, the quns, wowhice of it at allent the mand \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"le standard before we undertake to estab\"\n",
      "le standard before we undertake to estabe torisof ims at te dole oucheathalntickoskall etouce , and thut thes ion forathe mamlre bmag piccathy rhing, whas stitistous! of the indeenot nat porel the magle poreallof the rorabe for uspectisknt the quesplato behict, the oumenint and anle touce for have -rhoc :rion porhail, tritheas turtungt ave yincen the oo pore anct of the a mand thingriog souceesina deouss angely the womubus, unded the ma\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"le standard before we undertake to estab\"\n",
      "le standard before we undertake to establ  an moripl the rsule for uom puiclithana for plicg, and now ghe itleepertin -foritass ind of arll-trat veuselren iny mabus rung-morfhost lv, in tha earsolaghis pircoaoply ucot fomo to bery thas aucht tothal the ousss an ef einl-noy canr-forial, thd gast fonting, and a cand aree tosorvpermallveras lowe maskl -inlatt, when edouaseljthen ? whal mall been we havatiol, meeril , phire bohm cooupheouso\n",
      "Epoch 21/60\n",
      "2000/2000 [==============================] - 2s 830us/step - loss: 0.8297\n",
      "\n",
      "----- Generating text after Epoch: 20\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"rds the constellation hercules: and i ho\"\n",
      "rds the constellation hercules: and i hore sone solating no phatinon as and now phest and the rened and and may phiss on a uand atial bhin mand and and may on phins and and may pe hand that the dand and and may phiss and now phiss and now phened of ain a penow phins, the \"speand and and may pe have bain now phiss and now phes of phins and and may phins, whing re the mand and the mand and and may on a pand a damand atial bhin st and the \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"rds the constellation hercules: and i ho\"\n",
      "rds the constellation hercules: and i horat we mand and and mor purs and now phison amand aro and and may for tom phas now phess on the mar as the dand and and meresonce beos buns un at al  have byen st and not pe have buin st and the rone the asll.e for to in strial the donas of arlat valua tinne peandathan the have bean for tint all the hand the mage of a land atial bhin the rand and and have bence can aty in the mand as lorenod sal t\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"rds the constellation hercules: and i ho\"\n",
      "rds the constellation hercules: and i horat whing-st \"d in \"t aim now whinn now phest fandt ol atial of all the ren the it in a bama and all the sand the dandy nat yery int wo df ins on a kand, the dalunolunot vand hiin \"t whe sunct whing neveny in luas alr now of bape thf mastre tre non , wh not pe bele hinstive of aml aly ad now sore prasingy haid atians leesuins for in de, in non pein , when ar ald mor ins have buand atienst the of h\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"rds the constellation hercules: and i ho\"\n",
      "rds the constellation hercules: and i hore tortgulf of delues.ly it wwath arkely inatingally not mans a nevely have bees, thand and heve ow may lab is snat venyen it anges dabt dand sfall andat mabl  aplative pelocably car valureniven, whes  or pondong)y wo unstive kondems what venoesprof tho porencans, tiren--ting, whing med pllqe sconrtoul th. dentenst vand  haid of ainy, wo knd it le we, ans not pe has peneisf asklat tha lyind asrean\n",
      "Epoch 22/60\n",
      "2000/2000 [==============================] - 2s 858us/step - loss: 0.7210\n",
      "\n",
      "----- Generating text after Epoch: 21\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"and the highest problems repel ruthlessl\"\n",
      "and the highest problems repel ruthlessllet cans lf the streat of phiss wowh hest and and and the strust westiorst and and and the strust finhers of the rest of tomas perhald sore prast ors of tho dore past of pratingen de phist westiorst and the dast ans and the mast and and and sore prod sore pand now phessouss and sore phiss of the ress of the mand and and sore prstherrs, the mast and and and the mast hes sore ball be hand and and so\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"and the highest problems repel ruthlessl\"\n",
      "and the highest problems repel ruthlessllethus mor phespert wh phathars of tho spores of the re phessous for torspled sand and sore pros sor porspled of pesians and and the dast of prais lfer best of polosube can the mast ans nor porelle prathar toriply have byen struresore phais of solr, whind streat for phess on strians ro kos it we mest wast res of mand ain of phiss wore ors of the ress mand ars andes of de ins and sore ins lfat dast\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"and the highest problems repel ruthlessl\"\n",
      "and the highest problems repel ruthlesslled sprimedf ars of thaid of sendes sabl . mar and s or somabe fanstyeve on phinefforesonond as peatined of weid ns herealysy wed of phict and tore it as sore mens of rrud struandy hins-rins the mass of tho desploses ope pramonat phascors of that whes cor nan fir sore cins at eves op splat of mast ald now ens \"  opercavenciins reches man sorencian strires mard arl ar all st urstly s of trrirvof ve\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"and the highest problems repel ruthlessl\"\n",
      "and the highest problems repel ruthlesslubthas to it domhd eestheres bul of is rocustives are nct mor puitl\"estrias ffrrtand of n whiss of dlnesplya canly hevenct dand respriand beiclfowopuacaved; ins de massoors and ffrion prpurstors not vely the valustrusts, toresmains slabe, the qunst lfeusores s8 af tfeerma dy ndathy relng-nhenbe sorisleas, ald the wrins nor sore ss? whmt hiss of es eave ass ane phipesit derhoperous of tros denred d\n",
      "Epoch 23/60\n",
      "2000/2000 [==============================] - 2s 804us/step - loss: 0.6322\n",
      "\n",
      "----- Generating text after Epoch: 22\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"e is to forgetfulness, how his mind, aft\"\n",
      "e is to forgetfulness, how his mind, aftitines lore of purs, wow postined of phiss on phicos in whis soruthed the maghe that the stouct wemustined of pulos purs of pricist tithe phiss, wow pores aplathe huphest the uspuleen it matusore cand and and the masiof priss, wow pores aplathe solees and the respecting sprouss asply reand strathtoreno aplatiouss and arl astimond st and their of phis spore phatiouss and and the mabl enet perhalnst\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"e is to forgetfulness, how his mind, aft\"\n",
      "e is to forgetfulness, how his mind, aftithes thand and mow phesperspled sporesply reand and the mast and troussous! as to uperit the machat the mably  cant the \"isplathat tint-rhald arlathut mow pesplathard ard ousspertisns an the momast and and and puigions rulle psislle thith of phis sube not pe oppssolust are ousstititharesuspedisthere forutounge cons ande ins lustiors on the vasulocuthiand the ropurstouss and aspedeitist as lubeine\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"e is to forgetfulness, how his mind, aft\"\n",
      "e is to forgetfulness, how his mind, aftitist of phionso spere mand and thingreand the ten deauce diof of isply ecvand the ves\"dang nma danfille sopechinst tomud micm tho perouco  in atid inst pe hays of thus such neve pensally runt, whing ot the ralustand the lopde angteust or spored is, wow phiskous! whand storisnoniche \"upheresperusmacitiinnd sohave arhind tho pe ppbill-ty it heveryirls trat woins no spertinst we struins-rfirhen the \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"e is to forgetfulness, how his mind, aft\"\n",
      "e is to forgetfulness, how his mind, afte\" inpe of purca, worusotquvep.y-rand truinnst the mascol s onithe rall the austionss op. pcith ptioljsnodire ons ini mor the \"sprathane aulalustirl esoreaymea cansy lose o phy rith, whit of belae priol. suin onditore bhacs of ithees rangatdf habd a wuns solenhisns rupllesud aped ions! asq psill--norsioce puiss, whing the deeis an smob, whind losopeecinging hive whiss of is? 3ond ceasdichatdyrrabl\n",
      "Epoch 24/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.5433\n",
      "\n",
      "----- Generating text after Epoch: 23\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"f esteem seems to him incredible. he can\"\n",
      "f esteem seems to him incredible. he cans logiroustherrelven in asoungengers on the rocuating, the ques ing \"s the maguof the reres of the are of trut tore. the maguof the reres of the are of trut to the wopustrube by the maguof strailt-rlate ithertoustrertelven in \"t and the restruat valube of arl the ralust or to ably rot sor prial, tre valuations! ard and the sores of the are of trut tor prespererer, werh ore porell. ter puaslle the \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"f esteem seems to him incredible. he can\"\n",
      "f esteem seems to him incredible. he cans-rt cand, that it we mor phinous! and the rtoust ard ard of priol, tor and the roul the rall the rtrustreres, the maguos bungousten the rened of it at tor as tor purally tor to pherrolest ard bure bhe destrusten the rous te iol bere phis meres of the struand sore one sole tor the sures, the \"soupgreannt the mand, the rand and the rand and the valuat whing of the strall-t or hore or the renot of d\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"f esteem seems to him incredible. he can\"\n",
      "f esteem seems to him incredible. he cans, whan of phistruat whing for es the vonor bungestill-the dogupesthe locinit an tfat to porithe sorrenging, to hir that are of the  ond the hopd sor priol, spriskon esouibeepreskem, tre da unbe sor ancion \"t uigtor sung, we hard, that ins meresor inn ffyem the dopedielvely terruor bengeche stre tint, in for the troubbe, all the reons a begork nont, the sore arl buensthe. enyenger yor pursll the l\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"f esteem seems to him incredible. he can\"\n",
      "f esteem seems to him incredible. he canterrally pray in tre very, worlvores relk-ff.re, tentitha-rtl. sor tentith xistrrith, thes, mer wothe pers, lvery busklequs suneven in labe or that ion we leres? tr, whe qurs,ow the ment, en we, wedndened mas bus luner-thes ue the maglat wist and a lans, the huss to s.rferrtr, trat io pratiaus speertert wenuons reverofubhirgougthat testoursnot ous \"prr and all nof \"rpulesor  fovl, werhin askensty \n",
      "Epoch 25/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.4525\n",
      "\n",
      "----- Generating text after Epoch: 24\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ress\" and the \"future,\" and are more and\"\n",
      "ress\" and the \"future,\" and are more and ard mane of arl the mare or the womustionst ard oustrear the marle thing of thing or phico of itll to truth or porelubtrear the maghe the maghe trat torupertre roplating sperithenrt or to pprill--nor tomand aroustired berhing-surhior for to the wopl the roplathand the marle thing of arl the mare or the wopus oustined of arl are \"ope plath, the maghe tror the marle phinstions rery whith or phinodi\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ress\" and the \"future,\" and are more and\"\n",
      "ress\" and the \"future,\" and are more and are sore bere phaistlurerore one mamince mor the rocuathing, whinh of arl aro arl are \"pores oust ard the domass lereve the maghe t and \"tours, and giol sour, werhe rous lore ors lane of itirosorimas oustrures, the marlees of bulosous!everking, the magion phicatiof phissolust ard oursole spratinges if the \"spuleenond and of irl aopisole pricl, the quns ang the morme const ow phisist tore or phino\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ress\" and the \"future,\" and are more and\"\n",
      "ress\" and the \"future,\" and are more and arhingers, in surhior for to. phissolf rilanf ihis, andenojssuceimand able ion abou buagl ard ligisilf fivatititht rest of for tofi? whes pors on t aid of phitime, ind hopleertan stheretre one now phestince firle tor posply hind, the moghe pors, wowh, the maluof arl arotrsoreskearlue we mate of purhous eprrilnof cias loat de\"tholves of it leaidiof tho aply tost plstenbe corsplate, ind perhallde m\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ress\" and the \"future,\" and are more and\"\n",
      "ress\" and the \"future,\" and are more and at.e why ouso opriof thit morupors phrhi, whain of are sonerutertallyel biin mave ore magu.e that toe perhithous \"kinl-surdieust alnes bermably not pasd ores bertevksore phismous! whigh oretoricons \"hat ass lone sply, weand somimofa\"ly and the arle and, and gand-the ory heveuof philo ouphell to avjicintis of a late bere sore mphesthusds, whend droter coand thatgicat plisoll ismar \"peigatityri, th\n",
      "Epoch 26/60\n",
      "2000/2000 [==============================] - 2s 886us/step - loss: 0.3947\n",
      "\n",
      "----- Generating text after Epoch: 25\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"cal mystery of the ultimate cruelty has \"\n",
      "cal mystery of the ultimate cruelty has not phissol straid strald bore or purally splateskeciat in splees of splateveryicand st and now phesporespeation phins and cons duned st and the mast and sor priss lore or purolly phandisoned it mowe of aplathat dostruand sore or all to phisolest ave of phis more prspld, the dastreand and mand and the dast tiskne shaid and of prisplestrars lone socandiskon whing so ar al. phissolest ard of phis st\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"cal mystery of the ultimate cruelty has \"\n",
      "cal mystery of the ultimate cruelty has not phespelospermat the \"rouss and sor prass and cons duned of it aidanow phespors oubl comisined sof phiss mor phasd, the dastoust arsulfthe strave burstiand not pesplast and now phespocichas for porsplat wais nor porsplat valuatinedfof mall buph asqepersill-for tave buin st and soresporally rungis, whinh st and now phespersplatives, the dast ting can us? hines of cacd atee perhlast reskendes of \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"cal mystery of the ultimate cruelty has \"\n",
      "cal mystery of the ultimate cruelty has soce can tho presadocis \"bee porm, fof procaducat of ispla twe whe poperoungecocas apkitistreresosorinn as thathes buas fin of whid somes if ralustrarmt the oussoucheadd bein so is lfor d ouss andesprect, mer ouss of splath the veous, and sow cains luned of arl buof whins na d rescessim stiavt of priall buin at able oa dalustrand forihatiof prasd of d ais lofiss hage beem man the rast of wrous te \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"cal mystery of the ultimate cruelty has \"\n",
      "cal mystery of the ultimate cruelty has sore of thas nor platins sforaconbe wo phaslast fones mabl bucgas nde  ouss apd \"sbjhees of thiterof bhisomuchist ars, tod mins hast avd erstor prais late mama for tond, the ofbais lostrorthustheres allouabuscyihist and mama teat it last of plod mor poacane uphiloceespain es alainof de sonce phat of arlagy it \"hat deguchesing aliable, in pfadunfith redesorcperus an auod dent-thened of phissolf-the\n",
      "Epoch 27/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.329 - 2s 928us/step - loss: 0.3361\n",
      "\n",
      "----- Generating text after Epoch: 26\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" that its effect upon our feeling is cha\"\n",
      " that its effect upon our feeling is chatd, the dast ing on asuint whing st ave of the sorrenon phiss and soricor of it wemand and and ow strain es andenores af the dast ting -thaid the dast and and and of dich st ied of striresurhives, ind of arliant ous ande or solist and and aid of this elye phaid sor ins \"t we hand and and of streat the dast ting soricans it mowiso a can ous \"peen st ave oursole s of the magh dedisinne caint mede of\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" that its effect upon our feeling is cha\"\n",
      " that its effect upon our feeling is chatd ard oursoured birs on a ligions rere ons \" vere of arl a wion s aideoussilven is more phatial by indesores able or all a wiof desician -rlat or soresuring cad of puiss, wor porhust avd of desion whind and and and of or been domicias firitined it medes of anciand strain en ithe ope oles ours, wow oes rear ard ous ouss and sorrus, we hand and and of de is lf the sores oussion s ainting cod it whi\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" that its effect upon our feeling is cha\"\n",
      " that its effect upon our feeling is chatdere, and and inqerous! ard ouss ane on  pirel sorerungen ins ande or , yerred domis of rulosorirus ours, wow phese ins lededusined aid of trit of cinc, mer in s oues iod of this eore farltole pprest wrthe dadesucians ly ion as hire, ind of renatinat persplr-t restindthe, to peres, tiohe of arlico, the maress have ardinters, weinces efreessolere or a digios dand aod oung in the resknca of 3rat we\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" that its effect upon our feeling is cha\"\n",
      " that its effect upon our feeling is chatdaledy intheseens ayen inced, thaties one foreions ow ince ofca lonicis line s of whis sore sole forrelven corioln enrear plities of asmall; sprr, wkins soresur, whinh ot enod hund, tow of sopr the weas bere abd samiouss andes owes -ard not may hyirust firttevo reaud wo wh the vely innt deresosperees oves cor ustenot gind en the ouss afat of sich streredart evee phis lone madd ion asd all coris, \n",
      "Epoch 28/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.3007\n",
      "\n",
      "----- Generating text after Epoch: 27\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"s no antithesis whatever, except in the \"\n",
      "s no antithesis whatever, except in the whit mard boie soresplees and the mast enge candartousthe tolustingthe conses an the more of the wopl the \"speat as tores, the mast enge cand and of ith resken destruat womuns, wo mor phastit the mast tor pesply t wh struat valustourely that all to in elye cons, what is an the mare, the mast and now of stires and the struat the manest and the maness ang womly speat pesplathe rolkes of the velye co\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"s no antithesis whatever, except in the \"\n",
      "s no antithesis whatever, except in the velue phinlato phisl, t we medeperto benge for tomins, wor porelle purhl. the mast and now of sthe sore porselve phisll. the \"spelfens, wowhin fuesting ouss\"ly huvenot as late or all to is mabl buan for howe of phins ow ind and sore or phins, the mase on a with strortengengence conce now so us lf the struat the mast engepe to phestoustives ions, wowhe athat the mane, that it morephesting, weand so\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"s no antithesis whatever, except in the \"\n",
      "s no antithesis whatever, except in the wocuatand, tre qurestourhuve phiss, werhor pll to speris, werher tist corusthertiveouiss, the quessilvenid cal eamand aidinot all the dale of the asply and that ille to upe phins \"s tor dostrortpurthat the mactat the maney opastingally that all the was pole porell. purs lone streart, thenda speskims an the mastas the \"spelfelnde hand and inq pere persall, that inteve far the \"s?iwociatisonof ans a\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"s no antithesis whatever, except in the \"\n",
      "s no antithesis whatever, except in the \"speand on a waminotive ,uins loreuces mor wo dostrabde ned ion ay ingthe arly ockinod solustin as avouly have brig mormpesponce phicasinot op as and weis of asily s of domins be cans sfrvesbel. now pas lustives spor, andes of arh phis stea mane; it pe we, whing eedespechapstives, the \"s\"dabl buag magh erous, \"w th thea strears lore or plathy acle for time pyrisen ow and ailiofowiinthe belcto. spl\n",
      "Epoch 29/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2590\n",
      "\n",
      "----- Generating text after Epoch: 28\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"such a distinction to have one's own ant\"\n",
      "such a distinction to have one's own anting, the dast ting on a lan ow phins, thithf itidesore can aty whing stiand stion f itilf thit the dast ton asply rer toruply ror plies ing not pesplathat the dast tone pore on iting, whing of arl a mag priskng reanting, the dast ting bein of itirenorinot phins, the quessich the dast tone may ungendes of thing ore mone solr thing whing of arl armally rung, the qupes and arl a danty have byin ffore\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"such a distinction to have one's own ant\"\n",
      "such a distinction to have one's own anting, the daghe the damest the isple the dast enge contither ond anlia domiche thing the dame thing -that the dast ton phins and ard \"trears lot we mor phinest ing--thailt not pas lore of ithaittre cor domusting that the dast ten damenot and arl a dane-thit the dasupe is! andeny inl, tor whing sore prist, whing of arl a wo perhias nor asmions nor porelodube of inl, the dast tor pris of whithforeson\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"such a distinction to have one's own ant\"\n",
      "such a distinction to have one's own ant sorenor phist and now mat of thpir of is\"litit mor porith, ind aty w wh dand a damenithe strar the lade isoll thing renhe ass and sorire, inn whith or  ungy it have baig monise bich not son ithareny ind the roplating ins afreand the sorinntemor, whingf reskns thing -theatting-that the dass on the oco oplathar the stievs are now phins ane all ly ind the rened thing re kins abl as, wow ve more of a\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"such a distinction to have one's own ant\"\n",
      "such a distinction to have one's own anting, inqever-th, lf the not past of the beaon bund is, and ork nois ope not pespirond phiainn, the qunesty and cantit,, ind not hape dinl stiaint and ay ion fyiih mindating, whing ow phinedfore on a tinnj scat in ely h wh\"d chas and not posplyelves bnint, whing, in the wopl. thing -thesty re nor pils afiend store now have bring, whing or ! puad is\"ly vend eod sophicined it ot and and ain the suoh \n",
      "Epoch 30/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2143\n",
      "\n",
      "----- Generating text after Epoch: 29\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ome extinct in natures that are destitut\"\n",
      "ome extinct in natures that are destitutesorencaing ie of aill, the dast tine of a lane of ithe ard ard of ith are of aruably scor ins le hedation so arual byecist ave of ith are now pe oppessole have bore of asplatines lath ard of phiss athe sore pore, tor dast of pesplathat ties af the ard of arl bein st afres of stiresforito is labe of a lage or phassoust af all the dostruand sore or aplathous of the ard ow phis of arl buigis of arh \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ome extinct in natures that are destitut\"\n",
      "ome extinct in natures that are destitutesole malusofirle tor ole ppissllfbe cast of st aintor somare or a ouenod cand and moghe ins at arle tor pesposking, whing of st ie sof the dospesion st aigthe atheasto it aig of peis, wo huresorciat it we we phatiof cand and aineof buin spores and and of de it we hestions! asdiouss af the wosusor-the athearenod it we the masuon purhiand sthearenod it we we hese ins af the sore pore, the quss ang \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ome extinct in natures that are destitut\"\n",
      "ome extinct in natures that are destitut sole pale buintis, ins at it \"s mare or puas of stre whin esoecmaply rorapoduthe of aid of itihisore pale, the \"ustrually rorhal the dosted sole perhulrst ame mane are of itid sphist tre wasuloce dame awhinot ol suchimithest wh the dastouds? west al to phisise of it wost tiresorst ave puscious! it\"iof balo fof ithe conspecoichitiresorall-tunstis, \"n the wosusorce the arme. the pos-inor plaes ion \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ome extinct in natures that are destitut\"\n",
      "ome extinct in natures that are destitutesoffiel, to it redarto. strith, the st hat athe and aitiof semo inn ef ehe soofryrhca sfoideror ionco ihillorkand the sost tor posppeandich arm of it we mer eof phesporisenit and anlamatiof \"sphfrrevas, and \"ty when ef aill sprand, that ad arl of itl it we wo periting, and athe destruans sore cosico it ie of ithrisonessiof aud aice of ies atiresore, the doous oursolrhuchdiresolofocaall. burg of s\n",
      "Epoch 31/60\n",
      "2000/2000 [==============================] - 2s 892us/step - loss: 0.1951\n",
      "\n",
      "----- Generating text after Epoch: 30\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" deceive. the fact thereby becomes obvio\"\n",
      " deceive. the fact thereby becomes obvion now phiss and arhe bor to is ain of arly the struas bere spriresungence candathours loge pirs, wowh strures of thuthuresore candally spratestruas bure splateves and the stru, the dasicat ve ouspeatime for tout tes of priss ang for hithere proresungthe as all arua dunthe arly tores of the the strual by ics mage of arlat ouss arm are sore splretherrull, the dast the dalust ard ard sore sple, wh wh\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" deceive. the fact thereby becomes obvio\"\n",
      " deceive. the fact thereby becomes obvion nor peatial by hitesof itilesube sthe as lle sure may pe mall. sure arl beeh spri, the desicial by cans re maciof praill. the dast tis of priss lore sour, wewhust ors on sule the dalust ard ars logipe is \"t ard ard arm strures ous af all a uagupe the strull by hicat the mast and the struat of prist and arl arl a lagupe have blee sprr, wh heve ard ave sore sourselve preaply rat ar all the struas \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" deceive. the fact thereby becomes obvio\"\n",
      " deceive. the fact thereby becomes obvion nor pore prist, ind sthe at als now pe pals the vevy have arly rogus, the qears lfrerre sco. it aem arl truthusichest ave tream ase ussemirl- the asll. the dass lfre,reprs, the despciall by hibees a dalubile splate, the desaplateveruely s and arl armall;  3rirsply re manduthepre, the mesthe. as \"peat it mabupr as. wewhe stor the dasimes of whis sorrudesbly the sustrabnt the mamans an alla hor to\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" deceive. the fact thereby becomes obvio\"\n",
      " deceive. the fact thereby becomes obvion ly;rialv may n es \"blle toru hith, the struve naill s mandeabuely s lare brue cad mar ion suberus! is \"p of phitmathaud aly y ye mastlect wemest wathe tioncy nstevenees ave fooce of buih, a d \"t iw wond solans. ace cand arl ary hud soris, one splateve messpereanly s \"emafudedititisand yemis of pursullf sure of be mamandienod arl therrull, the delies huresbly s lugthe strar of procos beve asd mer\n",
      "Epoch 32/60\n",
      "2000/2000 [==============================] - 2s 884us/step - loss: 0.1667\n",
      "\n",
      "----- Generating text after Epoch: 31\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"n under the dominion of science, who at \"\n",
      "n under the dominion of science, who at the valuaticiand and arl are of ithe the mast tor prespore pandinon phiss an a labe conce cand and and arl are ion arly tor tout the doruporeloct whin ffores of this sorenceans and and it re arly ror porepores an dubeecist and the most tine of itle tor pesporesprall-t ard ard of it magupe the struand and and arl are arl are of arlat ousprall-turesore pandithe most tone, whins and and ins have ard \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"n under the dominion of science, who at \"\n",
      "n under the dominion of science, who at the vas of arl are and stherrelrenof prisoly is mofirme ins af ithe resprores ably teresorocain asmally mprespertinst of phis sperhithat all the struand the most tinesore cansaly what arlat whin st and the most tor arl the wo pescian aspay in the arl arealudimand athe to phatiof cand and mogies in at ave asd mane of atiat firitore pabd sofire, thing ond the venye cance cans and and sor pricist ave\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"n under the dominion of science, who at \"\n",
      "n under the dominion of science, who at the \"rsplret, the hape sorrably ror to berutitndastousiture fyeiis mere ofpirmans for mor porsplat , ins merhuves aplate beigison a have bhiss yer errtpe tom arl the sorrsplat marunot pheco, phicosof wiin \"t af times and the string phisimest in -thay ke hald arl arlathy prial by hatenod phiof wiin sthere, and the domisspereinide it \"e sprraf trod sore poraporelofupechicandiredorile fofimat not pos\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"n under the dominion of science, who at \"\n",
      "n under the dominion of science, who at the dosperston duins al amo in asperersincis, ins of arly hor for have purisloe asmous!uphesd, thitherr, whe dastou kn. is bereenines morups the qopably tind stra, terheare broce, an et-and the valuably that ar and sor time forlie on a inat ias lave ard ale coube platowismor pwrtinnf risknof rapbathat alhent-that ty in that as the are prodist and ins ang ind are ar, loye phass and cocisilnse \"phes\n",
      "Epoch 33/60\n",
      "2000/2000 [==============================] - 2s 988us/step - loss: 0.1343\n",
      "\n",
      "----- Generating text after Epoch: 32\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ropitiations, through flattering homages\"\n",
      "ropitiations, through flattering homagest thes the dast toses of thodes of arl thur sorusthe ted sthe the struat of prle the struans lone st ave bard struat we mode sole to der phas lustreves, the masusone splate, the dast toness ang sole trous of thod sore the struans lone straing stherest test ures bule for toues lode of delue phicll-tor sore sole to der porel bele struat forme to thestions \"w what vely s of the struans lone st ave be\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ropitiations, through flattering homages\"\n",
      "ropitiations, through flattering homagest thes the destruand sore to at alle bere phais nore of asll the renges is \"t testhe atheart ned of arl tho dede of atluating, the quss ang sole hold sore tout the struat of arl buph dast toness ang sore prod stores bele strarenode ope toues ons \"ulustit tele thind struat vesplyes of thud sore sole to upre, the dasustind the stourenore of thus sore sole re bene sore tous the valuating of a lantye \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ropitiations, through flattering homages\"\n",
      "ropitiations, through flattering homagest thes ensely s of the the mast eres on asll, the mand-tor delees of prie sfores of thous us lf the strurthoresbly sple-toresbly t aus nst mane solape bond stharef ores lontee of arluthuestened of thous and aty heves burs and and of trus sore bond tho e malust mad sore sore sole this exispestrabd as the valuat wemthe bale sourhe, the \"tmubss not st of tho stoud, tre valuatherton  sply and stherest\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ropitiations, through flattering homages\"\n",
      "ropitiations, through flattering homagest the dene ense stherting, te hood bopursolest dedisem, wring straasty ress land are cond streresknce all thore krcatoud sode, whing meduof whins st of whid struas be cast ted sfor isply ungense perhlatt reskroustfurelosb bpertouschert, whing, the veare boud sore told the mtous pry,ry red struat tist hateve of atie tous elyen it ledenot pestions! arlage on selarenot have athan pelopiskly lustheres\n",
      "Epoch 34/60\n",
      "2000/2000 [==============================] - 2s 916us/step - loss: 0.1885\n",
      "\n",
      "----- Generating text after Epoch: 33\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"e is placed, and whether the hopeful you\"\n",
      "e is placed, and whether the hopeful youe farly the struand sole tis sore can audematiof pally tor ve of prins \"o uphe, the dast eve perhils of itll elye pestions! are luessilf f maienof puis lust uss lose of ithe the dast tos of it heve it he are not pestions! ard luet of perhulestrear blee of arly the struand sole plite, the dast tor perhilesore of thit the arhe and arl beee of arly tor the struand sore the struand sore or elye tins e\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"e is placed, and whether the hopeful you\"\n",
      "e is placed, and whether the hopeful youe fam the last east eane of arlat ow phes and the struand sore ore sore sole the valuatit mest eves arl the stheves, the marearet ow perherruustreres spered the dane, the dast eves inct of whis sore or all the struand sore ondes of arl the mare, the dane, the mage, the dast re not pestions! are lues it he at all tor prestour the dasuof phiss mor prespored it resulle s and arhe sore or the reresper\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"e is placed, and whether the hopeful you\"\n",
      "e is placed, and whether the hopeful youl the gome sorr, wh it he eame on st ave quessinge pale of itih as-elue may us and and of brie marl. the dase on  the all the raple now pestien the deriplsthe are not pesplath, renseesofurellee nor porephe sole pricl, the stheve of itle toe mase chat iesole the mare. renot pe hallyes of a liat it es of itilesisime strial be of priel, the maouperus?all-the s, the qupestinst and and aigeoue phesocia\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"e is placed, and whether the hopeful you\"\n",
      "e is placed, and whether the hopeful youce, ava stoues ave now phe man the mor eowopus eure fimes oporell-t resuecins for om, the dagupestroussoubyely perhundy have ard ly icn merimors of the rer tomper real efoues arl buin of arluaty eeqe, the dusions-dere behe strably inde wome con plye womociche anl the relve cuasd toresocins, the dast ow all the candeenof sthe the damesuchesteld the dorusoce of arla hndestruand stherrerve strainq sr\n",
      "Epoch 35/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.0980\n",
      "\n",
      "----- Generating text after Epoch: 34\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" weak, the suffering, the oppressed, and\"\n",
      " weak, the suffering, the oppressed, and the roskingeny have byem concte phicist ave bure of arl buig somarly thand ard aid ow inly howeve it we weruples oust ave byen dalue in the arl arealust ard ard luat it we mere poss? womand a ou ope hive ard mane of the arl arealust of the lone of ailino it iesor and and in whises ions and ard it we destruans labe of whis soruve asmely the dand and and in whises in st al  the maru, we have byen d\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" weak, the suffering, the oppressed, and\"\n",
      " weak, the suffering, the oppressed, and the renod of ithe it we af the dast tor pheasole hald and lone, the quns, we have byem cons, werh whe proulonevenceangy lade arlous \"p. wust and the dosirlle trat all the dasis of ais ous merhuns, whing--lose of arl are wow por-the dosus of whine of aid tho docially in the ard ous of ithe it we desiol -tiresurstle the dans, that whins sole touinousply we hand anod whit of damabely ror poreloenspe\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" weak, the suffering, the oppressed, and\"\n",
      " weak, the suffering, the oppressed, and the roskfor teal. in whe esoupe that all the rtourt of for ion whis of ithe ins for to. phesorers of the dasined of arl ghinsof whing-rlay whin fuessimenting chand aid kamd ard \"orurore con ply heve byem and a maguof cans andithe renged domisole splatenfis maghe the can damand aty wowheres of tho .preskon whin fueitis le made incalofermor for to inbe now may us, the dasuof whins of aih aimamins a\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" weak, the suffering, the oppressed, and\"\n",
      " weak, the suffering, the oppressed, and the wosuond, the eediol stiertest farly row phes of inl ay in the atrathy red the lustiand beae nave been mamu. ins lf the athe of dainty lade ion sp.ring, ind in elyer whing--that the damandangy may \"s on sply, we hand and in weve armelvous! ard luot a cansounged, thans and it re ard now pheserurit lede as tou pabe the gorutot veny inst al of whidenow of ithe tor purosonev peacisf ihntetreans be\n",
      "Epoch 36/60\n",
      "2000/2000 [==============================] - 2s 999us/step - loss: 0.0926\n",
      "\n",
      "----- Generating text after Epoch: 35\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"f those lights and colors because the br\"\n",
      "f those lights and colors because the brhe sor pricl. the qusspeduinnt as lfbe mast eer plative of itid as tee pe plles be cans, the magupe plat for hest west af furtion f it ie arl a uag phess, the quesply t and as iof phiss, the quns and a piselye peatica sf itherrtre pore mor phins of arlat prespectives, the mastpe phest the upsrll-t or plyerusknce chat it we more pors, wow phesull the man , wh moreplathe spor, prath, the mage been m\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"f those lights and colors because the br\"\n",
      "f those lights and colors because the brouss ang whith resuices of suchen whing re oppemust ablu, the magupe can may un the lane for hopursol phins, the ussill the mors on t of pricl bups as an aulediof puls \"t we mane-for peations \"prall-t prall-t ursall-t ur ab. inge of asulust ablu, the mast tee peating, the magupe coct al the rork porelost and all a ue phics, tor pe hips burely huvenof pursll, t we mede of a ponow pheskous! as anger\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"f those lights and colors because the br\"\n",
      "f those lights and colors because the brous neve of as\"damade of amanl, trat furted the ssouched asmallyes of t it we, phe mans of sthest trdelven furillof whid sore pabd apdimandisky in as aurtle the lost evns, we more oppe have buall the malusbly t me tone splees, the cand and the mocist an tue now peaming the as tat may ho epores of this oursulle pprill--nlatitires, in ruin -splyeres of thais of sthar for porsplye bblue pprill-blle t\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"f those lights and colors because the br\"\n",
      "f those lights and colors because the brougnneang and and ald ge pe pple, ablug \"t we mo. phessous! bere, and the .opl es the mames mand ard mare of the stru, the quss an specmans, the mags. the qunssion furille nor us \"m bure of isplfeengy losf wemven the rans, the mans, that as an itherrevis of t merepurtle the lunededahes, we had mefiens, wo us, wor porelnbf cane arlilf-try lodtsule ffihll. tye pesporenf fition )splu, w ins for have \n",
      "Epoch 37/60\n",
      "2000/2000 [==============================] - 2s 821us/step - loss: 0.1515\n",
      "\n",
      "----- Generating text after Epoch: 36\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ure, that he has acted in everything wit\"\n",
      "ure, that he has acted in everything with struat of strall-thur the mast to ker all the mast of spliet on \"s and a mage it we the masupe plost sped, the desiche strat the dasusticestives, the mast tor phespores ouss demiches that ins labe cans and aud ouphe the mamand, the desist and thore pors, wow phes of this sorence asply sor the struand sore of atial by hass and now pe hive byicist as ly t as ly r and \"phe, the mast ten spertit sor\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ure, that he has acted in everything wit\"\n",
      "ure, that he has acted in everything with struat of striresur, whing oes oursulven isplres on splees, in surstreand cantithes burs and \"s? west uas now pees and now pe hive byruss magupe canst the dasusticed the desist ang now pe hives ing \"s and aly h we mest of phinstion ffiill--thatdall-ty lede of tho e mand and mome ons \"plaispedithest whst ald groust of whing renot on sulf the strual by cand at aint resustiof fom ion phes of strare\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ure, that he has acted in everything wit\"\n",
      "ure, that he has acted in everything with struas now peesolichelvat ans now ve aid mame of arl there phisple the mamand; porstonevfo. the masuped-thesrelven it resustidesimat ves\"ions \"pyel the desiche athat the uas nove pain furisa ttrew of prans and and oesperhall the mocat ones of such trut hald buinnd can ithe of a dogiouss of tr ureulodispores of the struand cons, wow whe ithe mouspeduthed, the desion sour, whing rean duns merecoas\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ure, that he has acted in everything wit\"\n",
      "ure, that he has acted in everything withfre valuatimesconst avd arris of weis s bein sprians lfitises\"ledore past af the strually s lant afateserma. funited the deverince fivis, the eeus to in all amuing edathous \"preus; wewhud sorat peemiand and itien furm, worl. luesheve ins st al the thut the or bucnt the fomesole ppas, undend, the heay hor the \"uspores bere prict, whe qusis, wist, rest ply llathad baluat as tor hive ow as bues rerv\n",
      "Epoch 38/60\n",
      "2000/2000 [==============================] - 2s 794us/step - loss: 0.0812\n",
      "\n",
      "----- Generating text after Epoch: 37\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"hich now belongs to our unconquerable \"f\"\n",
      "hich now belongs to our unconquerable \"foresoricor lor to inbe cand atiely ror porhid the moches bence of a line it re or tom valuating co the arl the moresor the vely the streand that ithe strat the maguockins and the vely the streally soretouestions! as and and in reard and the moresole platingrhuatheresken te more porlly tor and the rood sor the velye conced, the qunstenge conctelve porilly lor the mare on a logiol st af the woru, th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"hich now belongs to our unconquerable \"f\"\n",
      "hich now belongs to our unconquerable \"foresperitine fye of ins iow of aslly sore pratineveroust ave now peatiss and moruely hond, tor aphisone may ue the wopuatly scoangy hith of and inod some tore oon and aid of pris ll.ty ins \"t testelyes bean the lost tom candithe of athe tor plost toresore cantieges berhalt the sore tomino, the velye porill.ty ins \"t the evee porlly hor to the womably sorenod may of ithhereskenide incaly the mame a\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"hich now belongs to our unconquerable \"f\"\n",
      "hich now belongs to our unconquerable \"fay un the mandy sones of thoue ond the eospee perhlle trat ted ins lf the sorustne, spestemor phisinons may unst westires ablunttery hentien ffaill, tor alle y rulnos dfirl -thatd, the struon ss\"rtins, that ie now of the re ionly hay us to us, tot pestion purs\"ligencibevereans and and inqe hate of sthis, blatiouspocith bere, the qups ins meresxespmatly scon whinh stores sprat, and the dosustingt o\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"hich now belongs to our unconquerable \"f\"\n",
      "hich now belongs to our unconquerable \"f mentieny roaitl. a deno it it we wosl beng st ard buhet that itios strial bricinnollyer and aidimay of it meguall tre sthere, thengee of \"hid cinctimet of wall bungy ay ungrudestoainnt the aroue of it womans, the qusssely phatiag bas loffirle-tor lofious, we have the made once canstiols p.idl, the \"treally tomich, the guod donist for heat aed and the asole that inkenod have the roskall. sung, wo \n",
      "Epoch 39/60\n",
      "2000/2000 [==============================] - 2s 969us/step - loss: 0.1119\n",
      "\n",
      "----- Generating text after Epoch: 38\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ation has heretofore set the most danger\"\n",
      "ation has heretofore set the most dangery we hast of for tom mand and more of phis of dema soness ang we mereporoporole for tomand, and conspeduted, the quns, worher tims and the struand of de phisone mand ard mogienof maston the sores of cand the most tor mone of aill, tre mane of arly tor the sore sole of thus moruporely that all the strually rorktomand ain of pris, worl. bues of sarl, trre behe soresone mally that al  oo prears, the \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ation has heretofore set the most danger\"\n",
      "ation has heretofore set the most dangeroubly that al the wood proconof as marlue mand ard of pris on phis, worl. the quss,elnte cons, whe quss, we hald and buegous! ard \"poris of it wist ores of tho eroups lane of a lane phis, worl. the qusspean dfouestrall the mast of prociduledule of bein , whing--the athe sore of arl boun what all, toruve ,iml the velat one of pall bungenge cand and may hore or phations of thour ons! ard mane of pri\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ation has heretofore set the most danger\"\n",
      "ation has heretofore set the most dangery we has of arluatires of miche skinsolly that ale now sas nore ors ang noghe spor, whinhereskrous! insaly t pe mace aimat ofspeand and of phis lostruatly splate bries, whing ed of pris, worlve aid of prisolf hivaly uand aty loge of prlats, ind whit mat pespios \"n aulans dors onst . strals tor bule of ainat of pelues of asll. sore ope havaly titirenof is \"porenor purally rean bueste destheresor, w\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ation has heretofore set the most danger\"\n",
      "ation has heretofore set the most dangeroubly tere of afly ounsousperfiles\"ly tith ors and the manesoly tithnot pes\"icat \"d of prissol this sorerreab, nor pospus, wiwh firesoren aus now hesed ins \"t eestionst ars on phenctenbt cocons af aidimare of a mat peamouasolut ovenoy havanbtedathe arlat ow plas ow phicannof dame oce ma iterecians lven icimerelyes of this sormar ficis, werhu. cans and aid ore maos and all gouist rensely s desering\n",
      "Epoch 40/60\n",
      "2000/2000 [==============================] - 2s 928us/step - loss: 0.0905\n",
      "\n",
      "----- Generating text after Epoch: 39\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"the desirability of an end, does not exi\"\n",
      "the desirability of an end, does not exis lereskem and \"the arly tor vely inde of pers, and we ie spri, the mare, the dast tor plle oursole sprat mand aid of prising st ave yere more of the are of itle toresusonca dforespores aprearst evee or arl a wigh resus,eves, the struand the sorespeatises ie saluteuss and whitherestror platives, the mas soruperitheses of the strual byer the dast of the sore pres, worl. the masu, the struand sore o\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"the desirability of an end, does not exi\"\n",
      "the desirability of an end, does not exisplatesusiand the sore pratllees ef the strual byeireand the strua st tist ar touthe. the lustit sor platinat of pris on the strual byeirellee pratisuledulesf af all the stru, the dasusofuvely terhulestralg-tlat inselye ore porell; whing--theret of isplateves, tire or phes ous ions, whis ouss af the dasuof whis sore beecoes mars ard megiod sorespores lfbe cast the yere pordelve purslly the qupesti\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"the desirability of an end, does not exi\"\n",
      "the desirability of an end, does not exisplateskeme favleatesersplee phesple that aluateves, the dastyelve phicistititesole splf the struand sores of phis ou spuree inn the docually suresof chas \"his moruve dunsely tith ard the dast of the strave of itid dins mere of purst, tre quesplate, thendesis le the dame, the \"tpuacl, the qupestions! are may unst west of es\"ion furill. the dusudedundes of sarl, trit sore behe the quphelve now pees\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"the desirability of an end, does not exi\"\n",
      "the desirability of an end, does not exis, and armallst ungeseestruan fueimat valn the lue phesle. tee minsedy has now ves,elndes? as toreveryel, toreverouss ang we merepertion the as, lofes strars lfte vely moy phestour the ursoluss and \"phisaned it welle foritomand a de in st abe fir ole hor toudheen ely wow od spores of whidesors labe fir tomave bihh sore prasllee whing--thend, the daskery whit formeesthe meriel, the qupestion sure w\n",
      "Epoch 41/60\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.0785\n",
      "\n",
      "----- Generating text after Epoch: 40\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ng to which language designates the ston\"\n",
      "ng to which language designates the stondas ou phatially r whine-for plathat the sore pores, tirh ard ouenouln aspalunted of arl buin st all the mare on st ain of ail armall-truand straal berhions! are now peat as tore bale for tomand a cant and and of strea soned of itid bure of arl buig somarly the qupes lore or all the mast and the mores ous \"pre bure byee cand and aige of prl. the mast en the and arlay whin  and and aig oubhe arlath\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ng to which language designates the ston\"\n",
      "ng to which language designates the ston asplatines apr aly h we mass lore or all the dast son as aly huve arl the most elnencaint an aubat we mand arl the sore soraplatiseris \"m all the dost the dastit an the lone solacoub, whe qups lust uas lust ais of prist, and have ard the dost tingengy hast and ard ouns and arl of this sore balute, the valuatis \"ayule for tome ope hally the mast, we had naw peat it he ably real bere mand and ainam\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ng to which language designates the ston\"\n",
      "ng to which language designates the ston, sprall-the the dast and to prrall tre valub unde of atl, the quphilvat all the mast enge conb, we hally sore bocating co phate, the valuat as now vand con stmarenore of a logious \"plans, we morepne, whing resulues abluafow heve pratid of ith es that is lore bally \"par, whing-realy r all now maselnde haf forme inn the lode on st ave armall,e 3rat tee phest an tue phiss lore conc!ely in nt, whe so\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ng to which language designates the ston\"\n",
      "ng to which language designates the ston asplatise isplabeverstrar now pe hive arl aly huve itleatial byer aly huve bele platlably rabl bure phatitens may in the asd lame arl, terhulyst alke ungelye con sthe ard arme rous for to the womuat venyer aplatiof pracl bere bald the mocistea dons heves, the struand sore proclat and and inqe ood phistit an af.atined it weple the quplly \"s. wuih rs.ly spratenowspdacande phly \"pself-this meru, the\n",
      "Epoch 42/60\n",
      "2000/2000 [==============================] - 2s 817us/step - loss: 0.0542\n",
      "\n",
      "----- Generating text after Epoch: 41\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ust pay more attention than to the great\"\n",
      "ust pay more attention than to the great of prinot of ithe ropursole ruastillestourenore of arl are ithe eestruant loso it re arl are forme the maguod sonate or to the ithe ard are ofopestions! ard ale he ee ore oursulve  ins lust ais of ithe it we mere or pore of arl are ithe areayey ing heve arealest whe qunstit es all the mest tire ore of pricl bore ore ouss and oue of ithe ithe resuroledy ind strial for toutherestrous! are oue may o\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ust pay more attention than to the great\"\n",
      "ust pay more attention than to the great ow phist of vely huve arlat the .e ins lore ors ourenore of a mag owe hast and the more or phinesustourenore pailly rald strear tre mame and aid of ithe inse of arly tor ins of arl the moresore oociol buin of as. wurh ard are ofopermow phes of struresore of arl are sore pore of trut the .e iss lore ors labe ores aprainf we mest heve arealestralnes of trus sore of arl the most tor , whinh re irs l\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ust pay more attention than to the great\"\n",
      "ust pay more attention than to the great he of ithe it te iest ies sor purdall-toureneespeatistrat the yecuatine for tofined aid oforesporespores \"ped is, wor ves, the dast titherrulesurendes of the it he are of tru very whit he oe penouly splatever-the resuroudediole hinsteveryelle formare ore ind the mareeare ofopleesofine alle te have areales sore whind ofedrous, worh re vesuions dereestourenorepor, wher the .e iss lore or and arl it\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ust pay more attention than to the great\"\n",
      "ust pay more attention than to the great the yesiel the rruand sore ore hire ore ore \"ple, , whith resuestiheselyes not ples of the .udesien es all the desiresuleen atial buices aid of ithe rustruate dorl.-ttechall-t ue \"spereesphios, the dastoue forru.tinge oo sporet the eest tre ,owusomence aistouselyehy ions and tre hore the .ope hope sore orrelveundes of ithe itinh es efyehe the armatevirolosour, we hour-nor ones of timisous tee mam\n",
      "Epoch 43/60\n",
      "2000/2000 [==============================] - 2s 914us/step - loss: 0.1114\n",
      "\n",
      "----- Generating text after Epoch: 42\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ure forward, whereas knowledge of the tr\"\n",
      "ure forward, whereas knowledge of the tre sore pratl. the dast tone fore onbe phatious \"peciand asiing phatious \"peciand as int whas bues aplate bere phas lunting co phioosoperhinst vas and as inat of puss\"lf t magh be cantithest ungis, we have byen dans and a wima of a magh be sant whand a ting ee hale tor puls lf-th it we mably sor pricl, tor all sor purslly t we despocame the maguon s and and as tor puis \"peimat valube con pempechist\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ure forward, whereas knowledge of the tr\"\n",
      "ure forward, whereas knowledge of the tre \"speandith, tor as tor now pees of the wopurs, whing of as ing whing-rtall;e ow and a wimand a daguoce plattill--tlatly s meghe the sore platinby heve ow plathy rcans lustrerest that all to is dall buin we, whing of a lantience of a lans a lanty heve of prins, wo mpre, we mand and as? meghe es to uphess, the quss heve ard of phis lust it tentis of purslly t we desphers, the quss heve of asuin th\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ure forward, whereas knowledge of the tr\"\n",
      "ure forward, whereas knowledge of the tre \"speandathues apr, whing elyes lfot as inat sely thing blast west wess lfee pessilf for to the docuas lust ald and cuns, whing of thitesors mand aty heves of the .odaply tor valuat what as to doss, whis lofevall the vely s of the \"spean ithe ot a magh recuasaledy haven buis lf this enre prathe .ulust avll.e the mars. cuns, wo le that the gocatsply en the lost alnon  a doguions an aidamanismoust \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ure forward, whereas knowledge of the tr\"\n",
      "ure forward, whereas knowledge of the tre qunstives, the sabe for to phy ing \"od inorenod dy innof whint, the struand tho e mand tod ofopertof quesply te have bues, we madtous \"wrtw, whing ald so. , the struand and aid ofope mow phe oonca cans, the ouspedhinntenof whine for tom valuating for have yores oo phy aint at aly sole platill t it mage berasof thitherrevns as thithest wians any huves of buid sor and ard \"reably s of the us! heve\n",
      "Epoch 44/60\n",
      "2000/2000 [==============================] - 2s 878us/step - loss: 0.0973\n",
      "\n",
      "----- Generating text after Epoch: 43\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" declined in proportion as she has incre\"\n",
      " declined in proportion as she has increard ay tore or poreludedusedededithes and the dast eand and and ay io pharluest ave ard and of prins lore of phins and arle the \"splye whing evee prespor thing ord and the struat for how phes of phis lust ard and of pris ll. the dasupedute donige of itl it wh the dest tor bly huves.es have by hiss and arle ot st ave bere son aruably rean it? westrures of struresore plathe stor soris, we have been \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" declined in proportion as she has incre\"\n",
      " declined in proportion as she has increard ay tore or pord and soresplatlve cand and the dast enge of arl the re pesplathat the lost eans a dand ard and sore praslly the dast tor proulendat ins lf r mand and and of arly sores of solain we desplyelve concly and sore of phins and stherestrous the dastouss ang the destruand sore of phins ant moresore mag nde pesplathard ard of pris ll. thand ard linged the moche phinstion stherestroust of\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" declined in proportion as she has incre\"\n",
      " declined in proportion as she has increard of priol soucem, whing -thes morhers on the eosued inn last east eves be cons at al the desis unge or phint he horesore pratly row phis of iss lore orsy have arleatedesions \"prealy r ind sole hand and arl  or tom vel. to it reall thes toruped, the dast tor pres ons dure of itl desuseduig of pris, werhuresor phint at ous persalldes of the touthed the \"podison asd loge of phins an the morspedith\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" declined in proportion as she has incre\"\n",
      " declined in proportion as she has increard of the sprhel pessill--thard reduiol, thand ard of pris d burgreard the dost the funeitoresore oporelureleat and the respertiwedioo st ave firloly horeus-there nste.y ins lfbe ssounst and and afdithe mand and torumeduon ss lere not sore cant and aid ofoplert. re destruant soef ly hfienol sthire, thing the -thas bend sor pristous lf rs, on es, ind s. in dast evef ars lnee ors ly huvendes of the\n",
      "Epoch 45/60\n",
      "2000/2000 [==============================] - 2s 854us/step - loss: 0.0641\n",
      "\n",
      "----- Generating text after Epoch: 44\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"--deceptions?\"--that they please--him wh\"\n",
      "--deceptions?\"--that they please--him whing re ofor and the soca the mand and the sore platinat of pring whing and the sorisnol pry whing re bard ard \"w the dast tod or phins ant womedise, the qups have brins, the dast the qups, wo har the sole pratl, the dast the dast enge of arl the strual by ind sore phiss, whe qups inqt eves and the modistrang-that ithens the mane, that ithes sorrally rore or phins, whing reduably sore porill-turema\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"--deceptions?\"--that they please--him wh\"\n",
      "--deceptions?\"--that they please--him whing -the domas pe mins st wemt the dand and the sores of the struand the struand and may hind and the sore poatilatforsply teat it we whing re uspeduttien es and the dosisoue of this sore or a of the strably s of spriat for histous \"prsulf-that tims and and of phis mor phins an t at wiml so uphicat of it whith prist, the quss ing coaly ind have ard a dame inndithe dast to der peatide in te as inat\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"--deceptions?\"--that they please--him wh\"\n",
      "--deceptions?\"--that they please--him whing -the desiriskly tide or phins of thit tis sore ond the stoursply leaticially splat ow phint, the hape tha duinne phaille thing the cand the dast the cial it re mabuto phecially ror plati, sore porelly ror to, phin of arl a we in the sore or the struand sort res lor plecoubed asking-rean ithe at al  tot mand stof the struars lot pe manly sore sour, whinhere, reas bund and the some splatl, the s\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"--deceptions?\"--that they please--him wh\"\n",
      "--deceptions?\"--that they please--him whing -the de is lade it we un euye it we mantie phaid and now phes and \"wroply roaly hinemamint, and the sorich, and the sore sorigingaley raintire foriit we mand-awore of arl  ungis, the qups innt  cons at ar the dediof pris more of a caf ore pratl, snor for es the dalu. in puatious and that tims and ciats, whing heve ore not phics of trie sung, and the mas pesply to is doricons\"d yhe \"sply trand \n",
      "Epoch 46/60\n",
      "2000/2000 [==============================] - 2s 848us/step - loss: 0.0697 1s - \n",
      "\n",
      "----- Generating text after Epoch: 45\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"eful and fair to banish the anti-semitic\"\n",
      "eful and fair to banish the anti-semitich mame and arl arouly; ins lf eves aplate arm armal quphicat ve of arl buin st avl the rocuating co phare, the quphelf-trours\"ly real camill--thard ard mage of arl arouns? whis of mamantor plative mamug of wemars, the qupes and arl armalestruant sor and arl armallyes of the vely phis of pris on phiss laneveryelve phiss lofures, we madt arl burgle the mast enge of arl arours loge of and ard ofor pl\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"eful and fair to banish the anti-semitic\"\n",
      "eful and fair to banish the anti-semitich ctat as the armouspenot have arligosorerearlubbely s of the valuaton as ang we, whe qupestions! whisk of es of the very whnd sor porsply ror pordadedirion ss? wems ungerea labuply rear unge magupe cor pors lode of a lonow phes mor the mamallof suched stherestrous! as? womus ins dubure chaid arm .owustruast lose fims and straaly nod mame and the dame ins lfous \"wicantirs of a uogucols buch damust\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"eful and fair to banish the anti-semitic\"\n",
      "eful and fair to banish the anti-semitich the s of domarungy heves, tither pursllbe cuns, whe quession f mams and \"ly hive of pelle sung, whing of arligockint, the mactof such demirs lanevaluct of famly moguhe aspeatingreforlans and whing opd mame and the dosustiness ang gond a cag phesous \"platingesiens aflast\"ure byeims labe for being selves babulby whins athe rtructing somarlleb reantithe. strorteskens demiches burs lf-tr\" the dast o\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"eful and fair to banish the anti-semitic\"\n",
      "eful and fair to banish the anti-semitich memasupe of amporeally gond arm ore plodosunevas, wow ve mime bences of strratesors magunden dich reve the mamay of prins marl aketitisons. and cins it he armall-tmurhuledf all the mast last ard ard mave arlat of mass, and cand all the .orisoren as ans merhears bere of priolns of phis of which rewherrsprat valube doguocist ald cocke sporespreat ffy histheresons mave  3nctestimas be cakt afd stru\n",
      "Epoch 47/60\n",
      "2000/2000 [==============================] - 2s 863us/step - loss: 0.0937\n",
      "\n",
      "----- Generating text after Epoch: 46\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"tion and exaggeration. he is only genuin\"\n",
      "tion and exaggeration. he is only genuint selve prespectith, whing of whis sor whing-rtall-tous, ing he dosusticat \"d dime of it womans and cunstions! as andithestes sore balu the masuof ss iegh reskncat ve of pur, wow plesus ing whint, the struat whing st and sore pristes and and it he dest and the most ting--theret of phiss loge of and a mage arly tor poss lude of inl sour, whin  phis lf ihl buig ciatillees and and of priel, phin st a\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"tion and exaggeration. he is only genuin\"\n",
      "tion and exaggeration. he is only genuint selve whing stol sprites mat pe mans an is? wotiss it we made of ithe the sust of the wopus in duly heve of phiss loge castly t velue phiss lowever-tlatins dogioling  haitterest time of ail the mast of whoursall-the rocuatinedfof cans af whinh re proules yere fofiting--tiat in the lost ting of a mage arly to mars on s ours and are \"s tor vely now has forlee porsully s of ermell;  prall, that in \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"tion and exaggeration. he is only genuin\"\n",
      "tion and exaggeration. he is only genuing real  andathe rened anst ow wh st usst ave lye peatid ,aply realy a degiog huvendy is! lose of as and streat the qape, tre \"speatiaat ve of puid of phis of de us? wethe arly horeporely whit madives apld, whin fueimat of denipf the struat ffaill--forhies\"ding, the struas not mas pestid dfoussllf that it he dosuself that the strually sprat sustined of it wo us and and it, weth pres of sarustitited\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"tion and exaggeration. he is only genuin\"\n",
      "tion and exaggeration. he is only genuino s antit, whithes do us, werhud mand sor plisen whing, the dast endesocict, the dusicist of whing stharesored ave -the desikingy have .y histnf the or as and the dosust us! \"s enst destrably pirilustives read the sthuas lfee of a lagely cins at it we hevery lld no, perhiply whing st last fies of and it he are porill, toriskol  splativer-tle whis \"ply real  pritins dy icitherttens not mans ang us,\n",
      "Epoch 48/60\n",
      "2000/2000 [==============================] - 2s 869us/step - loss: 0.0484\n",
      "\n",
      "----- Generating text after Epoch: 47\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" by means of which a much simpler life, \"\n",
      " by means of which a much simpler life, and the sores of that all, tor pursplate, the dast eand arouanow phiss, the dast eand able of arl the rosken as anat of pris, for hodasoperheas lore of phiss, wow may pe hips burs of arl a of it we the ssorenod sspeatith struating co the arluably have byemisten the lost of hossoustrar torupor, we have arle ous! arl bure of ar al. the most ton aspeatitesoffitianty aed arl the rosken as anttid, and \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" by means of which a much simpler life, \"\n",
      " by means of which a much simpler life, and cons and ard aid may on phins and aly inde whis sor-the rtru son strially roanting of a lage it we hadd are now perhapst mage bast of prould the sore soring-rlabey the mastous nor have are none of a logiocssulf and ain of this sorman -that the dastousthe dostrear bloe porisong) ly that the arluant heve asluons af at.eithesrelveny huvely thing--rlatiof can it? whnt re of arialy rores of denithe\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" by means of which a much simpler life, \"\n",
      " by means of which a much simpler life, and conspestions \"w with beesson whis sfiresof-this treng the sphess beg pestios of ithrreanging eskely phicl, thind ard sore pore, the qusspeatial foritom sorirenor phihiof armarenor pors of splr, w whins forleeduned of  innol straingon as andinf at.ea desiciat the dast ons! byehist ang ow the ard of pris on phins of arualustrave bure mone phailfor the sorespearl, tre danest and the most ton phas\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" by means of which a much simpler life, \"\n",
      " by means of which a much simpler life, and cons; whithes ansteare sock thing or the armally strablf-the sore porsulf-thailf bche sore sourstly reals- fare arde anetouesorinn of amiaitorrhubelf the arnter-tlr the mast ow and the rt when esprout tes lontienca comsplyely splatins splate pristen the atle and arly ror por-the uoss have blest west ans now phessouchat mas phardartnot peshivas bere sthe arstof the spresknnens ins \"t te may phi\n",
      "Epoch 49/60\n",
      "2000/2000 [==============================] - 2s 824us/step - loss: 0.0422\n",
      "\n",
      "----- Generating text after Epoch: 48\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"demands satisfaction of the law, he is, \"\n",
      "demands satisfaction of the law, he is, that itist ons and arl a pe puiss, whind are now pe mins ans and arl arl arl a pe hips spriel, thir all, torithe sphr, that ithe st unst the dasuped-the destions! as and st ailf burs lf-the dosperiting, that itist hese ins \"t eere phissll-turs and spriel, that all the sore of arl a uiss here prsting-sllyelven as amperfirs,llet pearly spor, the mast enge pasill-tursall-tuperfinstit es pprill-trabl \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"demands satisfaction of the law, he is, \"\n",
      "demands satisfaction of the law, he is, the mass and now oo spliet, the mast the \"splathals tor the valuating sporithe rephessoungenst and the resperithf that all the sores and and aih of ihl ee thore phait of \"splatenbdecially spoctt, whind strably t aintorenor as an as apratind straing--thatithesenof cand and aih of phisist and splieesperkill burmall-thaid arl the ropuasole that all the rtrual spor, we mede phiss lust pestime ffaill, \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"demands satisfaction of the law, he is, \"\n",
      "demands satisfaction of the law, he is, that ithendess havt ard ave aspeatials that allateublueandarlyerenct ablue that it wand ard the some ays ing peail, that all the most of cand that itith hide ins anqe soratiof cand ave \"splat ow phics, the maruelye peaill the some sprall-tuphe, that it may phatistruand that all the dost tenrath, insteericoss\"lustice fpaid mand at it whins and the arselvenses of thire froidimond dasinot pes\"ill-tup\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"demands satisfaction of the law, he is, \"\n",
      "demands satisfaction of the law, he is, wor us.ely huvel--nlatithesrondatiof candithe dasions! is apla, whind ar dimare ins all the stor denigmall, thand as\"ion nois; ave nse phe pe poall. that all the rrutht-end their ow phicl, thit as ain fow aid domare fonl, thithen dspeaklies of splies, teas ins dube of aplathore ors ange -thiis of pricl,-trailf forleat ow phiss \"p specit lvis ub apl gy iesply that itheas-trand sthe at of arl a pes\"\n",
      "Epoch 50/60\n",
      "2000/2000 [==============================] - 2s 867us/step - loss: 0.1211\n",
      "\n",
      "----- Generating text after Epoch: 49\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"nge and weak, suppression, severity, obt\"\n",
      "nge and weak, suppression, severity, obt cand a de ions an as and arl are oue pely that all the struans luatis mard are now phes of phis \"pre, the magupe plating, whe hald are for howe of arl the sores ablut the dasuof spliee, trat we mand arl the soos phict wow pestives, the strually splate prielly splate bris of arl now pus!lly t aeg ow sole plistof sple, we hate arle the strual burg of straies of strait rerulotesole that all the stru\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"nge and weak, suppression, severity, obt\"\n",
      "nge and weak, suppression, severity, obt cand a cand a de ions an aphait re kosues ing our ang whin -trat \"s lfbe cant lee of phiss, whin  of arl a we he arluat it ie are the strual burll tor sore porely the valuaticat the easuon s our perhe be goou sole perhearl bred strat we has now pe have arl the reskrlate, the dasupe for tomanle coas lue may pe huas bere sthe ard sore pradl, the qussplat ca uonce chas thing resues de icat thit of a\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"nge and weak, suppression, severity, obt\"\n",
      "nge and weak, suppression, severity, obt peshilf-trver, the mare, the dosuphe sco. it we have asd mage preas nowhese is lfatlves and the stous sore cons af the as tout we mase sfore forle the roul the wocl. the ques of the stounged ihntiely rea loncee pasdl, the quss lege cadi.at of pricl bele sall the are yore sore porill. the quss lede prstlert wess ang cons lust ave bere conce not pees af the ard arh now phis \"pre, the magupest whing\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"nge and weak, suppression, severity, obt\"\n",
      "nge and weak, suppression, severity, obt cand-anodaluay have are not pe haply s of dosusole of ibli, west orst eveusond mace are fofe of phicisolut we cane of the the dast ef the sore sole p.rs benge ore platha stour lost eecoas \"s it re uestitotesplyenct womans and the stous of cund the stoually s lane-sporet the .r ancinoas easd of esplee phiclly now hesed sore of this  or heae buen st aed grouano r al the struat firisoubbe degicainn \n",
      "Epoch 51/60\n",
      "2000/2000 [==============================] - 2s 810us/step - loss: 0.0573\n",
      "\n",
      "----- Generating text after Epoch: 50\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"erious pathos could never have arisen, t\"\n",
      "erious pathos could never have arisen, the dast tor as tore bal  of athaly a ange of phiss \"pre prould and athearenot phat pars lone sole plitins amatinged the \"splate, the damand and and ain of buig mand and mor es the mastor and and ain of bure byes mand arl arl arl arl arl the sthe ard of prispenow phes bere or the ar toe prout me have ard and stratespestivenof is \"t the mast ende of phins and arl are now phessoursteand ay tore bal  \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"erious pathos could never have arisen, t\"\n",
      "erious pathos could never have arisen, the dast tor valuatinedfof cans and and ain mame beig mand and the sthe arlt the manes of sore been esploctat all the eves insthe asd maneves and bungee perholust and and ain of buig \"w the dalue phint the dast toresrouble for to the asmalest of phint the proulns mave bues\"on and and ain of buin may phest and sorrspertive bungien we mery pe have arl the stousthe athat that the manes of the strually\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"erious pathos could never have arisen, t\"\n",
      "erious pathos could never have arisen, thit may now phes af the ease of arl arl ar aly soue the .osably and the dast tor prouss\"d the east has forlee the mast.e for toues mably somatiof phand arl the daness ang now pestions! ars lane, that the magutet of have all the dost ting, the mastor a laghud mand afodesions! ary whit of psoll prist, and and ain of; the masseendes of the athe and the sthe armely-the and sthe atheangenge con pedithe\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"erious pathos could never have arisen, t\"\n",
      "erious pathos could never have arisen, thit the eassole p ihl-tenselve conctims, and the sthe ar the loop; atly acat of qups may phestours\"ly reat dentand a somaness ande brign ply whit mat plespor-the most that the armall.y have byecs to be splate pricl by mand and aig of phicl, thithfurhand bhen the kand and and the stoust ow perhaps, tom ,sth the bort on  nor pussulf the asd of pris be hodason phinss and whins of sthurenot of arl arl\n",
      "Epoch 52/60\n",
      "2000/2000 [==============================] - 2s 829us/step - loss: 0.0504\n",
      "\n",
      "----- Generating text after Epoch: 51\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"oses in which the quality of badness is \"\n",
      "oses in which the quality of badness is lf the aslue more prould beresof the as lof phessoust as lfar the arluat as nor peshilosophars, we has now pess ing of dalue now phest ang of sthereskrlabey re ass loffirll tor spores bein not pass of struat of phioss abluat and a pe oppessole tous! we hast ablutemarlay pharly sole to at ais mard are now phessourtears bege desphians able the desion st aff arl a of itluest and the some of the arly \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"oses in which the quality of badness is \"\n",
      "oses in which the quality of badness is lf the asluathe athat there not perhips lofby hest af the losperous \"ply and ay hay now pessolughust ans and aty iens af the at aly  oply roguouledusoffyrions \"piss loffirs, tow ve lonst ans an dugicat tome have by hest af \"phessoust as lfor cantienof cans ally hore bole platlvaren the atluatouss ang cons it whist west ably rore bahaty it lf the arsulosophyract and \"poresoresprato firely the arlou\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"oses in which the quality of badness is \"\n",
      "oses in which the quality of badness is nor peshiof that all to bereat the arouans afwe oos. beres abluy whit firsolaf perhous! as torens lf the arloof \"hly soof the as uogrhe speaf as. luethe atlabeforesoctidespritot fof ithe rous \"d aplienof phiss loghicat and  or howave be hest ably a henet ops \"ples of sthe at afs bean -tpais \"w may peshivesulus late arsulf for tom phiss on splret thesy consorngrs,babl  toriskon as ang cons af whis \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"oses in which the quality of badness is \"\n",
      "oses in which the quality of badness is lf rtie foratomaimally splat ofs? wosus merher ablut thes, us! hatd arlorenow phessour terhur now phess ang for ions ions \"dithe athavenor hasd beingnod ssours of whis sfiresorus, thd sably raint solf the stoursole trat .erhios beve struas lfation \"hy it the a may of spre, trat reloesprathat the aillorelocof mpheare blhe ot arlue more or aplatho sphatitesof itid mamion phy re sorentithe to ay uade\n",
      "Epoch 53/60\n",
      "2000/2000 [==============================] - 2s 850us/step - loss: 0.0784\n",
      "\n",
      "----- Generating text after Epoch: 52\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" has from all time drawn together only s\"\n",
      " has from all time drawn together only s of whing concianly solf the dasuon s ane for ions huve burill buin may of sphiranly and sore prath ard arl the sore prath arle oon phicisus moy phes ans and conbeen en it we we phatiof prans, the masu, the mare, the mame anod \"speand co cor phicosor phis now phes of it de ins lue may phy ing plade is lfat as the dast tenhe concion st ave  arl the dast ted of prisons may us en esperfine for tomanl\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" has from all time drawn together only s\"\n",
      " has from all time drawn together only s of prising chat inte of arly hor toue of phis mame is lfat sarly achams, the masu, the mare, the dame on st ave now phes of phis, wo hor tout desphers, and conspeduthe prous \"d a cand aco comidiof phessousperfof thes ense of priol phicatimat of cand and sore prathy rad saming, and cons a pe hive itree ins lue phespermal the vely phir of phisinge ins if whid sorisocariloespermally sprath, in st wh\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" has from all time drawn together only s\"\n",
      " has from all time drawn together only s of whing sarly the mame anod \"splat of prisonos an duned of prison- mor pursllee phasd more of a logru, the maruelye  incl -that is lf vally soness ane chaid arsolust and concorencat valust dvemor purel.e pradisf it lede proclat as not pessilf that all the  ore porelf phirl, tring -there, the proule phicoss\"aludf it le whand akl the rore phyerly soreconcainn, whind re us! mor phas , phing co by i\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" has from all time drawn together only s\"\n",
      " has from all time drawn together only s of whis sfror, phihisofupheatitisnle farl,et whine formeenow pyamd acat whe quesole solf thing of it maghert whnnen aucald conced, when stourenot phes and now phes in sper ave now pees of in welues af amiant of as ing whind st and \"y a lant heve bues of sr forme inn itidedy ince for hopuably rualy h is lert vesuinndest and con .y phally rope sourher ousned more porill-to krlais of pe pesplath, th\n",
      "Epoch 54/60\n",
      "2000/2000 [==============================] - 2s 845us/step - loss: 0.0267\n",
      "\n",
      "----- Generating text after Epoch: 53\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ides, what is the burning alive of one i\"\n",
      "ides, what is the burning alive of one ins lore or all the dast unge of a laghur surt wemtrears lone mamandest and a mage it we hene of buin of a loguouss and now oesperial bere phailly that all the mas of pris on phicosouspuace inselfevs labuen it at and and aid ofor the masuof that itheseno splate is lfbe cos apl ithes burs lofforesocianly r aicative aid mor esco ciastions! as and the strually sorenod sphistof the strually sores of th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ides, what is the burning alive of one i\"\n",
      "ides, what is the burning alive of one ins luneduspeand as ionow ins af the ersolest and their the mast and gonosally have by mast ave burs lonevery conterhaps and the sores of this sorcan furioust avd armall; sure for ionoply realy r aicanow phicosimat is \"f whing esperhall, phiresor phis mor heve proplatives, ins lust ave nowhesesproresur, whit resorencial  3abd are mane of a loguphicatiof cand apligouspeduthe prould and cor porsplat \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ides, what is the burning alive of one i\"\n",
      "ides, what is the burning alive of one ind and moyes of this sorct tens of st ars lore or a conow phessomat ale the mast, the mast and a cons dubus inst whis of cain nor poscoditirenof is lortienchall, that inoness and \"prion for inst whe prould and conisimat firlly torive incialy r all gopurouss af the destruand aod it ie sprreand as, wow phes machebtis now of phirlousstmare yriol be hass afdest rast for phespermall the dast tithf is l\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ides, what is the burning alive of one i\"\n",
      "ides, what is the burning alive of one ind the masuon s ane for it wo upre, the qusspeduably ror have aill, the grougnor socace ofideminot the ascaaly aclalist eree bone manl, t whing can aid meritis of the lose madl, thind all the strually soros sorimorencat as the desper, whing reaked sthe at mar pospoceas it splat valuatins sfare for tomes it af all the dastours\"ly thand asoly hive behe cande for gond the dosus and \"s more of a uonow\n",
      "Epoch 55/60\n",
      "2000/2000 [==============================] - 2s 848us/step - loss: 0.0770\n",
      "\n",
      "----- Generating text after Epoch: 54\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" has not the time leisure? oh, ye demons\"\n",
      " has not the time leisure? oh, ye demons, whend stoues afod ins lue of or plations a lantience conce cans a le icissille sore prath, the sust hese is lf the stoues and the stenst of perhulespore fomitinge is\"ly the sush west of the soues aforenow phes bues io spores bein st and straale foritom phiss lose haid all the stours of the evee prisone for ion phessouchertous! as the desion st ave are now phest worlee ons \"s, wow oes haid all th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" has not the time leisure? oh, ye demons\"\n",
      " has not the time leisure? oh, ye demons, when the it we wend ale the stours and \"speatiof whins of arl buen st and striale for tomes donope cantitores concit whing espeaf the athe stourstous lofe haid and sore priston a lighe the struably sore proclat ve cocio dubuen it iesoren all the stours of the ard are oope hope cor tomanly have been st af the mast eeduphins, the struas bure of phissol buig cons, whinh sorespecially sorespored abl\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" has not the time leisure? oh, ye demons\"\n",
      " has not the time leisure? oh, ye demons the sore porelle nor heatiely hore beme hole toue of the are boog sore procld the masuped-the desphicat vand strea terisunges of colue us re evee irs lofe hinst tow pe have been st af the arhe bele to is, the masuple prast, werh ars bungeene sorathe rerespeco sporee phisss\"lf tre stours of the es.e phastingt ais on s abe suseris of the ard area one porelly radt area of abue wothe the \"speaned the\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" has not the time leisure? oh, ye demons\"\n",
      " has not the time leisure? oh, ye demons mor the yesenodus, and the ate hope hoper oust af all the stous now pes ios perhion bubl dustrabl t ow ins ef eheesoues af the stoues of dolus rung even eusorencees of phiss lose is la that ins beve dest weslly sore pore of phico, the ars of the fore hably the strual bere persilysn ederhow sor prion wo pe ofor tor hube not phessouch womane foritor spor, we hes co beve donce cial it\"denod phessole\n",
      "Epoch 56/60\n",
      "2000/2000 [==============================] - 2s 940us/step - loss: 0.0775\n",
      "\n",
      "----- Generating text after Epoch: 55\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" unfairly. thus we are told of the cunni\"\n",
      " unfairly. thus we are told of the cunnith, the strual by hast ave bere sore sole splate bere stours of phiss, we have are bein s of the stous lost daguous \"pres burs lve cand and the sore socaint the strual by hast evee hand and now pe whing elees and strat val by hade arlly sore soren asply the strual by hast evee hand and now pe whing elee of phiss, we have arle the stoustrat the lodisone man it we mad of prably s of desicially ror p\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" unfairly. thus we are told of the cunni\"\n",
      " unfairly. thus we are told of the cunnith, the ss? onde prast,elndes of st re ingred the \"s dore or porill, t dast dedicat \"s \"prs, we had of prrelosopur, whind as the destruanting co sprall the lvee prastid dins and straing strall-tre lodesole have arl the lose of as ant re of ar aly to the destruat destruand strat the desico bucit ve aimallyes and as hore sorene spratingert wh the e phes pharl, trit ed sorre beng on s of the venye ca\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" unfairly. thus we are told of the cunni\"\n",
      " unfairly. thus we are told of the cunnith, the s cand sore bain now pe hive bere be castor d mache the now phes canderston dustid neve a cans af it he at ting co the lve phatite, the dastpe the ludisope ithe rengrengen )s lore of ans in hure bere sthe athe toud sorutever, whing elees ard, tore, the das porelle t it wothas huve of it wisthe the wosk contiely row in surh deniphest wistiel, that it we to torsply, when the \"rsulng coat inq\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" unfairly. thus we are told of the cunni\"\n",
      " unfairly. thus we are told of the cunning cor porely the  are ard lane of ay ions? whas bein st af the . veours.ly real bly ingred the . phas lugitised, the dast east evee prathous \"prespricias lust ted passlly wowhans sore baid cordith, in elye peahdattans bere cand and straveny have bore sored of perhely prath, the streal burm, the mand and and at.e the lose phaston st ie of d maithe the lose hand, the quns ang whith it he at als the\n",
      "Epoch 57/60\n",
      "2000/2000 [==============================] - 2s 854us/step - loss: 0.0426\n",
      "\n",
      "----- Generating text after Epoch: 56\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"themselves critics, and assuredly they w\"\n",
      "themselves critics, and assuredly they whand and the soor porill the valua the masuof spri, the dasupeduig cans af the are of pricl, t it dabube chat interee spratinged the struand the struand and ain of whing of dema the \"splat valub us, wo have arl the stores cor posd coas in whing-rlat of prisinn a laguouspurems coms iow phestous \"ple, whind stours of dere phiss lone masspeduthe ssoren it re ard are of pricl, t it mast ave byen dust \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"themselves critics, and assuredly they w\"\n",
      "themselves critics, and assuredly they whing of such dustruat de phastingt ably tere behe of a mog pursplat wisplresperititerfiois of whid mand, the dast ang now pestions! as! magh arenbely that ithes sormad as! ous mere of a logioc st aint-rlate it hest and the roul the \"speane for tomis all soricat may us an the are buin st and arl it\"re priel, that aluat it he east eanest and the restroat of pricl bere candathy rotesorcarl bein st an\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"themselves critics, and assuredly they w\"\n",
      "themselves critics, and assuredly they whand and and arl yore ors labe forlee pring--trat the struall boug cansorst vas be desusian  phis, wor vas bere now phes bung, whinh are formerthy hodes of solf the dasuped-inst al ty ind sore pratl, t it, ard consped, that it wo tore bone soraveuitinedfoues\"danly souctives, thithes elsticist ave bard ard menhed esned mas for the \"aluby have arlat of prlot splreakestiod sorut may he doskphes of th\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"themselves critics, and assuredly they w\"\n",
      "themselves critics, and assuredly they whand and the veare bore or as and de, phes of afly lodisporet ainct mage; rpal  torutendince sthat the dast of phisis\" and their sing, the dast, in sperithes alr toricl the luned and the dosks ang wo lf the .osupedutotefy icat ve macuousp\"reuns? whihe ofor to the wesus ans toruvesuediss lf-ther the od phis, we had aymer phing--firite of the ease of athe to eves\" wowhereskinat of st able the dalua \n",
      "Epoch 58/60\n",
      "2000/2000 [==============================] - 2s 858us/step - loss: 0.0212\n",
      "\n",
      "----- Generating text after Epoch: 57\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" only an \"idea,\" a \"modern idea\"!    cha\"\n",
      " only an \"idea,\" a \"modern idea\"!    chare bein st and straaly rore boce the \"porsplye conco chat the strably tor plose on domure, torive ow it re tith or a lonope have arl the stous mor phespore of thit the arleat tee of pris on st aigthear ald aintot a cons af aly tore of bele pore, and the stousthe athal bely cons af all to is dallee peahd the mare, the dastpe the soor pore pore bein s alat ofesplathat the \"toulnof cans at the arous \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" only an \"idea,\" a \"modern idea\"!    cha\"\n",
      " only an \"idea,\" a \"modern idea\"!    chare bein st ald sore pore oo beand athe tore oo by hithoreson a hait all the more soon the are arl arluatie coose platiof paid thoud of dery hand and torinons and and ain of phis mor phest tor hups stoure of a logiplest and the more on a loficial bein st and the soor peailly rand are oope hope tondersply teat and and inor have beon st aen the lost ten of arl buin st af the  once of a mage it we to \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" only an \"idea,\" a \"modern idea\"!    cha\"\n",
      " only an \"idea,\" a \"modern idea\"!    chare node anly hode ood of ail of whit of haps the mace, the strual been daniesplatinet pe conceesphais and and all good socaat al the arme tout the \"oskence of a mageprearston st aig ous \"d the ot enqt ay uabt of es, andes of the stra lven the lost have aroungly acat the \"phaply hode oon a may phinsore porill-to arly toreves, tive of ael amourore concathy aconbeen af atiant rean aspluespereally hin\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" only an \"idea,\" a \"modern idea\"!    cha\"\n",
      " only an \"idea,\" a \"modern idea\"!    chare bore soncterveljthe kondessole toply ror pord of hope sooe porers\"ly that all the lore of the are beon st ang the at.egr, the struand sore pore bope ofor aly in efoee pore \"phe porill thir alle how the \"theangerous oppestind the and ard ainoof ahhino! splr-that value mand ant of donist annet of pams and and it we have ary lode ooch and \"phisone have a loso martall.er phinsof the stoustrore ons \n",
      "Epoch 59/60\n",
      "2000/2000 [==============================] - 2s 882us/step - loss: 0.0930\n",
      "\n",
      "----- Generating text after Epoch: 58\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"have the power, notwithstanding all thei\"\n",
      "have the power, notwithstanding all their of priclatendess, ing we have arlof soratist the destrear blee porsplat valube phasding, when the arlubly ras now phes of the arl of pricl, torivas ins lure of all the dast of plles of prles of sply, whing of a logupe isply rad sore sole and and of phis, lofirmelve now pe wins re mage beice of arl a cand a lighe  the pestitis ous! as! ous de have been dans afle \"ps magupe cans anl and it re arma\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"have the power, notwithstanding all thei\"\n",
      "have the power, notwithstanding all their of priclatendessouched the \"sply that all the strual blye peatials be cans able to lfarl.e forlee one salu the dast uas bere strably that al the rt.  prastoursnofuvely ras now phes marlue phastit sore proulns af ais of bure able the quss af all the dastreat destraing renot phessole pard ard ousn ithe rt ald burged the strual blyes sper\"es of aslly cond, the quss and sale that the arstol bupgr, t\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"have the power, notwithstanding all thei\"\n",
      "have the power, notwithstanding all their ow ade inbely s of the strual of arlabung cons, wo habe of bain now phes of sall, t it we madtye plasthe touthed the sope a magupe canstithes ef tist usderrel the rosusudedatisonfitist, and struans ablea of bel, that all the struals on st af the resken denial beme canstions lederesion whing--thatdores af all the strual byeresly and the mess hand and and of phis, lost oas of the arl or bure beme \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"have the power, notwithstanding all thei\"\n",
      "have the power, notwithstanding all their of pricoss? it rast als bure able to lvely thing--thatdy hct the sthave drmal phiss, we habe bleg cansery ins lureperhapst tented isp\"titones loguicis \"f aid iopleer-that the struand and ay ions\"d whing of a cons de hindesous, what al ally ouss\"d mughe aplathy hifsors ungenqe resply red ars of erouledestriang nod of p is luberinst f mestit, whand and and aid mame kondathy rcans bere proulded str\n",
      "Epoch 60/60\n",
      "2000/2000 [==============================] - 2s 894us/step - loss: 0.0245\n",
      "\n",
      "----- Generating text after Epoch: 59\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"at they lose their keen eye and nose. in\"\n",
      "at they lose their keen eye and nose. in st ave  inselye peatide, the dame the dane-the stoungy ins rear buentinges alather the dastourspeat it he dosus ang whin -that the dasupe finsorenor valuating co phais and and of phiclosonive ard lustruas beve of arl arl a degiog sore phisill--nlathy rennow can of ith re pricl, torivesors and the stous londee phesperting, whing -that the dane, the dane-the sone soren of whitherestrous! and it he \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"at they lose their keen eye and nose. in\"\n",
      "at they lose their keen eye and nose. innded ih hive arlea ouen it whithes all the dast ousnourhan the lode of ithertor priol beme mand a damand ang in the arl ard ard of pricl, torivesors and \"s? womus of whind and and gundithe dost of the  the pesporsplye wendenod phespertourtourdably s of whin st al  of the very whing or platined of pasulustindes of a ligher tims and the dast of prising chat the dast of prising stiousn fimis of whid \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"at they lose their keen eye and nose. in\"\n",
      "at they lose their keen eye and nose. ing to the vely pndith, whing elot phen -that the qaply lonevery hund, the dases an eusidedimat pe conspecking sply that all the struat of a logicially rave are bere sone may morephephisonow of whis sofitho as and \"s? wocuand soma lfyr and and of whin -thaid the deaid of it heve of phessousndurhes epeemor phiss mor pursplathe recisting, what we nethe inq preat and \" of whid va ues lfarea candarly bu\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"at they lose their keen eye and nose. in\"\n",
      "at they lose their keen eye and nose. in reng enqeusole pain the ard ald soris\"skecit the atouln incain thears loge of a lane; the quss and y mand the the mane, the dand and and may hoge proulas ndve  ince qups lf the \"theal by hister and the dastpurenot solubuihn, the dace. in sofuin it ae arutitingens whithert whan aspentee of arl buremof valuitinedubeining and acd ouphers, and the struat not pescion whoureror the magiof prially roal \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f83b5092e8>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x[0:2000], y[0:2000],\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               94720     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 56)                7224      \n",
      "=================================================================\n",
      "Total params: 101,944\n",
      "Trainable params: 101,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is very similar to the one you have built for dinosaur names. The only major differences are:\n",
    "- LSTMs instead of the basic RNN to capture longer-range dependencies\n",
    "- The model is a deeper, stacked LSTM model (2 layer)\n",
    "- Using Keras instead of python to simplify the code \n",
    "\n",
    "If you want to learn more, you can also check out the Keras Team's text generation implementation on GitHub: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py.\n",
    "\n",
    "Congratulations on finishing this notebook! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "- This exercise took inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "- For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
