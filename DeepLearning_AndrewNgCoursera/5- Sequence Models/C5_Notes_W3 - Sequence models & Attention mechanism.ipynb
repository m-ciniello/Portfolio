{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C5_Notes_W3 - Sequence models & Attention mechanism\n",
    "\n",
    "> Sequence models can be augmented using an attention mechanism. This algorithm will help your model understand where it should focus its attention given a sequence of inputs. This week, you will also learn about speech recognition and how to deal with audio data.\n",
    "\n",
    "* [1. Various sequence to sequence architectures](#various-sequence-to-sequence-architectures)\n",
    " * [1.1 Basic Models](#basic-models)\n",
    " * [1.2 Picking the most likely sentence](#picking-the-most-likely-sentence)\n",
    " * [1.3 Beam Search](#beam-search)\n",
    " * [1.4 Refinements to Beam Search](#refinements-to-beam-search)\n",
    " * [1.5 Error analysis in beam search](#error-analysis-in-beam-search)\n",
    " * [1.6 BLEU Score](#bleu-score)\n",
    " * [1.7 Attention Model Intuition](#attention-model-intuition)\n",
    " * [1.8 Attention Model](#attention-model)\n",
    "\n",
    "\n",
    "* [2. Speech recognition - Audio data](#speech-recognition---audio-data)\n",
    " * [2.1 Speech recognition](#speech-recognition)\n",
    " * [2.2 Trigger Word Detection](#trigger-word-detection)\n",
    "\n",
    "\n",
    "* [3. Extras](#extras)\n",
    "  * [3.1 Machine translation attention model (From notebooks)](#machine-translation-attention-model-from-notebooks)\n",
    "\n",
    "# 1. Various sequence to sequence architectures\n",
    "\n",
    "#### Basic Models\n",
    "- In this section we will learn about sequence to sequence - Many to Many -  models which are useful in various applications including machine translation and speech recognition.\n",
    "- Lets start by the basic model:\n",
    "  - Given this machine translation problem in which X is a French sequence and Y is an English sequence.\n",
    "    - ![](Images/52.png)\n",
    "  - Our architecture will include **encoder** and **decoder**.\n",
    "  - The encoder is built with RNNs - LSTM or GRU are included - and takes the input sequence and then outputs a vector that should represent the whole input.\n",
    "  - After that the decoder network, are also built with RNNs and outputs the output sequence using the vector that has been built by the encoder.\n",
    "  - ![](Images/53.png)\n",
    "  - These ideas are from these papers:\n",
    "    - [[Sutskever](https://arxiv.org/abs/1409.3215) et al., 2014. Sequence to sequence learning with neural networks]\n",
    "    - [[Cho et](https://arxiv.org/abs/1406.1078) al., 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation]\n",
    "- With an architecture similar to the one previously mentioned works for image captioning problem:\n",
    "  - In this problem X is an image, while Y is a sentence.\n",
    "  - The model architecture image:\n",
    "    - ![](Images/54.png)\n",
    "  - The architecture uses a CNN pretrained AlexNet as an encoder for the image, and the decoder is an RNN.\n",
    "  - The ideas are from these papers (They share similar ideas):\n",
    "    - [[Maoet](https://arxiv.org/abs/1412.6632). al., 2014. Deep captioning with multimodal recurrent neural networks]\n",
    "    - [[Vinyalset](https://arxiv.org/abs/1411.4555). al., 2014. Show and tell: Neural image caption generator]\n",
    "    - [[Karpathy](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf) and Li, 2015. Deep visual-semantic alignments for generating imagedescriptions]\n",
    "\n",
    "#### Picking the most likely sentence\n",
    "- There are some similarities between the language model we have learned previously, and the machine translation model we have just discussed, but there are some differences as well.\n",
    "- The language model we have learned as so similar to the decoder of the machined translation model, except for a<sup>0</sup>\n",
    "  - ![](Images/55.png)\n",
    "- The problems formations also are different:\n",
    "  - In language model: P(y<sup>\\<1></sup>, ....y<sup>\\<Ty></sup>)\n",
    "  - In machine translation: P(y<sup>\\<1></sup>, ....y<sup>\\<Ty></sup> | x<sup>\\<1></sup>, ....x<sup>\\<Tx></sup>)\n",
    "- What we don't want in machine translation model, is not to sample the output at random. This may provide some choices as an output. Sometimes you may sample a bad output.\n",
    "  - Example: \n",
    "    - X = \"Jane visite lâ€™Afrique en septembre.\"\n",
    "    - Y may be:\n",
    "      - Jane is visiting Africa in September.\n",
    "      - Jane is going to be visiting Africa in September.\n",
    "      - In September, Jane will visit Africa.\n",
    "- So we need to get the best output, this can be take by the equation:\n",
    "  - ![](Images/56.png)\n",
    "- The most common algorithm is the beam search, which we will explain in the next section.\n",
    "- Why not use greedy search? Why not get the best choices each time?\n",
    "  - It turns out that this approach doesn't really work!\n",
    "  - Lets explain it with an example:\n",
    "    - The best output for the example we talked about is \"Jane is visiting Africa in September.\"\n",
    "    - Suppose that you when you are choosing with greedy approach, the first two words were \"Jane is\", the word that may come after that will be \"going\" as \"going\" is the most common word that comes after \"Noun is\" so the result may look like this: \"Jane is going to be visiting Africa in September.\" and that isn't the best/optimal solution.\n",
    "- So what is better than greedy approach, is trying to get an approximate solution, that will try to maximize the output.\n",
    "\n",
    "#### Beam Search\n",
    "- Beam search is the most widely used algorithm to get the best output sequence. Its a heuristic search algorithm.\n",
    "- To illustrate the algorithm we will be stick with the example from the previous section. We need Y = \"Jane is visiting Africa in September.\"\n",
    "- The algorithm has a parameter `B`  which is the beam width. Lets take `B = 3` which means the algorithm will get 3 outputs at a time.\n",
    "- For the first step you will get [\"in\", \"jane\", \"september\"] words that are the best candidates.\n",
    "- Then for each word in the first output, get B words from the 3 where the best are the result of multiplying both probabilities. Se we will have then [\"In September\", \"jane is\", \"jane visit\"]. Notice that we automatically ignored September.\n",
    "- Repeat the same process and get the best B words for [\"September\", \"is\", \"visit\"]  and so so.\n",
    "- In this algorithm, keep only B instances of your network.\n",
    "- If `B = 1` this will become the greedy search.\n",
    "\n",
    "#### Refinements to Beam Search\n",
    "- In the previous section we have discussed the basic beam search. In this section we will try to do some refinements to it to work even better.\n",
    "- The first thing is **Length optimization**\n",
    "  - In beam search we are trying to optimize:\n",
    "    - ![](Images/56.png)\n",
    "  - And to do that we multiply:\n",
    "    - P(y<sup>\\<1></sup> | x) * P(y<sup>\\<2></sup> | x, y<sup>\\<1></sup>) * ..... P(y<sup>\\<t></sup> | x, y<sup>\\<y(t-1)></sup>)\n",
    "  - Each probability is a fraction. Also maybe a small fraction.\n",
    "  - Multiplying small fractions will cause a **numerical overflow**! Meaning that it's too small for the floating part representation in your computer to store accurately.\n",
    "  - So in practice we use **summing** **logs** instead of multiplying directly.\n",
    "    - ![](Images/57.png)\n",
    "  - But theres another problem. The two optimization functions we have mentions tends to find small sequences! Because multiplying a lot of fractions gives a smaller value.\n",
    "  - So theres another change , by dividing by the number of elements in the sequence.\n",
    "    - ![](Images/58.png)\n",
    "    - alpha is a hyper parameter to tune.\n",
    "    - If alpha = 0, Then its like we do nothing.\n",
    "    - If alpha = 1, Then its like we are using full sequence length.\n",
    "    - In practice alpha = 0.7 is a good thing. \n",
    "- The second thing is who can we choose best `B`?\n",
    "  - The larger B, the larger possibilities, the better are the results. But it will be more computationally expensive.\n",
    "  - In practice, you might see a in the production sentence `B=10`\n",
    "  - `B=100`, `B=1000` are uncommon.\n",
    "  - Unlike exact search algorithms like BFS (Breadth First Search) or  DFS (Depth First Search), Beam Search runs faster but is not guaranteed to find exact solution.\n",
    "\n",
    "#### Error analysis in beam search\n",
    "- We have talked before on **Error analysis** in <u>Structuring Machine Learning Projects</u> chapter. We will apply these concepts to improve our beam search algorithm.\n",
    "- We will use error analysis to figure out if the `B` hyperparameter of the beam search is the problem - because it doesn't get an optimal solution  - or to in other hyperparameters like the RNN parameters.\n",
    "- Lets take an example:\n",
    "  - Our examples information:\n",
    "    - x = \"Jane visite lâ€™Afrique en septembre.\"\n",
    "    - y<sup>*</sup> = \"Jane visits Africa in September.\"\n",
    "    - y<sup>^</sup> = \"Jane visited Africa last September.\"\n",
    "  - Our model that has produced a sentence that are different in meaning because of the word \"last\"\n",
    "  - We now want to know who to blame, the RNN or the beam search.\n",
    "  - To do that, we calculate P(y<sup>*</sup> | X) and P(y<sup>^</sup> | X). There are two cases:\n",
    "    - Case 1 (P(y<sup>*</sup> | X)  > P(y<sup>^</sup> | X)): \n",
    "      - Conclusion: Beam search is at fault.\n",
    "    - Case 2 (P(y<sup>*</sup> | X)  <= P(y<sup>^</sup> | X)): \n",
    "      - Conclusion: RNN model is at fault.\n",
    "- The error analysis process is as following:\n",
    "  - You choose N error examples and make the following table:\n",
    "    - ![](Images/59.png)\n",
    "  - `B`  for beam search, `R` is for the RNN.\n",
    "  - Get counts and decide.\n",
    "\n",
    "#### BLEU Score\n",
    "- One of the challenges of machine translation, is that given a sentence in a language there are one or more possible good translation in another language. So how do we evaluate our results?\n",
    "- The way we do this is by using **BLEU score**. BLEU stands for bilingual evaluation understudy.\n",
    "- The intuition is so long as the machine generated translation is pretty close to any of the references provided by humans, then it will get a high BLEU score.\n",
    "\n",
    "\n",
    "- Lets take an example:\n",
    "\n",
    "  - X = \"Le chat est sur le tapis.\"\n",
    "  - Y1 = \"The cat is on the mat.\"\n",
    "  - Y2 = \"There is a cat on the mat.\"\n",
    "  - Suppose that the machine outputs: \"<u>the the the the the the the.</u>\"\n",
    "  - One way to evaluate the machine output is to look at each word in the output and check it in the references. This is called precision:\n",
    "    - precision = 7/7  because the appeared in Y1 or Y2\n",
    "  - This is not a useful measure!\n",
    "  - We can use a modified precision in which we are looking for the reference with the maximum number of a particular word and set the maximum appearing of this word to this number. So:\n",
    "    - modified precision = 2/7 because the max is 2 in Y1\n",
    "    - We clipped the 7 times by the max which is 2.\n",
    "  - The problem here is that we are looking at one word at a time, we may need to look at pairs\n",
    "\n",
    "- Another example (BLEU score on bigrams)\n",
    "\n",
    "  - The n-**grams** typically are collected from a text or speech corpus. When the items are words, n-**grams** may also be called shingles. An n-**gram** of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\".\n",
    "\n",
    "  - X = \"Le chat est sur le tapis.\"\n",
    "\n",
    "  - Y1 = \"The cat is on the mat.\"\n",
    "\n",
    "  - Y2 = \"There is a cat on the mat.\"\n",
    "\n",
    "  - Suppose that the machine outputs: \"<u>The cat the cat on the mat.</u>\"\n",
    "\n",
    "  - The bigrams in the machine output:\n",
    "\n",
    "  - | Pairs      | Count | Count clip |\n",
    "    | ---------- | ----- | ---------- |\n",
    "    | the cat    | 2     | 1 (Y1)     |\n",
    "    | cat the    | 1     | 0          |\n",
    "    | cat on     | 1     | 1 (Y2)     |\n",
    "    | on the     | 1     | 1 (Y1)     |\n",
    "    | the mat    | 1     | 1 (Y1)     |\n",
    "    | **Totals** | 6     | 4          |\n",
    "\n",
    "    Score = Count clip / Count = 4/6\n",
    "\n",
    "- So here are the equations for the n-grams:\n",
    "\n",
    "  - ![](Images/60.png)\n",
    "\n",
    "- Lets put this together to formalize the BLEU score:\n",
    "\n",
    "  - **P<sub>n</sub>** = Bleu score on n-grams only\n",
    "  - **Combined Bleu score** equation:\n",
    "    - ![](Images/61.png)\n",
    "    - For example if we want Bleu for 4, we compute P<sub>1</sub>, P<sub>2</sub>, P<sub>3</sub>, P<sub>4</sub> and then average them and take the exp.\n",
    "  - Another equation is **BP penalty** which stands for brevity penalty. It turns out that if a machine outputs a small number of words it will get a better score so we need to handle that.\n",
    "    - ![](Images/62.png)\n",
    "\n",
    "- Blue score is has several open source implementations and used in variety of systems like machine translation and image captioning.\n",
    "\n",
    "#### Attention Model Intuition\n",
    "\n",
    "- So far we are using sequence to sequence models with an encoder and decoders. There are a technique called attention which makes these models even better.\n",
    "- The attention algorithm, the attention idea has been one of the most influential ideas in deep learning. \n",
    "- The problem of long sequences:\n",
    "  - Given this model, inputs, and outputs.\n",
    "    - ![](Images/63.png)\n",
    "  - The encoder should memorize this long sequence into one vector, and the decoder has to process this vector to generate the translation.\n",
    "  - If a human would translate this sentence, he wouldn't read the whole sentence and memorize it then try to translate it. He translates a part at a time.\n",
    "  - The performance of this model decreases if a sentence is so long.\n",
    "  - We will discuss the attention model that works like a human that looks at parts at a time. That will significantly increase the accuracy even with bigger sequence:\n",
    "    -  ![](Images/64.png)\n",
    "    -  Blue is the normal model, while green is the model with attention mechanism.\n",
    "- In this section we will give just some intuitions about the attention model and in the next section we will discuss its details.\n",
    "- At first the attention model was developed for machine translation but then other applications used it like computer vision and new architectures like Neural Turing machine.\n",
    "- The attention model was descried in this paper:\n",
    "  - [[Bahdanau](https://arxiv.org/abs/1409.0473) et. al., 2014. Neural machine translation by jointly learning to align and translate]\n",
    "- Now for the intuition:\n",
    "  - Suppose that our decoder is a bidirectional RNN:\n",
    "    - ![](Images/65.png)\n",
    "  - We gave the French sentence to the encoder and it should generate a vector that represents the inputs.\n",
    "  - Now to generate the first word in English which is \"Jane\" we will make another RNN which is the decoder.\n",
    "  - attention weights are used to specify which words are needed when to generate a word. So to generate \"jane\" we will look at \"jane\", \"visite\", \"l'Afrique\"\n",
    "    - ![](Images/66.png)\n",
    "  - alpha<sup>1,1</sup>, alpha<sup>1,2</sup>, and alpha<sup>1,3</sup> are the attention weights used.\n",
    "  - And so to generate any word there will be a set of attention weights that controls which words we are looking at right now.\n",
    "    - ![](Images/67.png)\n",
    "  - â€‹\n",
    "\n",
    "#### Attention Model\n",
    "\n",
    "- Lets formalize the intuition from the last section into the exact details on how this can be implemented.\n",
    "- First we will have an bidirectional RNN - most common is LSTMs - that encodes French language:\n",
    "  - ![](Images/68.png)\n",
    "- For learning purposes, lets assume that a<sup>\\<t></sup> will include the both directions.\n",
    "- We will have an RNN to extract the output using a context `c` which is computer using the attention weights. This denotes how much information do it needs to look in a<sup>\\<t></sup>\n",
    "  - ![](Images/69.png)\n",
    "- Sum of the attention weights for each element in the sequence should be 1:\n",
    "  - ![](Images/70.png)\n",
    "- Also the context `c` are calculated using this equation:\n",
    "  - ![](Images/71.png)\n",
    "- Lets see how can we compute the attention weights:\n",
    "  - So alpha<sup>\\<t, t'></sup> = amount of attention y<sup>\\<t></sup> should pay to a<sup>\\<t'></sup>\n",
    "    - Like for example we payed attention to the first three words through alpha<sup>\\<1,1></sup>, alpha<sup>\\<1,2></sup>, alpha<sup>\\<1,3></sup>\n",
    "  - We are going to softmax the attention weights so that their sum is 1:\n",
    "    - ![](Images/72.png)\n",
    "  - Now we need to know how to calculate e<sup>\\<t, t'></sup>. We will compute e using a small neural network:\n",
    "    - ![](Images/73.png)\n",
    "    - s<sup>\\<t-1></sup> is the hidden state of the RNN s, and a<sup>\\<t'></sup> is the activation of the other bidirectional RNN. \n",
    "- One of the disadvantages of this algorithm is that it takes quadratic time or quadratic cost to run.\n",
    "- One fun way to see how attention works is by visualizing the attention weights:\n",
    "  - ![](Images/74.png)\n",
    "\n",
    "### Speech recognition - Audio data\n",
    "\n",
    "#### Speech recognition\n",
    "- The rise of accurate speech recognition was the most exciting work done in sequence to sequence deep learning models.\n",
    "- Lets define the speech recognition problem:\n",
    "  - X: audio clip\n",
    "  - Y: transcript\n",
    "  - If you plot an audio clip it should look like this:\n",
    "    - ![](Images/75.png)\n",
    "    - The horizontal axis is time while the vertical is changes in air pressure.\n",
    "  - What really is an audio recording? A microphone records little variations in air pressure over time, and it is these little variations in air pressure that your ear also perceives as sound. You can think of an audio recording is a long list of numbers measuring the little air pressure changes detected by the microphone. We will use audio sampled at 44100 Hz (or 44100 Hertz). This means the microphone gives us 44100 numbers per second. Thus, a 10 second audio clip is represented by 441000 numbers (= $10 \\times 44100$).\n",
    "  - It is quite difficult to work with \"raw\" representation of audio.\n",
    "  - Because even human ear doesn't process raw wave forms, the human ear can process different frequencies.\n",
    "  - There's a common preprocessing step for an audio to generate a spectrogram which works similarly to human ears.\n",
    "    - ![](Images/76.png)\n",
    "    - The horizontal axis is time while the vertical is frequencies. Intensity of different colors shows the amount of energy.\n",
    "  - A spectrogram is computed by sliding a window over the raw audio signal, and calculates the most active frequencies in each window using a Fourier transform.\n",
    "  - In the past days, speech recognition systems was built with phonemes that are a hand engineered basic units of sound. Linguists use to hypothesize any writing down audio in terms of phonemes which they thought would be the best way to do speech recognition.\n",
    "  - End to end deep learning found that phonemes was no longer needed. One of the things that made this possible is the large audio datasets.\n",
    "  - Research papers has 300 - 3000 hours while the best commercial systems are now trained on over 100,000 hours of audio.\n",
    "- You can build an accurate speech recognition system using the attention model that we have descried in the previous section:\n",
    "  - ![](Images/77.png)\n",
    "- One of the methods that seem to work well is CTC cost which stands for \"Connectionist temporal classification\" \n",
    "  - To explain this lets say that Y = \"<u>the quick brown fox</u>\"\n",
    "  - We are going to use an RNN with input, output structure:\n",
    "    - ![](Images/78.png)\n",
    "  - Hint: this is a bidirectional RNN, but it practice a bidirectional RNN are used.\n",
    "  - Notice that the number of inputs and number of outputs are the same here, but in speech recognition problem X tends to be a lot larger than Y.\n",
    "    - 10 seconds of audio gives us X with shape (1000, ). This 10 seconds can't have 1000 character!\n",
    "  - The CTC cost function allows the RNN to output something like this:\n",
    "    - `ttt_h_eee\\<SPC>___\\<SPC>qqq___`\n",
    "    - This covers from \"The quick\".\n",
    "    - The _ is a special character called blank and `<SPC>` is for space character.\n",
    "  - So the 19 character in our Y can be generated into 1000 character output using CTC and its special blanks.\n",
    "  - The ideas were taken from this paper:\n",
    "    - [[Graves](https://dl.acm.org/citation.cfm?id=1143891) et al., 2006. Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks]\n",
    "    - This paper also are used by baidue deep speech.\n",
    "- Both options attention models and CTC cost can give you an accurate speech recognition system.\n",
    "\n",
    "#### Trigger Word Detection\n",
    "- With the rise of deep learning speech recognition, there are a lot of devices that can be waked up by saying some words with your voice. These systems are called trigger word systems.\n",
    "- For example, Alexa - a smart device made by amazon - can answer your call \"Alexa, What time is it\" and then Alexa will reply you.\n",
    "- Trigger word detection system includes:\n",
    "  - ![](Images/79.png)\n",
    "- Now the trigger word detection literature is still evolving so there actually isn't a single universally agreed on algorithm for trigger word detection yet. But lets discuss an algorithm that can be used.\n",
    "- Lets now build a model that can solve this problem:\n",
    "  - X: audio clip\n",
    "  - X has been preprocessed and spectrogram features has been returned of X\n",
    "    - X<sup>\\<1></sup>, X<sup>\\<2></sup>, ... , X<sup>\\<t></sup>\n",
    "  - Y will be labels 0 or 1. 0 represents the non trigger word, while 1 is that trigger word that we need to detect.\n",
    "  - The model architecture can be like this:\n",
    "    - ![](Images/80.png)\n",
    "    - The vertical lines in the audio clip represents the trigger words. The corresponding to this will be 1.\n",
    "  - One disadvantage of this is the imbalanced dataset outputs. There will be a lot of zeros and little ones.\n",
    "  - A hack to solve this is to make an output a few ones for several times or for a fixed period of time before reverting back to zero.\n",
    "    - ![](Images/81.png)\n",
    "    - ![](Images/85.png)\n",
    "  - â€‹\n",
    "\n",
    "\n",
    "## Extras\n",
    "\n",
    "### Machine translation attention model (From notebooks)\n",
    "\n",
    "- The model is built with keras layers.\n",
    "- The attention model.\n",
    "  - ![](Images/83.png)\n",
    "  - There are two separate LSTMs in this model. Because the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism, we will call it *pre-attention* Bi-LSTM. The LSTM at the top of the diagram comes *after* the attention mechanism, so we will call it the *post-attention* LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps. \n",
    "  - The post-attention LSTM passes $s^{\\langle t \\rangle}, c^{\\langle t \\rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\\langle t\\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\\langle t\\rangle}$ and the hidden cell state $c^{\\langle t\\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\\langle t-1 \\rangle}$ as input; it only takes $s^{\\langle t\\rangle}$ and $c^{\\langle t\\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date. \n",
    "- What one \"Attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$, which are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "  - ![](Images/84.png)\n",
    "  - The diagram uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times, and then `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ to compute $e^{\\langle t, t'}$, which is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$. \n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<br><br>\n",
    "These Notes were made by [Mahmoud Badry](mailto:mma18@fayoum.edu.eg) @"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
