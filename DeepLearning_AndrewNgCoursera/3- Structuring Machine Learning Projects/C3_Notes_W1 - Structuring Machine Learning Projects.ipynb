{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3_Notes_W1 - Structuring Machine Learning Projects\n",
    "\n",
    "This is the third course of the deep learning specialization at [Coursera](https://www.coursera.org/specializations/deep-learning) which is moderated by [DeepLearning.ai](http://deeplearning.ai/). The course is taught by Andrew Ng.\n",
    "\n",
    "## Table of contents\n",
    "* [1. Course Summary](#why-ml-strategy)\n",
    "* [2. Why ML Strategy](#why-ml-strategy)\n",
    "* [3. Orthogonalization](#orthogonalization)\n",
    "* [4. Single number evaluation metric](#single-number-evaluation-metric)\n",
    "* [5. Satisfying and Optimizing metric](#satisfying-and-optimizing-metric)\n",
    "* [6. Train/dev/test distributions](#traindevtest-distributions)\n",
    "* [7. Size of the dev and test sets](#size-of-the-dev-and-test-sets)\n",
    "* [8. When to change dev/test sets and metrics](#when-to-change-devtest-sets-and-metrics)\n",
    "* [9. Why human-level performance?](#why-human-level-performance)\n",
    "* [10. Avoidable bias](#avoidable-bias)\n",
    "* [11. Understanding human-level performance](#understanding-human-level-performance)\n",
    "* [12. Surpassing human-level performance](#surpassing-human-level-performance)\n",
    "* [13. Improving your model performance](#improving-your-model-performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Course summary\n",
    "\n",
    "Here are the course summary as its given on the course [link](https://www.coursera.org/learn/machine-learning-projects):\n",
    "\n",
    "> You will learn how to build a successful machine learning project. If you aspire to be a technical leader in AI, and know how to set direction for your team's work, this course will show you how.\n",
    ">\n",
    "> Much of this content has never been taught elsewhere, and is drawn from my experience building and shipping many deep learning products. This course also has two \"flight simulators\" that let you practice decision-making as a machine learning project leader. This provides \"industry experience\" that you might otherwise get only after years of ML work experience.\n",
    ">\n",
    "> After 2 weeks, you will: \n",
    "> - Understand how to diagnose errors in a machine learning system, and \n",
    "> - Be able to prioritize the most promising directions for reducing error\n",
    "> - Understand complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance\n",
    "> - Know how to apply end-to-end learning, transfer learning, and multi-task learning\n",
    ">\n",
    "> I've seen teams waste months or years through not understanding the principles taught in this course. I hope this two week course will save you months of time.\n",
    ">\n",
    "> This is a standalone course, and you can take this so long as you have basic machine learning knowledge. This is the third course in the Deep Learning Specialization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Why ML Strategy\n",
    "\n",
    "- You have a lot of ideas for how to improve the accuracy of your deep learning system:\n",
    "  - Collect more data.\n",
    "  - Collect more diverse training set.\n",
    "  - Train algorithm longer with gradient descent.\n",
    "  - Try different optimization algorithm (e.g. Adam).\n",
    "  - Try bigger network.\n",
    "  - Try smaller network.\n",
    "  - Try dropout.\n",
    "  - Add L2 regularization.\n",
    "  - Change network architecture (activation functions, # of hidden units, etc.)\n",
    "- This course will give you some strategies to help analyze your problem to go in a direction that will help you get better results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Orthogonalization\n",
    "\n",
    "- Some deep learning developers know exactly what hyperparameter to tune in order to try to achieve one effect. This is a process we call orthogonalization.\n",
    "- In orthogonalization, you have some controls, but each control does a specific task and doesn't affect other controls.\n",
    "- For a supervised learning system to do well, you usually need to tune the knobs of your system to make sure that four things hold true - chain of assumptions in machine learning:\n",
    "  1. You'll have to fit training set well on cost function (near human level performance if possible).\n",
    "     - If it's not achieved you could try bigger network, another optimization algorithm (like Adam)...\n",
    "  2. Fit dev set well on cost function.\n",
    "     - If its not achieved you could try regularization, bigger training set...\n",
    "  3. Fit test set well on cost function.\n",
    "     - If its not achieved you could try bigger dev. set...\n",
    "  4. Performs well in real world.\n",
    "     - If its not achieved you could try change dev. set, change cost function..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Single number evaluation metric\n",
    "\n",
    "- Its better and faster to set a single number evaluation metric for your project before you start it.\n",
    "- Difference between precision and recall (in cat classification example):\n",
    "  - Suppose we run the classifier on 10 images which are 5 cats and 5 non-cats. The classifier identifies that there are 4 cats, but it identified 1 wrong cat.\n",
    "  - **Precision**: percentage of true cats in the recognized result: P = 3/(3 + 1) \n",
    "  - **Recall**: percentage of true recognition cat of the all cat predictions: R = 3/(3 + 2)\n",
    "  - **Accuracy**: (3+4)/10\n",
    "- Using a precision/recall for evaluation is good in a lot of cases, but separately they don't tell you which algothims is better. \n",
    "- A better thing is to combine precision and recall in one single (real) number evaluation metric. There a metric called `F1` score, which combines them\n",
    "  - You can think of `F1` score as an average of precision and recall\n",
    "    `F1 = 2 / ((1/P) + (1/R))`\n",
    "\n",
    "# 5. Satisfying and Optimizing metric\n",
    "\n",
    "- Its hard sometimes to get a single number evaluation metric.\n",
    "- So we can solve that by choosing a single optimizing metric and decide that other metrics are satisfying. Ex:\n",
    "  ```\n",
    "  Maximize F1                     # optimizing metric\n",
    "  subject to running time < 100ms # satisficing metric\n",
    "  ```\n",
    "- So as a general rule:\n",
    "  ```\n",
    "  Maximize 1     # optimizing metric (one optimizing metric)\n",
    "  subject to N-1 # satisficing metric (N-1 satisficing metrics)\n",
    "  ```\n",
    "\n",
    "### Train/dev/test distributions\n",
    "\n",
    "- Dev and test sets have to come from the same distribution.\n",
    "- Choose dev set and test set to reflect data you expect to get in the future and consider important to do well on.\n",
    "- Setting up the dev set, as well as the validation metric is really defining what target you want to aim at.\n",
    "\n",
    "### Size of the dev and test sets\n",
    "\n",
    "- An old way of splitting the data was 70% training, 30% test or 60% training, 20% dev, 20% test. \n",
    "- The old way was valid for a number of examples ~ <100000 \n",
    "- In the modern deep learning if you have a million or more examples a reasonable split would be 98% training, 1% dev, 1% test. \n",
    "\n",
    "### When to change dev/test sets and metrics\n",
    "\n",
    "- Let's take an example. In a cat classification example we have these metric results:\n",
    "\n",
    "  | Metric      | Classification error                                         |\n",
    "  | ----------- | ------------------------------------------------------------ |\n",
    "  | Algorithm A | 3% error (But a lot of porn images are treated as cat images here) |\n",
    "  | Algorithm B | 5% error                                                     |\n",
    "  - In the last example if we choose the best algorithm by metric it would be \"A\", but if the users decide it will be \"B\"\n",
    "  - Thus in this case, we want and need to change our metric. \n",
    "  - `OldMetric = (1/m) * sum(y_pred[i] != y[i] ,m)`\n",
    "    - Where m is the number of Dev set items.\n",
    "  - `NewMetric = (1/sum(w[i])) * sum(w[i] * (y_pred[i] != y[i]) ,m)`\n",
    "    - where:\n",
    "       - `w[i] = 1                   if x[i] is not porn`\n",
    "       - `w[i] = 10                 if x[i] is porn`\n",
    "\n",
    "- This is actually an example of an orthogonalization where you should take a machine learning problem and break it into distinct steps: \n",
    "\n",
    "  1. Figure out how to define a metric that captures what you want to do - place the target. \n",
    "  2. Worry about how to actually do well on this metric - how to aim/shoot accurately at the target.\n",
    "\n",
    "- Conclusion: if doing well on your metric + dev/test set doesn't correspond to doing well in your application, change your metric and/or dev/test set.\n",
    "\n",
    "### Why human-level performance?\n",
    "\n",
    "- We compare to human-level performance because of two main reasons:\n",
    "  1. Because of advances in deep learning, machine learning algorithms are suddenly working much better and so it has become much more feasible in a lot of application areas for machine learning algorithms to actually become competitive with human-level performance. \n",
    "  2. It turns out that the workflow of designing and building a machine learning system is much more efficient when you're trying to do something that humans can also do.\n",
    "- After an algorithm reaches the human level performance the progress and accuracy slow down.\n",
    "    ![01- Why human-level performance](Images/01-_Why_human-level_performance.png)\n",
    "- You won't surpass an error that's called \"Bayes optimal error\".\n",
    "- There isn't much error range between human-level error and Bayes optimal error.\n",
    "- Humans are quite good at a lot of tasks. So as long as Machine learning is worse than humans, you can:\n",
    "  - Get labeled data from humans.\n",
    "  - Gain insight from manual error analysis: why did a person get it right?\n",
    "  - Better analysis of bias/variance.\n",
    "\n",
    "### Avoidable bias\n",
    "\n",
    "- Suppose that the cat classification algorithm gives these results:\n",
    "\n",
    "  | Humans             | 1%   | 7.5% |\n",
    "  | ------------------ | ---- | ---- |\n",
    "  | **Training error** | 8%   | 8%   |\n",
    "  | **Dev Error**      | 10%  | 10%  |\n",
    "  - In the left example, because the human level error is 1% then we have to focus on the **bias**.\n",
    "  - In the right example, because the human level error is 7.5% then we have to focus on the **variance**.\n",
    "  - The human-level error as a proxy (estimate) for Bayes optimal error. Bayes optimal error is always less (better), but human-level in most cases is not far from it.\n",
    "  - You can't do better then Bayes error unless you are overfitting.\n",
    "  - `Avoidable bias = Training error - Human (Bayes) error`\n",
    "  - `Variance = Dev error - Training error`\n",
    "\n",
    "### Understanding human-level performance\n",
    "\n",
    "- When choosing human-level performance, it has to be chosen in the terms of what you want to achieve with the system.\n",
    "- You might have multiple human-level performances based on the human experience. Then you choose the human-level performance (proxy for Bayes error) that is more suitable for the system you're trying to build.\n",
    "- Improving deep learning algorithms is harder once you reach a human-level performance.\n",
    "- Summary of bias/variance with human-level performance:\n",
    "  1. human-level error (proxy for Bayes error)\n",
    "     - Calculate `avoidable bias = training error - human-level error`\n",
    "     - If **avoidable bias** difference is the bigger, then it's *bias* problem and you should use a strategy for **bias** resolving.\n",
    "  2. training error\n",
    "     - Calculate `variance = dev error - training error`\n",
    "     - If **variance** difference is bigger, then you should use a strategy for **variance** resolving.\n",
    "  3. Dev error\n",
    "- So having an estimate of human-level performance gives you an estimate of Bayes error. And this allows you to more quickly make decisions as to whether you should focus on trying to reduce a bias or trying to reduce the variance of your algorithm.\n",
    "- These techniques will tend to work well until you surpass human-level performance, whereupon you might no longer have a good estimate of Bayes error that still helps you make this decision really clearly. \n",
    "\n",
    "### Surpassing human-level performance\n",
    "\n",
    "- In some problems, deep learning has surpassed human-level performance. Like:\n",
    "  - Online advertising.\n",
    "  - Product recommendation.\n",
    "  - Loan approval.\n",
    "- The last examples are not natural perception task, rather learning on structural data. Humans are far better in natural perception tasks like computer vision and speech recognition.\n",
    "- It's harder for machines to surpass human-level performance in natural perception task. But there are already some systems that achieved it.\n",
    "\n",
    "### Improving your model performance\n",
    "\n",
    "- The two fundamental asssumptions of supervised learning:\n",
    "  1. You can fit the training set pretty well. This is roughly saying that you can achieve low **avoidable bias**. \n",
    "  2. The training set performance generalizes pretty well to the dev/test set. This is roughly saying that **variance** is not too bad.\n",
    "- To improve your deep learning supervised system follow these guidelines:\n",
    "  1. Look at the difference between human level error and the training error - **avoidable bias**.\n",
    "  2. Look at the difference between the dev/test set and training set error - **Variance**.\n",
    "  3. If **avoidable bias** is large you have these options:\n",
    "     - Train bigger model.\n",
    "     - Train longer/better optimization algorithm (like Momentum, RMSprop, Adam).\n",
    "     - Find better NN architecture/hyperparameters search.\n",
    "  4. If **variance** is large you have these options:\n",
    "     - Get more training data.\n",
    "     - Regularization (L2, Dropout, data augumentation).\n",
    "     - Find better NN architecture/hyperparameters search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
