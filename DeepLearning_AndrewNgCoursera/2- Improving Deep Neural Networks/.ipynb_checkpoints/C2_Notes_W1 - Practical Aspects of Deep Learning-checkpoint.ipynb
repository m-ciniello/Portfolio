{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2_Notes_W1 - Practical Aspects of Deep Learning\n",
    "\n",
    "This is the second course of the deep learning specialization at [Coursera](https://www.coursera.org/specializations/deep-learning) which is moderated by [DeepLearning.ai](http://deeplearning.ai/). The course is taught by Andrew Ng.\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* [1. Course Summary](#train--dev--test-sets)\n",
    "* [2. Train / Dev / Test sets](#train--dev--test-sets)\n",
    "* [3. Bias / Variance](#bias--variance)\n",
    "* [4. Basic Recipe for Machine Learning](#basic-recipe-for-machine-learning)\n",
    "* [5. Regularization](#regularization)\n",
    "* [6. Why regularization reduces overfitting?](#why-regularization-reduces-overfitting)\n",
    "* [7. Dropout Regularization](#dropout-regularization)\n",
    "* [8. Understanding Dropout](#understanding-dropout)\n",
    "* [9. Other regularization methods](#other-regularization-methods)\n",
    "* [10. Normalizing inputs](#normalizing-inputs)\n",
    "* [11. Vanishing / Exploding gradients](#vanishing--exploding-gradients)\n",
    "* [12. Weight Initialization for Deep Networks](#weight-initialization-for-deep-networks)\n",
    "* [13. Numerical approximation of gradients](#numerical-approximation-of-gradients)\n",
    "* [14. Gradient checking implementation notes](#gradient-checking-implementation-notes)\n",
    "* [15. Initialization summary](#initialization-summary)\n",
    "* [16. Regularization summary](#regularization-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Course summary\n",
    "\n",
    "Here are the course summary as its given on the course [link](https://www.coursera.org/learn/deep-neural-network):\n",
    "\n",
    "> This course will teach you the \"magic\" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. \n",
    ">\n",
    "> After 3 weeks, you will: \n",
    "> - Understand industry best-practices for building deep learning applications. \n",
    "> - Be able to effectively use the common neural network \"tricks\", including initialization, L2 and dropout regularization, Batch normalization, gradient checking, \n",
    "> - Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. \n",
    "> - Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance\n",
    "> - Be able to implement a neural network in TensorFlow. \n",
    ">\n",
    "> This is the second course of the Deep Learning Specialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train / Dev / Test sets\n",
    "\n",
    "- Its impossible to get all your hyperparameters right on a new application from the first time.\n",
    "- So the idea is you go through the loop: `Idea ==> Code ==> Experiment`.\n",
    "- You have to go through the loop many times to figure out your hyperparameters.\n",
    "- Your data will be split into three parts:\n",
    "  - Training set.       (Has to be the largest set)\n",
    "  - Hold-out cross validation set / Development or \"dev\" set.\n",
    "  - Testing set.\n",
    "- You will try to build a model upon training set then try to optimize hyperparameters on dev set as much as possible. Then after your model is ready you try and evaluate the testing set.\n",
    "- so the trend on the ratio of splitting the models:\n",
    "  - If size of the  dataset is 100 to 1000000  ==> 60/20/20\n",
    "  - If size of the  dataset is 1000000  to INF  ==> 98/1/1 or  99.5/0.25/0.25\n",
    "      - Main idea here is that, if you have 10 million observations, you may only need like 0.1% (10 thousand) to do the evaluation (test or hold-out set)\n",
    "- The trend now gives the training data the biggest sets.\n",
    "- Make sure the dev and test set are coming from the same distribution.\n",
    "  - For example if cat training pictures is from the web and the dev/test pictures are from users cell phone they will mismatch. It is better to make sure that dev and test set are from the same distribution.\n",
    "- The dev set rule is to try them on some of the good models you've created.\n",
    "- Its OK to only have a dev set without a testing set. But a lot of people in this case call the dev set as the test set. A better terminology is to call it a dev set as its used in the development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bias / Variance\n",
    "\n",
    "- Bias / Variance techniques are Easy to learn, but difficult to master.\n",
    "- So here the explanation of Bias / Variance:\n",
    "  - If your model is underfitting (logistic regression of non linear data) it has a \"high bias\"\n",
    "  - If your model is overfitting then it has a \"high variance\"\n",
    "  - Your model will be alright if you balance the Bias / Variance\n",
    "  - For more:\n",
    "    - ![](Images/01-_Bias_-_Variance.png)\n",
    "- Another idea to get the bias /  variance if you don't have a 2D plotting mechanism:\n",
    "  - High variance (overfitting) for example:\n",
    "    - Training error: 1%\n",
    "    - Dev error: 11%\n",
    "  - high Bias (underfitting) for example:\n",
    "    - Training error: 15%\n",
    "    - Dev error: 14%\n",
    "  - high Bias (underfitting) && High variance (overfitting) for example:\n",
    "    - Training error: 15%\n",
    "    - Test error: 30%\n",
    "  - Best (low bias and low variance):\n",
    "    - Training error: 0.5%\n",
    "    - Test error: 1%\n",
    "  - These Assumptions came from that human has 0% error. If the problem isn't like that you'll need to use human error as baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Basic Recipe for Machine Learning\n",
    "\n",
    "#### Does you model have high bias? (underfitting):\n",
    "  - Try to make your NN bigger (size of hidden units, number of layers)\n",
    "  - Try a different model that is suitable for your data.\n",
    "  - Try to run it longer.\n",
    "  - Different (advanced) optimization algorithms.\n",
    "  - Generally you try these until you can fit at least the training data pretty well... \"until you reduce bias to an acceptable level\"\n",
    "\n",
    "#### Does your model have high variance (overfitting or poor generalization)?\n",
    "  - More data.\n",
    "  - Try regularization.\n",
    "  - Try a different model that is suitable for your data.\n",
    "  - **You should try the previous two points until you have a low bias and low variance.**\n",
    "  \n",
    "In the older days before deep learning, there was a \"Bias/variance tradeoff\". But because now you have more options/tools for solving the bias and variance problem its really helpful to use deep learning... deep learning (particularily using bigger networks and adding more data) can allow us to reduce our bias and variance at the same time\n",
    "- NOTE: Training a bigger neural network never hurts. The main cost is just longer training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Regularization\n",
    "\n",
    "- Adding regularization to NN will help it reduce variance (overfitting)\n",
    "- L1 matrix norm:\n",
    "  - `||W|| = Sum(|w[i,j]|)  # sum of absolute values of all w`\n",
    "- L2 matrix norm because of arcane technical math reasons is called Frobenius norm:\n",
    "  - `||W||^2 = Sum(|w[i,j]|^2)\t# sum of all w squared`\n",
    "  - Also can be calculated as `||W||^2 = W.T * W`\n",
    "- Regularization for logistic regression:\n",
    "  - The normal cost function that we want to minimize is: `J(w,b) = (1/m) * Sum(L(y(i),y'(i)))`\n",
    "  - The L2 regularization version: `J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|^2)`\n",
    "  - The L1 regularization version: `J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|)`\n",
    "  - The L1 regularization version makes a lot of w values become zeros, which makes the model size smaller.\n",
    "  - L2 regularization is being used much more often.\n",
    "  - `lambda` here is the regularization parameter (hyperparameter)\n",
    "- Regularization for NN:\n",
    "  - The normal cost function that we want to minimize is:   \n",
    "    `J(W1,b1...,WL,bL) = (1/m) * Sum(L(y(i),y'(i)))`\n",
    "\n",
    "  - The L2 regularization version:   \n",
    "    `J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum((||W[l]||^2)`\n",
    "\n",
    "  - We stack the matrix as one vector `(mn,1)` and then we apply `sqrt(w1^2 + w2^2.....)`\n",
    "\n",
    "  - To do back propagation (old way):   \n",
    "    `dw[l] = (from back propagation)`\n",
    "\n",
    "  - The new way:   \n",
    "    `dw[l] = (from back propagation) + lambda/m * w[l]`\n",
    "\n",
    "  - So plugging it in weight update step:\n",
    "\n",
    "    - ```\n",
    "      w[l] = w[l] - learning_rate * dw[l]\n",
    "           = w[l] - learning_rate * ((from back propagation) + lambda/m * w[l])\n",
    "           = w[l] - (learning_rate*lambda/m) * w[l] - learning_rate * (from back propagation) \n",
    "           = (1 - (learning_rate*lambda)/m) * w[l] - learning_rate * (from back propagation)\n",
    "      ```\n",
    "\n",
    "  - **In practice this penalizes large weights and effectively limits the freedom in your model.**\n",
    "\n",
    "  - The new term `(1 - (learning_rate*lambda)/m) * w[l]`  causes the **weight to decay** in proportion to its size.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm using numpy: 2.86153762764\n",
      "Norm calcd manually: 2.86153762764\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Frobenius norm is just the sqrt of each squared term in the matrix... \n",
    "# ... Its basically the L2 norm but for matrices.\n",
    "#  Numpys default norm for \n",
    "W=np.random.rand(5,5)\n",
    "print('Norm using numpy:', np.linalg.norm(W, 'fro'))\n",
    "\n",
    "squared = W.reshape(1,-1) * W.reshape(1,-1)\n",
    "print('Norm calcd manually:', np.sqrt(squared.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Why regularization reduces overfitting?\n",
    "\n",
    "So why is it that if we reduce the L2 norm (or L1 norm) of the weight vectors, we reduce overfitting? The bais intuition is that if we restrict the weight sizes, then we are inherently reducing the freedom of the model. We control how much we want to regularize wi the lambda hyperparameter:\n",
    "  - Intuition 1:\n",
    "     - If `lambda` is too large - a lot of w's will be close to zeros which will make the NN simpler (you can think of it as it would behave closer to logistic regression). \n",
    "     - You actually dont get any weights that are literally zeroed out, but many become so close to zero, so they effectively dont really make an impact on the model, resulting in a much simpler model.\n",
    "     - If `lambda` is perfect it will just reduce some weights that makes the neural network overfit.\n",
    "  - Intuition 2 (with _tanh_ activation function):\n",
    "     - If `lambda` is too large, w's will be small (close to zero) - will use the linear part of the _tanh_ activation function, so we will go from non linear activation to _roughly_ linear which would make the NN a _roughly_ linear classifier... because if every layer is linear, then you have a linear model!\n",
    "     - If `lambda` is perfect it will just make SOME of _tanh_ activations _roughly_ linear which will prevent overfitting.\n",
    "     \n",
    "![](images/c2w1_regs.png)\n",
    "     \n",
    "_**Implementation tip**_: if you implement gradient descent, one of the steps to debug gradient descent is to plot the cost function J as a function of the number of iterations of gradient descent and you want to see that the cost function J decreases **monotonically** after every elevation of gradient descent with regularization. If you plot the old definition of J (no regularization) then you might not see it decrease monotonically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Dropout Regularization\n",
    "\n",
    "- In most cases Andrew Ng tells that he uses the L2 regularization.\n",
    "- The dropout regularization eliminates some neurons/weights on each iteration based on a probability.\n",
    "- A most common technique to implement dropout is called \"Inverted dropout\".\n",
    "- Code for Inverted dropout:\n",
    "\n",
    "  ```python\n",
    "  keep_prob = 0.8   # 0 <= keep_prob <= 1\n",
    "  l = 3  # this code is only for layer 3\n",
    "  # the generated number that are less than 0.8 will be dropped. 80% stay, 20% dropped\n",
    "  d3 = np.random.randn(a[l].shape[0], a[l].shape[1]) < keep_prob\n",
    "\n",
    "  a3 = np.multiply(a3,d3)   # keep only the values in d3\n",
    "  # NOTE THIS IS ELEMENT WISE MULTIPLICAITON THAT ZEROES OUT DROPPED NEURONS\n",
    "\n",
    "  # increase a3 to not reduce the EXPECTED VALUE of output\n",
    "  # (ensures that the expected value of a3 remains the same) - to solve the scaling problem... \n",
    "  # THIS IS THE INVERTED DROPOUT TECHNIQUE!!! \n",
    "  a3 = a3 / keep_prob       \n",
    "  ```\n",
    "- Vector d[l] is used for forward and back propagation and is the same for them, but it is different for each iteration/epoch (each run through ALL of the training examples)\n",
    "- At test time we DONT USE DROPOUT. If you implement dropout at test time - it would just add noise to predictions and make your model suck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Understanding Dropout\n",
    "\n",
    "- In the previous video, the intuition was that dropout randomly knocks out units in your network. So it's as if on every iteration you're working with a smaller NN, and so using a smaller NN seems like it should have a regularizing effect.\n",
    "- Another intuition: **individual units cannot rely on any one feature, so have to spread out weights.**\n",
    "    - Because weights are spread out, this has the effect of shrinking the **shared norm** of the weights... This is simlar to the effect of L2 regulatization!\n",
    "    - It is possible to show that dropout has a similar effect as L2 regulatization\n",
    "- It is possible to have different `keep_prob` values for each per layer of your network.\n",
    "    - So for larger layers (with more neurons and a higher probability of overfitting) you can have a lower keep_prob (eg. 0.5), and for layers that you are not worried about overfitting you can set keep_prob very close to 1.\n",
    "- The input layer dropout has to be near 1 (or 1 - no dropout) because you don't want to eliminate a lot of features.\n",
    "- If you're more worried about some layers overfitting than others, you can set a lower `keep_prob` for some layers than others. The downside is, this gives you even more hyperparameters to search for using cross-validation. One other alternative might be to have some layers where you apply dropout and some layers where you don't apply dropout and then just have one hyperparameter, which is a `keep_prob` for the layers for which you do apply dropouts.\n",
    "- A lot of researchers are using dropout with Computer Vision (CV) because they have a very big input size and almost never have enough data, so overfitting is the usual problem. And dropout is a regularization technique to prevent overfitting.\n",
    "- A downside of dropout is that the cost function J is not well defined and it will be hard to debug (plot J by iteration).\n",
    "  - To solve that you'll need to turn off dropout, set all the `keep_prob`s to 1, and then run the code and check that it monotonically decreases J and then turn on the dropouts again.\n",
    "  \n",
    "**KEY: Dropout is a regulaization technique... So if your model is not really overfitting, then you probably dont't need to use dropout!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Other regularization methods\n",
    "\n",
    "- **Data augmentation**:\n",
    "  - For example in a computer vision data:\n",
    "    - You can flip all your pictures horizontally this will give you m more data instances.\n",
    "    - You could also apply a random position and rotation to an image to get more data.\n",
    "  - For example in OCR, you can impose random rotations and distortions to digits/letters.\n",
    "  - New data obtained using this technique isn't as good as the real independent data, but still can be used as a regularization technique.\n",
    "- **Early stopping**:\n",
    "  - In this technique we plot the training set and the dev set cost together for each iteration. At some iteration the dev set cost will stop decreasing and will start increasing.\n",
    "  - We will pick the point at which the training set error and dev set error are best (lowest training cost with lowest dev cost).\n",
    "  - We will take these parameters as the best parameters.\n",
    "      - When we start training (iterations 1), W params will be very close to zero. As we increase iterations W will increase, but at a certain point W will grow too larger and start to overfit the training data (this is why regularization is important as it limits the size of weights).\n",
    "      - Early stopping essentially means we are picking a W with a smaller norm ||W||2F.\n",
    "\n",
    "![](Images/02-_Early_stopping.png)\n",
    "- Andrew prefers to use L2 regularization instead of early stopping because this technique simultaneously tries to minimize the cost function and not to overfit which contradicts the orthogonalization approach (will be discussed further).\n",
    "  - But its advantage is that you don't need to search a hyperparameter like in other regularization approaches (like `lambda` in L2 regularization).\n",
    "\n",
    "- **Model Ensembles**:\n",
    "  - Algorithm:\n",
    "    - Train multiple independent models.\n",
    "    - At test time average their results.\n",
    "  - It can get you extra 2% performance.\n",
    "  - It reduces the generalization error.\n",
    "  - You can use some snapshots of your NN at the training ensembles them and take the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Normalizing inputs\n",
    "\n",
    "#### If you normalize your inputs this will speed up the training process a lot.\n",
    "- Normalization has two main steps:\n",
    "  1. **Zero out the mean**:\n",
    "      - Get the mean of the training set: `mean = (1/m) * sum(x(i))`\n",
    "      - Subtract the mean from each input: `X = X - mean`\n",
    "      - This makes your inputs centered around 0.\n",
    "  2. **Normalize the variance:**\n",
    "      - Get the variance of the training set: `variance = (1/m) * sum(x(i)^2)`\n",
    "      - Normalize the variance. `X /= variance`\n",
    "      - This will make the variances equal... so notice in graph below (at step 2, x2 has a much larger variance, and in step 3 the variance appears relatively equal!)\n",
    "\n",
    "![](images/c2w1_normalize.png)\n",
    "- These steps should be applied to training, dev, and testing sets (**but always use the mean and variance of the TRAIN SET ONLY).**\n",
    "    - **REPEAT: USE THE SAME VARIANCE AND MEAN WHEN SCALING (NORMALIZING) THE DEV AND TEST SETS!!!**\n",
    "\n",
    "#### Why normalize?\n",
    "  - If we don't normalize the inputs (and our features are on very different scales) our cost function will be deep and its shape will be inconsistent (elongated) then optimizing it will take a long time.\n",
    "  - But if we normalize it the opposite will occur. The shape of the cost function will be consistent (look more symmetric like circle in 2D example) and **we can use a larger learning rate alpha - the optimization will be faster.**\n",
    "  - This is particularily important when features comes from crazy different scales... However it pretty much never hurts to normalize, so it makes sense to just do it either way.\n",
    "![](images/c2w1_normalize2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Vanishing / Exploding gradients\n",
    "\n",
    "- The Vanishing / Exploding gradients occurs when your derivatives become very small or very big.\n",
    "- To understand the problem, suppose that we have a deep neural network with number of layers L, and all the activation functions are **linear** and each `b = 0`\n",
    "  - Then:   \n",
    "    ```\n",
    "    Y' = W[L]W[L-1].....W[2]W[1]X\n",
    "    ```\n",
    "  - Then, if we have 2 neurons per layer and x1 = x2 = 1, we get the following:\n",
    "\n",
    "    ```\n",
    "    if W[l] = [1.5   0] \n",
    "              [0   1.5] (l != L because of different dimensions in the output layer)\n",
    "    Y' = W[L] [1.5  0]^(L-1) X = 1.5^L \t# which will be very large\n",
    "              [0  1.5]\n",
    "    ```\n",
    "    ```\n",
    "    if W[l] = [0.5  0]\n",
    "              [0  0.5]\n",
    "    Y' = W[L] [0.5  0]^(L-1) X = 0.5^L \t# which will be very small\n",
    "              [0  0.5]\n",
    "    ```\n",
    "\n",
    "#### The last example explains that activation outputs at each layer (and similarly derivatives) will progressively decreas/increase exponentially AS A FUNCTION OF THE NUMBER OF LAYERS (L)...\n",
    "\n",
    "- So If **W IS GREARTHER THAN I (Identity matrix)** the activation and gradients will explode.\n",
    "- And If **W IS LESS THAN I (Identity matrix)** the activation and gradients will vanish.\n",
    "\n",
    "#### The following examples is replicated (somewhat) in code below!\n",
    "![](images/c2w1_explodgrads.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (3, 1)\n",
      "Wl shape:  (3, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "identity = np.array(([1,0,0],[0,1,0], [0,0,1]))\n",
    "X = np.array(([1],[1],[1]))\n",
    "Wl_explode = np.array(([1.5,0,0],[0,1.5,0],[0,0,1.5]))\n",
    "Wl_vanish = np.array(([0.5,0,0],[0,0.5,0],[0,0,0.5]))\n",
    "\n",
    "print(\"X shape: \",X.shape)\n",
    "print(\"Wl shape: \",Wl_explode.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.25]\n",
      " [ 2.25]\n",
      " [ 2.25]]\n",
      "[[ 3.375]\n",
      " [ 3.375]\n",
      " [ 3.375]]\n",
      "[[ 5.0625]\n",
      " [ 5.0625]\n",
      " [ 5.0625]]\n",
      "[[ 7.59375]\n",
      " [ 7.59375]\n",
      " [ 7.59375]]\n",
      "[[ 11.390625]\n",
      " [ 11.390625]\n",
      " [ 11.390625]]\n",
      "[[ 17.0859375]\n",
      " [ 17.0859375]\n",
      " [ 17.0859375]]\n",
      "[[ 25.62890625]\n",
      " [ 25.62890625]\n",
      " [ 25.62890625]]\n",
      "[[ 38.44335938]\n",
      " [ 38.44335938]\n",
      " [ 38.44335938]]\n",
      "[[ 57.66503906]\n",
      " [ 57.66503906]\n",
      " [ 57.66503906]]\n"
     ]
    }
   ],
   "source": [
    "# EXPLODING GRADIENT\n",
    "layer = np.dot(Wl_explode,X)\n",
    "for l in range(1,10):\n",
    "    layer = np.dot(Wl_explode,layer)\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25]\n",
      " [ 0.25]\n",
      " [ 0.25]]\n",
      "[[ 0.125]\n",
      " [ 0.125]\n",
      " [ 0.125]]\n",
      "[[ 0.0625]\n",
      " [ 0.0625]\n",
      " [ 0.0625]]\n",
      "[[ 0.03125]\n",
      " [ 0.03125]\n",
      " [ 0.03125]]\n",
      "[[ 0.015625]\n",
      " [ 0.015625]\n",
      " [ 0.015625]]\n",
      "[[ 0.0078125]\n",
      " [ 0.0078125]\n",
      " [ 0.0078125]]\n",
      "[[ 0.00390625]\n",
      " [ 0.00390625]\n",
      " [ 0.00390625]]\n",
      "[[ 0.00195312]\n",
      " [ 0.00195312]\n",
      " [ 0.00195312]]\n",
      "[[ 0.00097656]\n",
      " [ 0.00097656]\n",
      " [ 0.00097656]]\n"
     ]
    }
   ],
   "source": [
    "# VANISHING GRADIENT\n",
    "layer = np.dot(Wl_vanish,X)\n",
    "for l in range(1,10):\n",
    "    layer = np.dot(Wl_vanish,layer)\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Recently Microsoft trained 152 layers (ResNet)! which is a really big number. With such a deep neural network, if your activations or gradients increase or decrease exponentially as a function of L, then these values could get really big or really small. And this makes training difficult, especially if your gradients are exponentially smaller than L, **then gradient descent will take tiny little steps. It will take a long time for gradient descent to learn anything.**\n",
    "- There is a partial solution that doesn't completely solve this problem but it helps a lot - careful choice of how you initialize the weights (next video)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Weight Initialization for Deep Networks\n",
    "\n",
    "- A partial solution to the Vanishing / Exploding gradients in NN is better or more careful choice of the random initialization of weights\n",
    "- In a single neuron (Perceptron model): `Z = w1x1 + w2x2 + ... + wnxn`\n",
    "  - So if `n_x` is large we want `W`'s to be smaller to not explode the cost.\n",
    "\n",
    "#### So it turns out that we need the variance of `W` to be `1/n_x` \n",
    "- A few notes: \n",
    "    - Multiplying a random variable 'x' by the square root of 'v' will set the variance of 'x' to v. This is what we are doing below. \n",
    "    - Also we use n[l-1] for the inputs, because the number of inputs (columns of the weight vector) is equal to the number of neuonrs in the previous layer... because the shape each weight vector vector is (n[nl], n[l-1])\n",
    "\n",
    "\n",
    "- So lets say when we initialize `W`'s like this (better to use with `tanh` activation):   \n",
    "  ```\n",
    "  np.random.rand(shape) * np.sqrt(1/n[l-1])\n",
    "  ```\n",
    "  or variation of this (Bengio et al.):   \n",
    "  ```\n",
    "  np.random.rand(shape) * np.sqrt(2/(n[l-1] + n[l]))\n",
    "  ```\n",
    "- Setting initialization part inside sqrt to `2/n[l-1]` for `ReLU` is better:   \n",
    "  ```\n",
    "  np.random.rand(shape) * np.sqrt(2/n[l-1])\n",
    "  ```\n",
    "\n",
    "\n",
    "- Number 1 or 2 in the nominator can also be a hyperparameter to tune (but not the first to start with)\n",
    "- This is one of the best way of partially solution to Vanishing / Exploding gradients (ReLU + Weight Initialization with variance) which will help gradients not to vanish/explode too quickly\n",
    "- The initialization in this video is called \"He Initialization / Xavier Initialization\" and has been published in 2015 paper.\n",
    "\n",
    "#### So in summation, weight initialization helps prevent your weights from exploding or from vanishing too quickly so you can train a reasonably deep network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.410106237724\n"
     ]
    }
   ],
   "source": [
    "nl_prev = 5\n",
    "W1_neurons = 5\n",
    "DESIRED_VARIANCE = 2/nl_prev\n",
    "\n",
    "# SAMPLEE GAUSIAN RANDOM VARIABLE\n",
    "W_pre = np.random.randn(W1_neurons, nl_prev)\n",
    "\n",
    "# SET VARIANCE TO DESIRED VARIANCE BY USING SQUARE ROOT RULE\n",
    "W_post = W_pre * np.sqrt(DESIRED_VARIANCE)\n",
    "\n",
    "print(DESIRED_VARIANCE)\n",
    "print(np.var(W_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Numerical approximation of gradients\n",
    "\n",
    "- There is an technique called gradient checking which tells you if your implementation of backpropagation is correct.\n",
    "- There's a numerical way to calculate the derivative:   \n",
    "  ![](Images/03-_Numerical_approximation_of_gradients.png)\n",
    "- Gradient checking approximates the gradients and is very helpful for finding the errors in your backpropagation implementation but it's slower than gradient descent (so use only for debugging).\n",
    "- Implementation of this is very simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Gradient checking implementation notes\n",
    "\n",
    "#### Gradient checking Implementation\n",
    "- Our goal is simply to figure out if `d_theta` is indeed the gradient of `J(theta)`\n",
    "- NOTE: W1 and dW1 should have same dimensions. So theta and d_theta will have same dimensions.\n",
    "\n",
    "  - First take `W[1],b[1],...,W[L],b[L]` and reshape into one big vector (`theta`)\n",
    "  - The cost function will be `J(theta)`... instead of `J(W1,b1,...Wn,bn)`\n",
    "  - Then take `dW[1],db[1],...,dW[L],db[L]` into one big vector (`d_theta`)\n",
    "      \n",
    " \n",
    "- **Algorithm**:   \n",
    "```\n",
    "eps = 10^-7   # small number\n",
    "for i in len(theta):\n",
    "      d_theta_approx[i] = (J(theta1,...,theta[i] + eps) -  J(theta1,...,theta[i] - eps)) / 2*eps\n",
    "```\n",
    "- Finally we evaluate this formula `(||d_theta_approx - d_theta||) / (||d_theta_approx||+||d_theta||)` (`||` - Euclidean vector norm) and check (with eps = 10^-7):\n",
    "\n",
    "\n",
    "- if it is < 10^-7  - great, very likely the backpropagation implementation is correct\n",
    "- if around 10^-5   - can be OK, but need to inspect if there are no particularly big values in `d_theta_approx - d_theta` vector\n",
    "- if it is >= 10^-3 - bad, probably there is a bug in backpropagation implementation\n",
    "\n",
    "- Don't use the gradient checking algorithm at training time because it's very slow.\n",
    "- Use gradient checking only for debugging.\n",
    "- If algorithm fails grad check, look at components (a specific W or b value) to try to identify the bug.\n",
    "- Don't forget to add `lamda/(2m) * sum(W[l])` to `J` if you are using L1 or L2 regularization.\n",
    "- Gradient checking doesn't work with dropout because J is not consistent. \n",
    "  - You can first turn off dropout (set `keep_prob = 1.0`), run gradient checking and then turn on dropout again.\n",
    "- Run gradient checking at random initialization and train the network for a while maybe there's a bug which can be seen when w's and b's become larger (further from 0) and can't be seen on the first iteration (when w's and b's are very small)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Initialization summary\n",
    "\n",
    "- The weights $W^{[l]}$ should be initialized randomly to break symmetry\n",
    "\n",
    "- It is however okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly\n",
    "\n",
    "- Different initializations lead to different results\n",
    "\n",
    "- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n",
    "\n",
    "- Don't intialize to values that are too large\n",
    "\n",
    "- He initialization works well for networks with ReLU activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Regularization summary\n",
    "\n",
    "#### 1. L2 Regularization\n",
    "\n",
    "**Observations**:\n",
    "\n",
    "- The value of Î» is a hyperparameter that you can tune using a dev set.\n",
    "- L2 regularization makes your decision boundary smoother. If Î» is too large, it is also possible to \"oversmooth\", resulting in a model with high bias.\n",
    "\n",
    "**What is L2-regularization actually doing?**:\n",
    "\n",
    "**L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights.** Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.\n",
    "\n",
    "**What you should remember:** the implications of L2-regularization on:\n",
    "\n",
    "- The cost computation:\n",
    "  - A regularization term is added to the cost\n",
    "- The backpropagation function:\n",
    "  - There are extra terms in the gradients with respect to weight matrices\n",
    "- Weights end up smaller (\"weight decay\"):\n",
    "  - Weights are pushed to smaller values.\n",
    "\n",
    "####  2. Dropout\n",
    "\n",
    "**What you should remember about dropout:**\n",
    "\n",
    "- Dropout is a regularization technique.\n",
    "- You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.\n",
    "- Apply dropout both during forward and backward propagation.\n",
    "- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
