{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2_Notes_W3 - Hyperparams Tuning, Batch Norm, and Programming Frameworks\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* [1. Tuning process](#tuning-process)\n",
    "* [2. Using an appropriate scale to pick hyperparameters](#using-an-appropriate-scale-to-pick-hyperparameters)\n",
    "* [3. Hyperparameters tuning in practice: Pandas vs. Caviar](#hyperparameters-tuning-in-practice-pandas-vs-caviar)\n",
    "* [4. Normalizing activations in a network](#normalizing-activations-in-a-network)\n",
    "* [5. Fitting Batch Normalization into a neural network](#fitting-batch-normalization-into-a-neural-network)\n",
    "* [6. Why does Batch normalization work?](#why-does-batch-normalization-work)\n",
    "* [7. Batch normalization at test time](#batch-normalization-at-test-time)\n",
    "* [8. Softmax Regression](#softmax-regression)\n",
    "* [9. Training a Softmax classifier](#training-a-softmax-classifier)\n",
    "* [10. Deep learning frameworks](#deep-learning-frameworks)\n",
    "* [11. TensorFlow](#tensorflow)\n",
    "* [12. Extra Notes](#extra-notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tuning process\n",
    "\n",
    "**We need to tune our hyperparameters to get the best out of them:**\n",
    "- Hyperparameters importance are (as for Andrew Ng):\n",
    "  1. **Learning rate (MOST IMPORTANT)**\n",
    "  2. Mini-batch size.\n",
    "  3. No. of hidden units.\n",
    "  4. No. of layers.\n",
    "  5. Learning rate decay.\n",
    "  6. Regularization lambda.\n",
    "  7. Activation functions.\n",
    "  8. **Adam beta1, beta2, epsilon (Momentum beta (0.9), RMSprop beta(0.99) and epsilon (10^-8))**... These are generally fixed. \n",
    "- Its hard to decide which hyperparameter is the most important in a problem. It depends a lot on your problem.\n",
    "- One of the ways to tune is to sample a grid with `N` hyperparameter settings and then try all settings combinations on your problem.\n",
    "\n",
    "**Try random values, DONT USE A GRID:**\n",
    "- Choose points at random because it is difficult to know in advance which hyperparams are going to be best for your problem. \n",
    "- And generally some hyperparams are more important than others... \n",
    "\n",
    "**You can use COURSE TO FINE sampling scheme:**\n",
    "- When you find some hyperparameters values that give you a better performance - zoom into a smaller region around these values and sample more densely within this space.\n",
    "- These methods can be automated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using an appropriate scale to pick hyperparameters\n",
    "\n",
    "- Let's say you have a specific range for a hyperparameter from \"a\" to \"b\".\n",
    "- If you are trying to search for a optimal number of layers or neurons, where the hyperparam is an integer within a small range (like 10-20), then you could probably just do a GridSearchCV... cuz there arent that many options here.\n",
    "- **HOWEVER, if you are trying to choose the the appropriate alpha term (from 0.0001 to 0.1) then it's better to search for the right ones using the logarithmic scale rather then in linear scale.**\n",
    "- The reason is, you want your alpha parameter to be ver close to 0.1... an alpha close to 1 is probably not a good alpha. As such, you want to use a logarithmic scale to push the values closer to 0.0001.\n",
    "    - Take the log of the low and high values of your range\n",
    "    - Put these into the following calc:\n",
    "    \n",
    "    - `r = low * np.random.rand() #This will give random value between #log10(low) and log10(high)`\n",
    "    - `dist = 10^r #for a bunch of random values of r. This will give you your dist`\n",
    "  \n",
    "**For example:**\n",
    "  - Calculate: `low_log = log(low)  # e.g. low = 0.0001 then np.log10(0.001) = -4`\n",
    "  - Calculate: `high_log = log(high)  # e.g. high = 1  then np.log10(1) = 0`\n",
    "  - Then:\n",
    "    ```\n",
    "    r = Uniformly sample random value between log_low and low_high \n",
    "    param_value = 10^r\n",
    "    ```\n",
    "**If we want to use the last method on exploring on the \"momentum beta\":**\n",
    "- Beta best range is from 0.9 to 0.999.\n",
    "- You should search for `1 - beta in range 0.001 to 0.1 (1 - 0.9 and 1 - 0.999)` and the use `a = 0.001` and `b = 0.1`. Then:\n",
    "```\n",
    "a_log = -3\n",
    "b_log = -1\n",
    "# This does same thing as above... gets a randomly sampled value between a_log and b_log\n",
    "r = (a_log - b_log) * np.random.rand() + b_log\n",
    "beta = 1 - 10^r   # because 1 - beta = 10^r\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f092776d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "low=0.0001\n",
    "high=1\n",
    "\n",
    "x_normal = np.random.uniform(low=low, high=high, size=(1000,))\n",
    "plt.hist(x_normal,bins=100, alpha=0.3)\n",
    "\n",
    "r = np.log10(low)*np.random.rand(500)\n",
    "x_logn = [10**i for i in r]\n",
    "x_logn\n",
    "plt.hist(x_logn,bins=100, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameters tuning in practice: Pandas vs. Caviar \n",
    "\n",
    "- Intuitions about hyperparameter settings from one application area may or may not transfer to a different one.\n",
    "- If you don't have much computational resources you can use the \"babysitting model\":\n",
    "  - Day 0 you might initialize your parameter as random and then start training.\n",
    "  - Then you watch your learning curve gradually decrease over the day.\n",
    "  - And each day you nudge your parameters a little during training.\n",
    "  - Called panda approach.\n",
    "- If you have enough computational resources, you can run some models in parallel and at the end of the day(s) you check the results.\n",
    "  - Called Caviar approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Normalizing activations in a network\n",
    "\n",
    "- In the rise of deep learning, one of the most important ideas has been an algorithm called **batch normalization**, created by two researchers, Sergey Ioffe and Christian Szegedy.\n",
    "- Batch Normalization speeds up learning.\n",
    "- Before we normalized input by subtracting the mean and dividing by variance. This helped a lot for the shape of the cost function and for reaching the minimum point faster.\n",
    "- The question is: *for any hidden layer can we normalize `A[l]` to train `W[l]`, `b[l]` faster?* This is what batch normalization is about.\n",
    "- There are some debates in the deep learning literature about whether you should normalize values before the activation function `Z[l]` or after applying the activation function `A[l]`. In practice, normalizing `Z[l]` is done much more often and that is what Andrew Ng presents.\n",
    "- Algorithm:\n",
    "  - Given `Z[l] = [z(1), ..., z(m)]`, i = 1 to m (for each input)\n",
    "  - Compute `mean = 1/m * sum(z[i])`\n",
    "  - Compute `variance = 1/m * sum((z[i] - mean)^2)`\n",
    "  - Then `Z_norm[i] = (z(i) - mean) / np.sqrt(variance + epsilon)` (add `epsilon` for numerical stability if variance = 0)\n",
    "    - Forcing the inputs to a distribution with zero mean and variance of 1.\n",
    " - **Then `Z_tilde[i] = gamma * Z_norm[i] + beta`**. \n",
    "     - Gamma and Beta are the hyperparameters, and are used to make inputs belong to other distribution (with other mean and variance), which the model can learn.\n",
    "     - Making the NN learn the distribution of the outputs.\n",
    "     - _Note:_ if `gamma = sqrt(variance + epsilon)` and `beta = mean` then `Z_tilde[i] = Z_norm[i]`... do the math and see for yourself, its pretty simple!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fitting Batch Normalization into a neural network\n",
    "\n",
    "- Using batch norm in 3 hidden layers NN:\n",
    "    ![](Images/bn.png)\n",
    "- Our NN parameters will be:\n",
    "  - `W[1]`, `b[1]`, ..., `W[L]`, `b[L]`, `beta[1]`, `gamma[1]`, ..., `beta[L]`, `gamma[L]`\n",
    "  - `beta[1]`, `gamma[1]`, ..., `beta[L]`, `gamma[L]` are updated using any optimization algorithms (like GD, RMSprop, Adam)\n",
    "- If you are using a deep learning framework, you won't have to implement batch norm yourself:\n",
    "  - Ex. in Tensorflow you can add this line: `tf.nn.batch-normalization()`\n",
    "- Batch normalization is usually applied with mini-batches.\n",
    "- If we are using batch normalization parameters `b[1]`, ..., `b[L]` doesn't count because they will be eliminated after mean subtraction step, so:\n",
    "  ```\n",
    "  Z[l] = W[l]A[l-1] + b[l] => Z[l] = W[l]A[l-1]\n",
    "  Z_norm[l] = ...\n",
    "  Z_tilde[l] = gamma[l] * Z_norm[l] + beta[l]\n",
    "  ```\n",
    "  - Taking the mean of a constant `b[l]` will eliminate the `b[l]`\n",
    "- So if you are using batch normalization, you can remove b[l] or make it always zero.\n",
    "- So the parameters will be `W[l]`, `beta[l]`, and `alpha[l]`.\n",
    "- Shapes:\n",
    "  - `Z[l]       - (n[l], m)`\n",
    "  - `beta[l]    - (n[l], m)`\n",
    "  - `gamma[l]   - (n[l], m)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Why does Batch normalization work?\n",
    "\n",
    "- The first reason is the same reason as why we normalize X.\n",
    "- The second reason is that batch normalization reduces the problem of input values changing (shifting).\n",
    "- Batch normalization does some regularization:\n",
    "  - Each mini batch is scaled by the mean/variance computed of that mini-batch.\n",
    "  - This adds some noise to the values `Z[l]` within that mini batch. So similar to dropout it adds some noise to each hidden layer's activations.\n",
    "  - This has a slight regularization effect.\n",
    "  - Using bigger size of the mini-batch you are reducing noise and therefore regularization effect.\n",
    "  - Don't rely on batch normalization as a regularization. It's intended for normalization of hidden units, activations and therefore speeding up learning. For regularization use other regularization techniques (L2 or dropout).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Batch normalization at test time\n",
    "\n",
    "- When we train a NN with Batch normalization, we compute the mean and the variance of the mini-batch.\n",
    "- In testing we might need to process examples one at a time. The mean and the variance of one example won't make sense.\n",
    "- We have to compute an estimated value of mean and variance to use it in testing time.\n",
    "- We can use the weighted average across the mini-batches.\n",
    "- We will use the estimated values of the mean and variance to test.\n",
    "- This method is also sometimes called \"Running average\".\n",
    "- In practice most often you will use a deep learning framework and it will contain some default implementation of doing such a thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Softmax Regression\n",
    "\n",
    "- In every example we have used so far we were talking about binary classification.\n",
    "- There are a generalization of logistic regression called Softmax regression that is used for multiclass classification/regression.\n",
    "- For example if we are classifying by classes `dog`, `cat`, `baby chick` and `none of that`\n",
    "  - Dog `class = 1`\n",
    "  - Cat `class = 2`\n",
    "  - Baby chick `class = 3`\n",
    "  - None `class = 0`\n",
    "  - To represent a dog vector `y = [0 1 0 0]`\n",
    "  - To represent a cat vector `y = [0 0 1 0]`\n",
    "  - To represent a baby chick vector `y = [0 0 0 1]`\n",
    "  - To represent a none vector `y = [1 0 0 0]`\n",
    "- Notations:\n",
    "  - `C = no. of classes`\n",
    "  - Range of classes is `(0, ..., C-1)`\n",
    "  - In output layer `Ny = C`\n",
    "- Each of C values in the output layer will contain a probability of the example to belong to each of the classes.\n",
    "- In the last layer we will have to activate the Softmax activation function instead of the sigmoid activation.\n",
    "- Softmax activation equations:\n",
    "  ```\n",
    "  t = e^(Z[L])                      # shape(C, m)\n",
    "  A[L] = e^(Z[L]) / sum(t)          # shape(C, m), sum(t) - sum of t's for each example (shape (1, m))\n",
    "  ``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember, log of x where x is between 0 and 1, x{0,1}, will look like the graph below, producing large negative numbers until it gets very close to 1. Log(1) = 0. This is why this works for softmax, as you take the 'negative log' of the probability... so if a class is a 1, and the probability is LOW, then the cost function will increase! For example:\n",
    "- observation x is class 1\n",
    "- predict probability of 0.001\n",
    "- log(0.001) = -6.90\n",
    "- negative log(0.001) = 6.90\n",
    "- Thus adding a large number to the cost! Because we had a low probability for a class 1!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.9077552789821368"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xt0XGd57/HvI8myLpZkWbIkXyT5\nEl+TGCdR4lwgCYkTkhTIahMg6aEhEHBLDqWlF1qalcIBDqVQyjlwgOKcGmhJA+mhJWkgDRgCKQEn\ncRLbkRM7vlu+yJIsjW4zkkbSc/6YsRG2ZI0lzWzNzO+z1ixpa2b2frYt7d/s9333u83dERERyQm6\nABERmR4UCCIiAigQREQkToEgIiKAAkFEROIUCCIiAigQREQkToEgIiKAAkFEROLygi7gfFRWVvqi\nRYuCLkNEJK28+OKLbe4+d7zXpVUgLFq0iK1btwZdhohIWjGzQ4m8Tk1GIiICKBBERCROgSAiIoAC\nQURE4hQIIiICBBwIZnaLme02s71m9pdB1iIiku0CCwQzywW+AtwKrAbuNrPVQdUjIpLtgrwO4Qpg\nr7vvBzCz7wC3A68GWJOISKD6okO0dvfT0t1Ha3f/6cedl9VSV1GU1G0HGQgLgKYRy0eAdWe+yMw2\nABsA6urqUlOZiMgUcne6+gZp6eqjJX6wb+nqj3/fT0tXH609/bR29dPdP3jW+3MMLqkvz+hAsFF+\n5mf9wH0jsBGgoaHhrOdFRILi7nT3xw70J7piB/oTXf2c6Iod8E/EA+BEVx/9g8Nnvb9wRi5VpTOp\nKpnJqppSrl02k7klseW5Jae+L2BOcT65OaMdMqdWkIFwBKgdsbwQOBZQLSIiv2FgcJgTXX2c6Oqj\nuevXB/oTXX00d/76QB8eGDrrvbNm5lFVMpPq0gIuqZtNdWnB6YP8qe+rSguYNXN6zR4UZDUvAMvM\nbDFwFLgL+N0A6xGRLBEeGOR4Z+zAHvsaOb3cHD/gn+wdOOt9+Xk51JQWUF06kwvnl3LDyiqqS2MH\n+el8oE9UYFW7+6CZfQh4CsgFNrn7zqDqEZHMEBkY4lhnhOOhvtNfm7tiB/zjoT6Od0bo6ju7nb68\naAY1ZYXUlM5kzcIyqksLqCktoKYs9qguKWB20QzMkt90E5RAY8zdfwj8MMgaRCR9DA4Nc6K7n+Oh\nCEdDEY6F+jgWinC8M8LR+ME+FI6e9b7KWfnMKyukrqKIK5fMoaaskHnxA/28stin+4IZuQHs0fSS\nnuc1IpKRwgODHO2IcCQU4VgowtGO+Nf4wb+5q4+h4d8cW1JWOIN5ZQUsmF3IZfWzmVdWyPzZBbGv\nZYVUl81kZp4O9olQIIhIyvT2D3KkI8KRjjBN7WGOdMQO9kdDEY50RGg/o90+L8eoiR/s1y2ew/zZ\nhSwoL4x9jR/0i9O0vX460r+kiEyZ/sEhjnZEaOqI0NQeO+g3dYTjIXD2AX9mXg4LywtZUF7EhfPL\nWFheGFuOH/irSgpSMtxSYhQIIpIwd6etZ4DD8YP94RGPpvYwzV19+IgWnfzcHBbED/IXLYgd8GvL\ni+IH/iIqZ+VndCdtulEgiMhvGBp2joUiHDoZ5lB7L4dPhjl4spdDJ2MH/jPH3VeXzqR+TjFXLa2g\nbk4RteVF1M4ponZOIdUlBeToE37aUCCIZKFTB/39bb0cOtnLwbYwh072cuBkL03tYaJDv/6Yn5+X\nQ215IfUVxVy5pIL6iiLqK4qom1PEwvIijc7JIAoEkQzl7rR297OvtZcDbb0caOvhQFuYA209NLVH\nGBj69VQKRfm51FcUs6K6hJtX17Coooj6imLqK4qoKdWn/GyhQBBJc33RIfa39rKvtYd9rT0caOtl\nfzwEekZMlJafl8OiiiIuqJrF+tXVLKksZlFFMYsqi6kqmam2fFEgiKSL9t4B9rb0sKelm30tvw6A\no6HI6Y5cM5hfVsiSucXccekClsydxZK5xSyuLGZ+WaE+6cs5KRBEphF3p7Wnnz0nenj9RDd7WnrY\nG3+MHLJZOCOXJXOLubSunHdcVsvSqmKWVM5icWUxhflq05eJUSCIBKS9d4Ddzd28fqKb3Se62RMP\ngJFTL5QVzmBZ1SxuXl3NBVWzTj/0aV+SQYEgkmR90SFeP9HNruPd7BoRAK3d/adfU1qQx/LqEm69\naB7Lq2exvLqEZdWzmDtLbfuSOgoEkSni7hzv7OPVY13sau7iteZudh3v4kBbL6em3ymYkcPy6hKu\nWz6XFdUlrKiJPdSpK9OBAkFkAgaHhtnb2sOrx7pij+Oxx8jmnro5RaysKeG31sxnVU0JK+eVUjen\nSFMxyLSlQBAZR//gEK8399B4rJPGo500Huti1/Gu07dEnJmXw8qaEm69qIbV80pZNa+UlfNK0/Ym\nKZK99BsrMkJ0aJjdzd3sONLJK0dD7DjSye7mbgbjbT4lBXlcNL+Me66q58L5ZVw4v5TFlcXk5eYE\nXLnI5CkQJGsNDzsHT/ayrSnE9qYQ24908urxLgbin/zLCmewZmEZH7h2CRcvKOOi+WXUzilUW79k\nLAWCZI323gG2NXWw7XCIl+MhcOpWikX5uVy8oIz3XFXPmoWzWbOwjLo5RTr4S1ZRIEhGGhp29rR0\n89KhEC8e6uDlwx3sb+sFIMdgRU0pv7VmHmtrZ7O2tpwLqmaps1eyXiCBYGbvAD4BrAKucPetQdQh\nmaMvOsS2phBbD7bz/MEOXj7UQXd8Hp+K4nwuqSvnzoaFXFpXzpqFZRTl67OQyJmC+qtoBH4H+HpA\n25c01xmJxg7+B9p54WA7rxztPD1l84rqEt62dj4N9eVcWldOfYWafkQSEUgguPtrgP5IJWGd4SjP\nH2znuf0n2XLgJDuPdeEOM3KNNQtnc98bl3D5onIuqy9ndlF+0OWKpCWdN8u0FB4Y5IWDHfxybxvP\n7ms7HQD5eTlcUjubD9+wjHVL5nBpXblu0CIyRZIWCGa2GagZ5akH3P2x81jPBmADQF1d3RRVJ9PN\n4NAw24+E+MWekzy7r42XD3cQHXJm5BqX1pXzRzcu48olFaytna0AEEmSpAWCu6+fovVsBDYCNDQ0\n+DgvlzRyLBThmddbeWZPK7/Y00ZX3yBmcOH8Ut53zWKuvqCSyxeVqwNYJEX0lyYpMzA4zAsH23l6\nVws/f72VPS09ANSUFnDLRTVcu3wu1yytpLxYfQAiQQhq2OlvA18G5gI/MLNt7v6WIGqR5Grr6edn\nu1v56a4TPPN6Gz39g+Tn5bBu8Rze2VDLdSvmsqxqlgYYiEwDQY0y+nfg34PYtiTf3pYentrZzObX\nTrCtKYQ7VJfO5G1vmMcNK6u55oIKNQOJTEP6q5RJc3deOdrJUzubeWrnCfbGm4LWLCzjI+uXc8PK\nKi6cX6qzAJFpToEgE+LuvHQ4xBM7jvFUYzPHOvvIzTHWLZ7D711Zz80XVjOvrDDoMkXkPCgQJGGn\nzgSe2HGcH+w4ztFQhPzcHK5dXslHblrO+lXV6hAWSWMKBBnXvtYe/v2lo/zHjmMcOhkmL8d407JK\n/uSm5dx0YTWlBTOCLlFEpoACQUbV0TvAEzuO8b2XjrKtKUSOwTUXVHL/9Ut5y4U1mh5CJAMpEOS0\n6NAwT+9q4d9eOspPdp0gOuSsrCnhgdtWcfva+VSVFgRdoogkkQJBaGoP890Xmnh0axMt3f1Uzsrn\nnqsW8TuXLmD1PI0OEskWCoQsFR0a5ievtfDI84d5Zk8rBly/ooq7Lq/lhpVVukewSBZSIGSZtp5+\nHt5ymIefO0RLdz81pQV8+IZlvPPyWhbM1jBRkWymQMgSrx7r4hvPHuCx7ccYGBzm+hVz+cy6eq5f\nMVdnAyICKBAy2vCw85NdLWz6xQF+tf8khTNyeVdDLfdes4ilc2cFXZ6ITDMKhAw0ODTM49uP8ZWn\n97KvtZf5ZQV87NaV3HV5HWVFumZAREanQMgg/YNDfO/Fo3zt53tpao+wsqaEL919CbddVKNmIREZ\nlwIhA0QGhnjk+cNsfGY/zV19vKF2Nh9/64XcuKpKQ0ZFJGEKhDQ2ODTMo1uP8L82v05Ldz/rFs/h\n8+9YwxsvqFQQiMh5UyCkIXfnqZ3NfO6p3exv7eWy+nK+fPclrFtSEXRpIpLGFAhp5rn9J/mbJ3ex\nrSnEBVWzeOieBtaraUhEpoACIU0cPhnmk0/sZPNrLdSUFvC3d1zMHZcuVGexiEwZBcI01z84xEPP\n7OfLP91LXo7x0VtW8N6rF1OYnxt0aSKSYRQI09ize9t48LFG9rf2ctvFNTz41tW6C5mIJE0ggWBm\nnwfeBgwA+4D3unsoiFqmo5auPj79g9d4fPsx6uYU8c33Xs71K6qCLktEMlxQZwg/Bj7m7oNm9rfA\nx4C/CKiWaeX7Lx/lwcca6Y8O8+Ebl3H/9UspmKHmIRFJvkACwd1/NGJxC3BnEHVMJ52RKA9+v5HH\ntx+job6cz925hiWab0hEUmg69CG8D/juWE+a2QZgA0BdXV2qakqpX+07yZ8+uo2W7n7+7Obl/MF1\nSzV6SERSLmmBYGabgZpRnnrA3R+Lv+YBYBB4eKz1uPtGYCNAQ0ODJ6HUwPQPDvH3P36djc/sZ1FF\nMd/74NW8oXZ20GWJSJZKWiC4+/pzPW9m7wHeCtzo7hl1oE/EwbZe7n/4JV493sXdV9Tx4FtXUZQ/\nHU7YRCRbBTXK6BZincjXuXs4iBqC9Mt9bdz/8EsAPHRPAzetrg64IhGR4PoQ/g8wE/hxfMqFLe7+\nBwHVklIPP3eIjz+2k0WVxfzjexqorygOuiQRESC4UUYXBLHdIA0ODfPpH7zGN395kOtXzOVLd19C\naYFuViMi04carVOgMxLlDx95mWdeb+W+Ny7mr25bRW6OJqMTkelFgZBkh0728t5vvsDhk2H+9o6L\nedflmTl0VkTSnwIhiQ6fDHPXxi1EokN8+/3ruFL3KxCRaUyBkCRN7WHufigWBo984EpWzSsNuiQR\nkXPS5bBJcDQU4e6HttDTP8i371unMBCRtKBAmGLHOyPcvXELnZEo375vHRctKAu6JBGRhCgQptCJ\nrj5+96Hn6Ogd4J/vW8fFCxUGIpI+FAhTpKW7j7sf2kJLVx/ffN8VrNWcRCKSZtSpPAX6okPcu+kF\nmjv7+Nb7ruCy+vKgSxIROW8KhCnwicd38urxLjbd28Dli+YEXY6IyISoyWiS/nVrE995oYn//ual\n3LBSk9SJSPpSIEzCa8e7ePCxRq5aUsFH1i8PuhwRkUlRIExQd1+U+x9+idKCGfzvu9fqDmcikvbU\nhzAB7s5ffG8Hh9vD/Mv711FVUhB0SSIik6aPtRPwjWcP8sNXmvnoW1awTvMTiUiGSOgMwcwagDcB\n84EI0Ahsdvf2JNY2Lb14qIPP/PA1blpdzYZrlwRdjojIlDnnGYKZ3WtmLwEfAwqB3UAL8EZidzv7\nlpllzXzOfdEhPvzIy8ybXcDfveMNxO/2JiKSEcY7QygGrnH3yGhPmtlaYBlweKoLm442PXuAo6EI\n39lwJWWFutuZiGSWcwaCu39lrOfMLM/dt019SdNTe+8AX3t6H+tXVem+BiKSkcZrMvoPM6sf5efr\ngQmHgZl9ysx2mNk2M/uRmc2f6LpS5cs/3UPvwCB/ccvKoEsREUmK8UYZfQd42sweMLMZZjbfzB4F\nPg28ZxLb/by7r3H3tcATwF9PYl1Jd/hkmG9vOcS7Lq9lWXVJ0OWIiCTFOQPB3R8GLgHqgNeAXwGb\ngavc/cWJbtTdu0YsFgM+0XWlwud/tJu8nBz+WFcji0gGS2TY6WrgCuB5oAGojr8vOpkNm9n/BO4B\nOoE3T2ZdybS9KcR/bD/Gh2+4gOpSXYAmIplrvD6E/wt8Bbjf3X+X2NlCGbDdzG4e572bzaxxlMft\nAO7+gLvXAg8DHzrHejaY2VYz29ra2nqeuzc57s5nfvgaFcX5bLhuaUq3LSKSauOdIewEft/dhwDc\nvRf4MzP7FvBV4EdjvdHd1ydYw78APwA+PsZ6NgIbARoaGlLatPT07haeO9DOp26/kFkzNcuHiGS2\n8foQvngqDM74+Svu/qaJbtTMlo1YfDuwa6LrSpahYeezT+5icWUxd12RNdfeiUgWG6/JaKOZXTzG\nc8Vm9j4z+28T2O5n481HO4CbgT+awDqS6nsvHuH1Ez189C0rmKGZTEUkC4zXDvJV4MF4KDQCrUAB\nsauTS4FNxPoAzou733G+70mlvugQX/jxbi6pm80tF9UEXY6ISEqMd6XyNuCdZjaL2AijecQmt3vN\n3XenoL5APL2rhRNd/Xz+Ts1XJCLZI6GeUnfvAX6W3FKmjycbm5lTnM/VSzVFhYhkj0Snv36Fsy8e\n6wS2Ap9295NTXVhQ+geH+OmuFt66Zp7ugiYiWSXRsZRPAkPEhogC3BX/2gV8E3jb1JYVnF/saaOn\nf1B9ByKSdRINhGvc/ZoRy6+Y2bPufo2ZvTsZhQXlycZmSgryuHppZdCliIikVKJtIrPMbN2pBTO7\nApgVXxyc8qoCEh0a5sevnuCmVdXk56m5SESyS6JnCO8HNsVHGxmxpqL7zKwY+JtkFZdqW/afpDMS\nVXORiGSlREcZvQBcbGZlgLl7aMTTjyalsgA82dhMUX4u1y6fG3QpIiIpl1C7iJmVmdnfAz8BNpvZ\nF+LhkDGGhp0f7WzmzSurKJiRG3Q5IiIpl2hD+SagG3hn/NEFfCNZRQVh68F22noGuFXNRSKSpRLt\nQ1h6xnQT/8PMMup+yk82NjMzL4c3r6gKuhQRkUAkeoYQMbM3nlows2uITWGREYaHnad2NnPt8rkU\na5prEclSiR79/gD4pxH9Bh1M7p7K08r2IyGOd/bx529ZEXQpIiKBSXSU0XbgDWZWGl/uMrM7gB3J\nLC5V/rOxmRm5xo2rqoMuRUQkMOd19ZW7d7l7V3zxi0moJ+XcnScbm7l6aSVlhTOCLkdEJDCTuRw3\nI+aFfvV4F4fbwxpdJCJZbzKBkNL7GyfLfzY2k2Nw02o1F4lIdjtnH8IY015D7OwgI46gTzY2s25x\nBRWzZgZdiohIoMbrVH5rSqoIyN6Wbva29HDPVfVBlyIiErjxbqF5KFWFBOHFQx0AXKe5i0REJt6H\nYGYbJ7txM/szM3MzC+TmAx3hKABzS9RcJCIymU7lr09mw2ZWC9wEHJ7MeiYjFI6Sn5tDoSazExE5\nv0Aws1IzKwFw9xcnue0vAh8lwNFKnZEByopmYJYRI2hFRCYl0emvG+IjjnYAjWa23cwum+hGzezt\nwNH4FdCBCYWjlBfpYjQREUh8LqNNwP3u/l8A8YnuvgGsGesNZrYZGO1qrweAvwJuTmTDZrYB2ABQ\nV1eXYLmJCYWjzC7Mn9J1ioikq0QDoftUGAC4+y/MrPtcb3D39aP93MwuBhYD2+NNNQuBl8zsCndv\nHmU9G4GNAA0NDVPavNQRHqB2TtFUrlJEJG0lGgjPm9nXgUeItfm/C/iZmV0K4O4vJbpBd38FOH3T\nATM7CDS4e1ui65gqnZEoF2v+IhERIPFAWBv/+tfxr0YsGK6Of71hiutKiVA4ymz1IYiIAIkHwq3A\nHcCiEe9xd//kZAtw90WTXcdE9EWHiESHmF2kPgQREUg8EL4PhICXgL74z9J6cruuSOyiNE15LSIS\nk2ggLHT3W5JaSYqF4oFQrjMEEREg8QvTfhkfHZQxQvFpK9SHICISk+gZwhuBe83sANBPvFPZ3ce8\nDmG66wgPAGoyEhE55Xw6lTNKp84QRER+Q0KBkInTYIcisTMEjTISEYmZzGynaS0UjpKXYxTna6ZT\nERHI5kCIxC5K00ynIiIxWRsIneGomotEREbI2kAIRQaYrRFGIiKnZW0gdPRqHiMRkZGyNhA6I1HK\ndC8EEZHTsjYQQuEBnSGIiIyQlYEwMDhM78CQ+hBEREbIykDojOgqZRGRM2VpIOgqZRGRM2VlIGim\nUxGRs2VlIHScCgSNMhIROS0rAyEUPtVkpDMEEZFTsjIQTnUqlykQREROCyQQzOwTZnbUzLbFH7el\ncvuhcJTcHKNkZqK3gxARyXxBHhG/6O5/F8SGQ5EBygo106mIyEhZ2WQUCmseIxGRMwUZCB8ysx1m\ntsnMysd6kZltMLOtZra1tbV1SjbcGYnqKmURkTMkLRDMbLOZNY7yuB34GrAUWAscB74w1nrcfaO7\nN7h7w9y5c6ekto7wgC5KExE5Q9L6ENx9fSKvM7OHgCeSVcdoQuEoy6tKUrlJEZFpL6hRRvNGLP42\n0JjK7XeGoxpyKiJyhqBGGX3OzNYCDhwEfj9VG44ODdPdP6irlEVEzhBIILj77wWxXYAuzXQqIjKq\nrBt2GlIgiIiMKvsCIaypr0VERpOFgXBqplOdIYiIjJS9gaAmIxGR35B9gRDRvRBEREaTdYHQGR7A\nDEoKNNOpiMhIWRcIoUiUssIZ5ORoplMRkZGyLxDCUco1wkhE5CxZFwgd4di9EERE5DdlXSB0RnQv\nBBGR0WRdIITCuheCiMhosjAQdC8EEZHRZFUgDA07XX2D6kMQERlFVgWCZjoVERlbVgWCZjoVERlb\nVgVCh2Y6FREZU1YFQqdmOhURGVNWBUIoojMEEZGxZFcg6AxBRGRMgQWCmf2hme02s51m9rlUbPNU\nIJQqEEREzhLIHNBm9mbgdmCNu/ebWVUqttsZiVJakEeuZjoVETlLUGcIHwQ+6+79AO7ekoqN6ipl\nEZGxBRUIy4E3mdlzZvZzM7s8FRvtCEcp1zUIIiKjSlqTkZltBmpGeeqB+HbLgSuBy4FHzWyJu/so\n69kAbACoq6ubVE2hSJQynSGIiIwqaYHg7uvHes7MPgj8WzwAnjezYaASaB1lPRuBjQANDQ1nBcb5\n6AwPUD+naDKrEBHJWEE1GX0fuAHAzJYD+UBbsjca0r0QRETGFNSd5jcBm8ysERgA3jNac9FUGh72\n2M1xNORURGRUgQSCuw8A707lNrv7BnFHfQgiImPImiuVT09boTMEEZFRZU0gdMSvUi4vViCIiIwm\nawIhFJ/6uqxQTUYiIqPJmkDo1M1xRETOKWsCQTOdioicW9YFQpkCQURkVNkTCJEBSmbmkZebNbss\nInJesubo2BmOUqb+AxGRMWVNIHSEByjXRWkiImPKmkDQPEYiIueWNYHQGY6qQ1lE5ByyJhB0hiAi\ncm5ZEQjDwx67faauUhYRGVNWBELPwCDDrquURUTOJSsCoVMXpYmIjCsrAqEjPrGdhp2KiIwtKwLh\n9DxGajISERlTdgSCZjoVERlXVgRCp+6FICIyrqwIBM10KiIyvrwgNmpm3wVWxBdnAyF3X5us7YUi\nUYrzc8nPy4r8ExGZkEACwd3fdep7M/sC0JnM7S2rmsVvrZmXzE2IiKS9QALhFDMz4J3ADcnczl1X\n1HHXFXXJ3ISISNoLug3lTcAJd98TcB0iIlkvaWcIZrYZqBnlqQfc/bH493cDj4yzng3ABoC6On3K\nFxFJFnP3YDZslgccBS5z9yOJvKehocG3bt2a3MJERDKMmb3o7g3jvS7IJqP1wK5Ew0BERJIryEC4\ni3Gai0REJHUCG2Xk7vcGtW0RETlb0KOMRERkmlAgiIgIEOAoo4kws1bg0ATfXgm0TWE56UD7nB20\nz9lhMvtc7+5zx3tRWgXCZJjZ1kSGXWUS7XN20D5nh1Tss5qMREQEUCCIiEhcNgXCxqALCID2OTto\nn7ND0vc5a/oQRETk3LLpDEFERM4h4wLBzG4xs91mttfM/nKU52ea2Xfjzz9nZotSX+XUSWB//8TM\nXjWzHWb2EzOrD6LOqTTePo943Z1m5maW9qNREtlnM3tn/P96p5n9S6prnGoJ/G7XmdnTZvZy/Pf7\ntiDqnEpmtsnMWsyscYznzcy+FP832WFml05pAe6eMQ8gF9gHLAHyge3A6jNecz/wD/Hv7wK+G3Td\nSd7fNwNF8e8/mM77m+g+x19XAjwDbAEagq47Bf/Py4CXgfL4clXQdadgnzcCH4x/vxo4GHTdU7Df\n1wKXAo1jPH8b8CRgwJXAc1O5/Uw7Q7gC2Ovu+919APgOcPsZr7kd+Fb8+/8H3Bi/c1s6Gnd/3f1p\ndw/HF7cAC1Nc41RL5P8Y4FPA54C+VBaXJIns8weAr7h7B4C7t6S4xqmWyD47UBr/vgw4lsL6ksLd\nnwHaz/GS24F/8pgtwGwzm7L7A2daICwAmkYsH4n/bNTXuPsgsfs5V6SkuqmXyP6OdB+xTxfpbNx9\nNrNLgFp3fyKVhSVRIv/Py4HlZvasmW0xs1tSVl1yJLLPnwDebWZHgB8Cf5ia0gJ1vn/z5yXQeyon\nwWif9M8cRpXIa9JFwvtiZu8GGoDrklpR8p1zn80sB/gicG+qCkqBRP6f84g1G11P7Czwv8zsIncP\nJbm2ZElkn+8GvunuXzCzq4B/ju/zcPLLC0xSj1+ZdoZwBKgdsbyQs08jT78mfte2Ms59ijadJbK/\nmNl64AHg7e7en6LakmW8fS4BLgJ+ZmYHibWzPp7mHcuJ/l4/5u5Rdz8A7CYWEOkqkX2+D3gUwN1/\nBRQQm+8nkyX0Nz9RmRYILwDLzGyxmeUT6zR+/IzXPA68J/79ncBPPd5bk4bG3d9488nXiYVBurcr\nwzj77O6d7l7p7ovcfRGxfpO3u3s633s1kd/r7xMbQICZVRJrQtqf0iqnViL7fBi4EcDMVhELhNaU\nVpl6jwP3xEcbXQl0uvvxqVp5RjUZufugmX0IeIrYKIVN7r7TzD4JbHX3x4F/JHZquZfYmcFdwVU8\nOQnu7+eBWcC/xvvOD7v72wMrepIS3OeMkuA+PwXcbGavAkPAn7v7yeCqnpwE9/lPgYfM7CPEmk3u\nTeMPdwCY2SPEmv0q430jHwdmALj7PxDrK7kN2AuEgfdO6fbT/N9PRESmSKY1GYmIyAQpEEREBFAg\niIhInAJBREQABYKIiMQpEEQmyMxqzeyAmc2JL5fHl9N+RlnJTgoEkQly9ybga8Bn4z/6LLDR3Q8F\nV5XIxOk6BJFJMLMZwIvAJmIzjl4Sn51TJO1k1JXKIqnm7lEz+3PgP4GbFQaSztRkJDJ5twLHiU2q\nJ5K2FAgik2Bma4GbiM2q+pFlRp9cAAAAbElEQVSpvFmJSKopEEQmKH6nva8Bf+zuh4lNJPh3wVYl\nMnEKBJGJ+wCx2WN/HF/+KrDSzNL9JkSSpTTKSEREAJ0hiIhInAJBREQABYKIiMQpEEREBFAgiIhI\nnAJBREQABYKIiMQpEEREBID/D+dn4PSoXQwFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x167263f69b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(0.001,1)\n",
    "y = np.log(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('np.Log(X)')\n",
    "plt.xlabel('X')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84203357,  0.04192238,  0.00208719,  0.11395685])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax(logits):\n",
    "    t = np.exp(logits)\n",
    "    return t/sum(t)\n",
    "\n",
    "softmax([5,2,-1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6094379124341003"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth = [0, 1, 0, 0]\n",
    "predict = [0.3, 0.2, 0.1, 0.4]\n",
    "\n",
    "def softmax_loss(truth, predict):\n",
    "    sums=[]\n",
    "    for y, yHat in zip(truth, predict):\n",
    "        # if y is zero, then 0\n",
    "        if y==0:\n",
    "            sums.append(0)\n",
    "        # else take log(yHat)\n",
    "        else:\n",
    "            sums.append(np.log(yHat))  \n",
    "    return -sum(sums)\n",
    "    \n",
    "softmax_loss(truth, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Multclass cross entropy is slightly different than binary cross entropy (same idea though!)\n",
    "Taken from: http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "\n",
    "Cross-entropy loss, or log loss, **measures the performance of a classification model whose output is a probability value between 0 and 1.** \n",
    "- Cross-entropy loss increases as the predicted probability diverges from the actual label. \n",
    "- So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. \n",
    "- A perfect model would have a log loss of 0.\n",
    "\n",
    "The graph above shows the range of possible loss values given a **true observation (isDog = 1)**. \n",
    "- As the predicted probability approaches 1, log loss slowly decreases. \n",
    "- As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications that are confident and wrong!\n",
    "\n",
    "Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
    "\n",
    "### Math\n",
    "In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as:\n",
    "\n",
    "$-{(y\\log(p) + (1 - y)\\log(1 - p))}$\n",
    "\n",
    "If M>2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result.\n",
    "\n",
    "$-\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$\n",
    "\n",
    "- M: number of classes (dog, cat, fish)\n",
    "- log: the natural log\n",
    "- y: binary indicator (0 or 1) if class label c is the correct classification for observation o\n",
    "- p: predicted probability observation o is of class c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training a Softmax classifier\n",
    "\n",
    "- There's an activation which is called hard max, which gets 1 for the maximum value and zeros for the others.\n",
    "  - If you are using NumPy, its `np.max` over the vertical axis.\n",
    "- The Softmax name came from softening the values and not harding them like hard max.\n",
    "- Softmax is a generalization of logistic activation function to `C` classes. If `C = 2` softmax reduces to logistic regression.\n",
    "- The loss function used with softmax:\n",
    "  ```\n",
    "  L(y, y_hat) = - sum(y[j] * log(y_hat[j])) # j = 0 to C-1\n",
    "  ```\n",
    "- The cost function used with softmax:\n",
    "  ```\n",
    "  J(w[1], b[1], ...) = - 1 / m * (sum(L(y[i], y_hat[i]))) # i = 0 to m\n",
    "  ```\n",
    "- Back propagation with softmax:\n",
    "  ```\n",
    "  dZ[L] = Y_hat - Y\n",
    "  ```\n",
    "- The derivative of softmax is:\n",
    "  ```\n",
    "  Y_hat * (1 - Y_hat)\n",
    "  ```\n",
    "- Example:\n",
    "    ![](Images/07-_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Deep learning frameworks\n",
    "\n",
    "- It's not practical to implement everything from scratch. Our numpy implementations were to know how NN works.\n",
    "- There are many good deep learning frameworks.\n",
    "- Deep learning is now in the phase of doing something with the frameworks and not from scratch to keep on going.\n",
    "- Here are some of the leading deep learning frameworks:\n",
    "  - Caffe/ Caffe2\n",
    "  - CNTK\n",
    "  - DL4j\n",
    "  - Keras\n",
    "  - Lasagne\n",
    "  - mxnet\n",
    "  - PaddlePaddle\n",
    "  - TensorFlow\n",
    "  - Theano\n",
    "  - Torch/Pytorch\n",
    "- These frameworks are getting better month by month. Comparison between them can be found [here](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n",
    "- How to choose deep learning framework:\n",
    "  - Ease of programming (development and deployment)\n",
    "  - Running speed\n",
    "  - Truly open (open source with good governance)\n",
    "- Programming frameworks can not only shorten your coding time but sometimes also perform optimizations that speed up your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. TensorFlow\n",
    "\n",
    "- In this section we will learn the basic structure of TensorFlow programs.\n",
    "- Lets see how to implement a minimization function:\n",
    "  - Example function: `J(w) = w^2 - 10w + 25`\n",
    "  - The result should be `w = 5` as the function is `(w-5)^2 = 0`\n",
    "  - Code v.1:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    \n",
    "    w = tf.Variable(0, dtype=tf.float32)                 # creating a variable w\n",
    "    cost = tf.add(tf.add(w**2, tf.multiply(-10.0, w)), 25.0)        # can be written as this - cost = w**2 - 10*w + 25\n",
    "    train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    session = tf.Session()\n",
    "    session.run(init)\n",
    "    session.run(w)    # Runs the definition of w, if you print this it will print zero\n",
    "    session.run(train)\n",
    "\n",
    "    print(\"W after one iteration:\", session.run(w))\n",
    "\n",
    "    for i in range(1000):\n",
    "    \tsession.run(train)\n",
    "\n",
    "    print(\"W after 1000 iterations:\", session.run(w))\n",
    "    ```\n",
    "  - Code v.2 (we feed the inputs to the algorithm through coefficients):\n",
    "\n",
    "    ```python\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    \n",
    "    coefficients = np.array([[1.], [-10.], [25.]])\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [3, 1])\n",
    "    w = tf.Variable(0, dtype=tf.float32)                 # Creating a variable w\n",
    "    cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "\n",
    "    train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    session = tf.Session()\n",
    "    session.run(init)\n",
    "    session.run(w)    # Runs the definition of w, if you print this it will print zero\n",
    "    session.run(train, feed_dict={x: coefficients})\n",
    "\n",
    "    print(\"W after one iteration:\", session.run(w))\n",
    "\n",
    "    for i in range(1000):\n",
    "    \tsession.run(train, feed_dict={x: coefficients})\n",
    "\n",
    "    print(\"W after 1000 iterations:\", session.run(w))\n",
    "    ```\n",
    "- In TensorFlow you implement only the forward propagation and TensorFlow will do the backpropagation by itself.\n",
    "- In TensorFlow a placeholder is a variable you can assign a value to later.\n",
    "- If you are using a mini-batch training you should change the `feed_dict={x: coefficients}` to the current mini-batch data.\n",
    "- Almost all TensorFlow programs use this:\n",
    "  ```python\n",
    "  with tf.Session() as session:       # better for cleaning up in case of error/exception\n",
    "  \tsession.run(init)\n",
    "  \tsession.run(w)\n",
    "  ```\n",
    "- In deep learning frameworks there are a lot of things that you can do with one line of code like changing the optimizer.\n",
    "_**Side notes:**_\n",
    "- Writing and running programs in TensorFlow has the following steps:\n",
    "  1. Create Tensors (variables) that are not yet executed/evaluated.\n",
    "  2. Write operations between those Tensors.\n",
    "  3. Initialize your Tensors.\n",
    "  4. Create a Session.\n",
    "  5. Run the Session. This will run the operations you'd written above.\n",
    "- Instead of needing to write code to compute the cost function we know, we can use this line in TensorFlow :\n",
    "  `tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)`\n",
    "- To initialize weights in NN using TensorFlow use:\n",
    "  ```\n",
    "  W1 = tf.get_variable(\"W1\", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "\n",
    "  b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "  ```\n",
    "- For 3-layer NN, it is important to note that the forward propagation stops at `Z3`. The reason is that in TensorFlow the last linear layer output is given as input to the function computing the loss. Therefore, you don't need `A3`!\n",
    "- To reset the graph use `tf.reset_default_graph()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Extra Notes\n",
    "\n",
    "- If you want a good papers in deep learning look at the ICLR proceedings (Or NIPS proceedings) and that will give you a really good view of the field.\n",
    "- Who is Yuanqing Lin?\n",
    "  - Head of Baidu research.\n",
    "  - First one to win ImageNet\n",
    "  - Works in PaddlePaddle deep learning platform.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
