{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....................................................................................................................................................................\n",
    "# C4_Notes_W4 - Convolutional Neural Networks\n",
    "\n",
    "This is the forth course of the deep learning specialization at [Coursera](https://www.coursera.org/specializations/deep-learning) which is moderated by [DeepLearning.ai](http://deeplearning.ai/). The course is taught by Andrew Ng.\n",
    "\n",
    "**Week 4: Special applications: Face recognition &amp; Neural style transfer**\n",
    "\n",
    "Discover how CNNs can be applied to multiple fields, including art generation and face recognition. Implement your own algorithm to generate art and recognize faces!\n",
    "\n",
    "  * [1. Face Recognition](#face-recognition)\n",
    "     * [1.1 What is face recognition?](#what-is-face-recognition)\n",
    "     * [1.2 One Shot Learning](#one-shot-learning)\n",
    "     * [1.3 Siamese Network](#siamese-network)\n",
    "     * [1.4 Triplet Loss](#triplet-loss)\n",
    "     * [1.5 Face Verification and Binary Classification](#face-verification-and-binary-classification)\n",
    "  * [2. Neural Style Transfer](#neural-style-transfer)\n",
    "     * [2.1 What is neural style transfer?](#what-is-neural-style-transfer)\n",
    "     * [2.2 What are deep ConvNets learning?](#what-are-deep-convnets-learning)\n",
    "     * [2.3 Cost Function](#cost-function)\n",
    "     * [2.4 Content Cost Function](#content-cost-function)\n",
    "     * [2.5 Style Cost Function](#style-cost-function)\n",
    "     * [2.6 1D and 3D Generalizations](#1d-and-3d-generalizations)\n",
    "     * [2.6 Extras: Keras](#extras-keras) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition \n",
    "<a id='face-recognition'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is face recognition?\n",
    "<a id='what-is-face-recognition'></a>\n",
    "\n",
    "\n",
    "- Face recognition system identifies a person's face. It can work on both images or videos.\n",
    "- **<u>Liveness detection</u>** within a video face recognition system prevents the network from identifying a face in an image. It can be learned by supervised deep learning using a dataset for live human and in-live human and sequence learning.\n",
    "- Face verification vs. face recognition:\n",
    "  - Verification:\n",
    "    - Input: image, name/ID. (1 : 1)\n",
    "    - Output: whether the input image is that of the claimed person.\n",
    "    - \"is this the claimed person?\"\n",
    "  - Recognition:\n",
    "    - Has a database of K persons\n",
    "    - Get an input image\n",
    "    - Output ID if the image is any of the K persons (or not recognized)\n",
    "    - \"who is this person?\"\n",
    "- We can use a face verification system to make a face recognition system. The accuracy of the verification system has to be high (around 99.9% or more) to be use accurately within a recognition system because the recognition system accuracy will be less than the verification system given K persons. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Shot Learning\n",
    "<a id='one-shot-learning'></a>\n",
    "\n",
    "- One of the face recognition challenges is to solve one shot learning problem.\n",
    "- One Shot Learning: A recognition system is able to recognize a person, learning from one image.\n",
    "- Historically deep learning doesn't work well with a small number of data.\n",
    "- This is also important because if we have a system that has new people and faces coming in and our, then our system needs to be able to be able to recognize them with only a few pictures (otherwise we'd have to train new classifiers with their face as distinct classes... which clearly isn't practical)\n",
    "- Instead to make this work, we will learn a **similarity function**:\n",
    "  - d( **img1**, **img2** ) = degree of difference between images.\n",
    "  - We want d result to be low in case of the same faces.\n",
    "  - We use tau T as a threshold for d:\n",
    "    - If d( **img1**, **img2** ) <= T    Then the faces are the same.\n",
    "- Similarity function helps us solving the one shot learning. Also its robust to new inputs.\n",
    "- So essentially, we train the model on a bunch of images, and the output should be a similarity score comparing each image to each image... Obviouslly the same people will have similarity scores close to zero (the images should have very similar encodings), and people who look nothing alike will have a similarity score that is very high (they have highly dissimilar encodings).\n",
    "\n",
    "![](images/face_ver1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network\n",
    "<a id='siamese-network'></a>\n",
    "\n",
    "- We will implement the similarity function using a type of Neural Network called **Siamease Network**, in which we can pass multiple inputs into the a network with the same architecture and parameters.\n",
    "- Essentially we pass the images (in the inputs) through the same network (same architecture and params) to get an encodings (generally a 1x128 vector), and then compare the encodings\n",
    "- Siamese network architecture are as the following:\n",
    "![](Images/35.png)\n",
    "  - We make 2 identical conv nets which encodes an input image into a vector. In the above image the vector shape is (128, )\n",
    "  - The loss function will be the norm of the difference of the encoding vectors: `d(x1, x2) = || f(x1) - f(x2) ||^2`\n",
    "  - If `X1`, `X2` are the same person, we want d to be low. If they are different persons, we want d to be high.\n",
    "  - [[Taigman et. al., 2014. DeepFace closing the gap to human level performance]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss\n",
    "<a id='triplet-loss'></a>\n",
    "\n",
    "Triplet Loss is one of the loss functions we can use to solve the similarity distance in a Siamese network. Our learning objective in the triplet loss function is to get the distance between three images:\n",
    "- an **Anchor** image,\n",
    "- a **positive** imgae (a match with the anchor, eg. the same person)\n",
    "- a **negative** image (a picture of a different person)\n",
    "\n",
    "The triplet name comes from the fact that we are comparing an anchor A with a positive P and a negative N image. Formally we want:\n",
    "- Positive distance to be less than negative distance (not ^2 refers to the l2 norm)\n",
    "  - `||f(A) - f(P)||^2  <= ||f(A) - f(N)||^2`\n",
    "- Then\n",
    "  - `||f(A) - f(P)||^2  - ||f(A) - f(N)||^2 <= 0`\n",
    "- To make sure the NN won't get an output of zeros easily, we add the alph term **8(similar to how you add the C term to adjust the margin sizes with SVMs)**:\n",
    "  - `||f(A) - f(P)||^2  - ||f(A) - f(N)||^2 <= -alpha`\n",
    "  - Alpha is a small number. Sometimes its called the margin.\n",
    "- Then\n",
    "  - `||f(A) - f(P)||^2  - ||f(A) - f(N)||^2 + alpha <= 0`\n",
    "- Final Loss function:\n",
    "  - Given 3 images (A, P, N)\n",
    "  - `L(A, P, N) = max (||f(A) - f(P)||^2  - ||f(A) - f(N)||^2 + alpha , 0)`\n",
    "  - `J = Sum(L(A[i], P[i], N[i]) , i)` for all triplets of images.\n",
    "- **NOTE: You need multiple images of the same person in your dataset** Then get some triplets out of your dataset. Dataset should be big enough. For example if you have 10k images, you may only have 1k unique people (different picture of same people)\n",
    "\n",
    "Choosing the triplets A, P, N:\n",
    "- During training if A, P, N are chosen randomly (Subjet to A and P are the same and A and N aren't the same) then one of the problems this constrain is easily satisfied (because ranomly selected poeple will not look very similar!) \n",
    "    - `d(A, P) + alpha <= d (A, N)` \n",
    "    - So the NN wont learn much. What we want to do is choose triplets that are **hard** to train on (eg, chose a Negative image that looks at least somewhat similar to anchor/positive image)\n",
    "    - So for all the triplets we want this to be satisfied:\n",
    "    - `d(A, P) + alpha <= d (A, N)`\n",
    "    - This can be achieved by, for example, using the same poses!\n",
    "    - Find more at the paper.\n",
    "- Details are in this paper [[Schroff et al.,2015, FaceNet: A unified embedding for face recognition and clustering]](https://arxiv.org/abs/1503.03832)\n",
    "- Commercial recognition systems are trained on a large datasets like 10/100 million images.\n",
    "- There are a lot of pretrained models and parameters online for face recognition.\n",
    "\n",
    "![](images/face_ver2.png)\n",
    "![](images/face_ver3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Verification and Binary Classification\n",
    "<a id='face-verification-and-binary-classification'></a>\n",
    "\n",
    "- Triplet loss is one way to learn the parameters of a conv net for face recognition, but you can also **learn these parameters as a straight binary classification problem**.\n",
    "- Learning the similarity function another way:\n",
    "![](Images/36.png)\n",
    "- The final layer is a sigmoid layer.\n",
    "  - `Y' = wi * Sigmoid ( f(x(i)) - f(x(j)) ) + b` where the subtraction is the **Manhattan distance** between f(x(i)) and f(x(j))\n",
    "  - Some other similarities can be Euclidean and Ki square similarity.\n",
    "  - The NN here is Siamese means the top and bottom convs has the same parameters.\n",
    "- The paper for this work: [[Taigman et. al., 2014. DeepFace closing the gap to human level performance]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html)\n",
    "- A good performance/deployment trick:\n",
    "  - Pre-compute all the images that you are using as a comparison to the vector f(x(j))\n",
    "  - When a new image that needs to be compared, get its vector f(x(i)) then put it with all the pre computed vectors and pass it to the sigmoid function.\n",
    "- This version works quite as well as the triplet loss function.\n",
    "- Available implementations for face recognition using deep learning includes:\n",
    "  - [Openface](https://cmusatyalab.github.io/openface/)\n",
    "  - [FaceNet](https://github.com/davidsandberg/facenet)\n",
    "  - [DeepFace](https://github.com/RiweiChen/DeepFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Neural Style Transfer\n",
    "<a id='neural-style-transfer'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is neural style transfer?\n",
    "<a id='what-is-neural-style-transfer'></a>\n",
    "\n",
    "- Neural style transfer is one of the application of Conv nets.\n",
    "- Neural style transfer takes a content image `C` and a style image `S` and generates the content image `G` with the style of style image.\n",
    "![](Images/37.png)\n",
    "- In order to implement this you need to look at the features extracted by the Conv net at the shallower and deeper layers.\n",
    "- It uses a previously trained convolutional network like VGG, and builds on top of that. The idea of using a network trained on a different task and applying it to a new task is called transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are deep ConvNets learning?\n",
    "<a id='what-are-deep-convnets-learning'></a>\n",
    "\n",
    "- Visualizing what a deep network is learning:\n",
    "  - Given this AlexNet like Conv net:\n",
    "    - ![](Images/38.png)\n",
    "  - Pick a unit in layer l. Find the nine image patches that maximize the unit's activation. \n",
    "    - Notice that a hidden unit in layer one will see relatively small portion of NN, so if you plotted it it will match a small image in the shallower layers while it will get larger image in deeper layers.\n",
    "  - Repeat for other units and layers.\n",
    "  - It turns out that layer 1 are learning the low level representations like colors and edges.\n",
    "- You will find out that each layer are learning more complex representations.\n",
    "  - ![](Images/39.png)\n",
    "- The first layer was created using the weights of the first layer. Other images are generated using the receptive field in the image that triggered the neuron to be max.\n",
    "- [[Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks]](https://arxiv.org/abs/1311.2901)\n",
    "- A good explanation on how to get **receptive field** given a layer:\n",
    "  - ![](Images/receptiveField.png)\n",
    "  - From [A guide to receptive field arithmetic for Convolutional Neural Networks](https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "<a id='cost-function'></a>\n",
    "\n",
    "- We will define a cost function for the generated image that measures how good it is.\n",
    "- Give a content image C, a style image S, and a generated image G:\n",
    "  - `J(G) = alpha * J(C,G) + beta * J(S,G)`\n",
    "  - `J(C, G)` measures how similar is the generated image to the Content image.\n",
    "  - `J(S, G)` measures how similar is the generated image to the Style image.\n",
    "  - alpha and beta are relative weighting to the similarity and these are hyperparameters.\n",
    "- Find the generated image G:\n",
    "  1. Initiate G randomly\n",
    "     - For example G: 100 X 100 X 3\n",
    "  2. Use gradient descent to minimize `J(G)`\n",
    "     - `G = G - dG`  We compute the gradient image and use gradient decent to minimize the cost function.\n",
    "- The iterations might be as following image:\n",
    "  - To Generate this:\n",
    "\n",
    "![](Images/40.png)\n",
    "  - You will go through this:\n",
    "\n",
    "![](Images/41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Cost Function\n",
    "<a id='content-cost-function'></a>\n",
    "\n",
    "- In the previous section we showed that we need a cost function for the content image and the style image to measure how similar is them to each other.\n",
    "- Say you use hidden layer `l` to compute content cost. \n",
    "  - If we choose `l` to be small (like layer 1), we will force the network to get similar output to the original content image.\n",
    "  - In practice `l` is not too shallow and not too deep but in the middle.\n",
    "- Use pre-trained ConvNet. (E.g., VGG network)\n",
    "- Let `a(c)[l]` and `a(G)[l]` be the activation of layer `l` on the images.\n",
    "- If `a(c)[l]` and `a(G)[l]` are similar then they will have the same content\n",
    "  - `J(C, G) at a layer l = 1/2 || a(c)[l] - a(G)[l] ||^2`\n",
    "\n",
    "\n",
    "<img src=\"images/style_transfer.png\" align='left' style=\"width:500px;height:250px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Cost Function\n",
    "<a id='style-cost-function'></a>\n",
    "\n",
    "- Calculating the ***style*** of an image:\n",
    "  - Say you are using layer l's activation to measure ***style***.\n",
    "  - Define style as correlation between **activations** across **channels**. \n",
    "    - That means given an activation like this:\n",
    "      - ![](Images/42.png)\n",
    "    - How correlate is the orange channel with the yellow channel?\n",
    "    - Correlated means if a value appeared in a specific channel a specific value will appear too (Depends on each other).\n",
    "    - Uncorrelated means if a value appeared in a specific channel doesn't mean that another value will appear (Not depend on each other)\n",
    "  - The correlation tells you how a components might occur or not occur together in the same image.\n",
    "- The correlation of style image channels should appear in the generated image channels.\n",
    "- Style matrix (Gram matrix):\n",
    "  - Let `a(l)[i, j, k]` be the activation at l with `(i=H, j=W, k=C)`\n",
    "  - Also `G(l)(s)` is matrix of shape `nc(l) x nc(l)`\n",
    "    - We call this matrix style matrix or Gram matrix.\n",
    "    - In this matrix each cell will tell us how correlated is a channel to another channel.\n",
    "  - To populate the matrix we use these equations to compute style matrix of the style image and the generated image.\n",
    "    - ![](Images/43.png)\n",
    "    - As it appears its the sum of the multiplication of each member in the matrix.\n",
    "- To compute gram matrix efficiently:\n",
    "  - Reshape activation from H X W X C to HW X C\n",
    "  - Name the reshaped activation F.\n",
    "  - `G[l] = F * F.T`\n",
    "- Finally the cost function will be as following:\n",
    "  - `J(S, G) at layer l = (1/ 2 * H * W * C) || G(l)(s) - G(l)(G) ||`\n",
    "- And if you have used it from some layers\n",
    "  - `J(S, G) = Sum (lamda[l]*J(S, G)[l], for all layers)`\n",
    "- Steps to be made if you want to create a tensorflow model for neural style transfer:\n",
    "  1. Create an Interactive Session.\n",
    "  2. Load the content image.\n",
    "  3. Load the style image\n",
    "  4. Randomly initialize the image to be generated\n",
    "  5. Load the VGG16 model\n",
    "  6. Build the TensorFlow graph:\n",
    "     - Run the content image through the VGG16 model and compute the content cost\n",
    "     - Run the style image through the VGG16 model and compute the style cost\n",
    "     - Compute the total cost\n",
    "     - Define the optimizer and the learning rate\n",
    "  7. Initialize the TensorFlow graph and run it for a large number of iterations, updating the generated image at every step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D and 3D Generalizations\n",
    "<a id='1d-and-3d-generalizations'></a>\n",
    "\n",
    "- So far we have used the Conv nets for images which are 2D.\n",
    "- Conv nets can work with 1D and 3D data as well.\n",
    "- An example of 1D convolution:\n",
    "  - Input shape (14, 1)\n",
    "  - Applying 16 filters with F = 5 , S = 1\n",
    "  - Output shape will be 10 X 16\n",
    "  - Applying 32 filters with F = 5, S = 1\n",
    "  - Output shape will be 6 X 32\n",
    "- The general equation `(N - F)/S + 1` can be applied here but here it gives a vector rather than a 2D matrix.\n",
    "- 1D data comes from a lot of resources such as waves, sounds, heartbeat signals. \n",
    "- In most of the applications that uses 1D data we use Recurrent Neural Network RNN.\n",
    "- 3D data also are available in some applications like CT scan:\n",
    "  - ![](Images/44.png)\n",
    "- Example of 3D convolution:\n",
    "  - Input shape (14, 14,14, 1)\n",
    "  - Applying 16 filters with F = 5 , S = 1\n",
    "  - Output shape (10, 10, 10, 16)\n",
    "  - Applying 32 filters with F = 5, S = 1\n",
    "  - Output shape will be (6, 6, 6, 32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Extras: Keras\n",
    "<a id='extras-keras'></a>\n",
    "\n",
    "- Keras is a high-level neural networks API (programming framework), written in Python and capable of running on top of several lower-level frameworks including TensorFlow, Theano, and CNTK.\n",
    "- Keras was developed to enable deep learning engineers to build and experiment with different models very quickly.\n",
    "- Just as TensorFlow is a higher-level framework than Python, Keras is an even higher-level framework and provides additional abstractions.\n",
    "- Keras will work fine for many common models.\n",
    "- Layers in Keras:\n",
    "  - Dense (Fully connected layers).\n",
    "    - A linear function followed by a non linear function.\n",
    "  - Convolutional layer.\n",
    "  - Pooling layer.\n",
    "  - Normalisation layer.\n",
    "    - A batch normalization layer.\n",
    "  - Flatten layer\n",
    "    - Flatten a matrix into vector.\n",
    "  - Activation layer\n",
    "    - Different activations include: relu, tanh, sigmoid, and softmax.\n",
    "- To train and test a model in Keras there are four steps:\n",
    "  1. Create the model.\n",
    "  2. Compile the model by calling `model.compile(optimizer = \"...\", loss = \"...\", metrics = [\"accuracy\"])`\n",
    "  3. Train the model on train data by calling `model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)`\n",
    "     - You can add a validation set while training too.\n",
    "  4. Test the model on test data by calling `model.evaluate(x = ..., y = ...)`\n",
    "- Summarize of step in Keras: Create->Compile->Fit/Train->Evaluate/Test\n",
    "- `Model.summary()` gives a lot of useful informations regarding your model including each layers inputs, outputs, and number of parameters at each layer.\n",
    "- To choose the Keras backend you should go to `$HOME/.keras/keras.json` and change the file to the desired backend like Theano or Tensorflow or whatever backend you want.\n",
    "- After you create the model you can run it in a tensorflow session without compiling, training, and testing capabilities.\n",
    "- You can save your model with `model_save` and load your model using `model_load ` This will save your whole trained model to disk with the trained weights.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
