{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on TensorFlow\n",
    "\n",
    "From Chapter 9 of Hands-On Machine Learning textbook by Aurelien Geron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is a powerful open source library for numerical computation, particualy well suited and fine tuned for large scale machine learning. \n",
    "\n",
    "\n",
    "Its basic principle is simple: first define in Python a graph of computations to perform, and then tensor flow takes the graph and runs i efficiently using optimized C++ code. \n",
    "\n",
    "![](Pictures/tensorflow.png)\n",
    "\n",
    "**Most importantly, it is possible to break up the graph into several chunk and run them in parallel across multiple CPU or GPUs. Tensor Flow also supports distributed computing, so you can train colossal neural networks on humongous training sets in a reasonable amount of time by splitting the computations across hundreds of servers. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor flow highlights:\n",
    "- It runs on mobile devices!\n",
    "- It provides a very smple Python API called TF.Learn (tensorflow.contrib.learn), compatible with Scikit-Learn. You can use it to train various types of neural networks in just a few lines of code. \n",
    "- It also provides another simple API called TF-slim (tensorflow.contrib.slim) to simplify building, trianing and evaluating neural nets\n",
    "- several other hiugh level APIs have been bui8lt independently on Ttop of TensorFlow, such as Keras and Pretty tensor\n",
    "- Its main Python API offers much more flexilibity to create all sorts of computations, including any neural net architecture u can think of\n",
    "- It uncludes highly efficient C++ implementations of many ML operations, particularily those needed to build nerual networks. **There is also a C++ API to define your own high-performance operations**\n",
    "- It provides sevreal advanced optimization nodes to search for parameters that minimize a cost function. **These are very easy to use since TensorFlow automatically takes care of computing the gradients of the funcitons you define. THIS IS CALLED AUTOMATIC DIFFERENTIATING (OR AUTODIFF)**\n",
    "- It also comes with a great visualization tool called TensorBoard that allows you to browse through the computation graph, view learning curves, and more. \n",
    "- Google has also launched a cloud service to run TensorFlow graphs\n",
    "- Last but not least, it has a dedicated team of passionate and helpful developers. It is one of the most populat open source projects on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your first graph and running it in a session\n",
    "\n",
    "The following code creates the graph represented below:\n",
    "![](Pictures/Picture2.emf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f=x*x*y +y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! The most important thing to understand is that this code does not actually perform any computation, **it just creates a computation graph**. <font color=red>In fact, even the vraiables are nont initialized yet.</font> To evaluate this graph, you need to open a TensorFlow session and use it to initialize the variables and evaluate 'f'.\n",
    "\n",
    "**A TensorFlow session takes care of placing the operations onto devices such as CPUs and GPUs and running them, and it holds all the varialbe values. The following code creates a session, initializes the varables, and evaluates, and f then closes the session (which frees up resources).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "42\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to repeat session.run() all the time is a bit cumbersome, heres a better way:\n",
    "\n",
    "Inside the 'with' block, the session is set as the default session. Calling ***x.initializer.run()** is equivalent to calling **tf.get_default_session().run(x.initializer)**, and similarily **f.eval()** is equivalent to calling **tf.get_default_session().run(f)**.  This makes the code easier to read, and the code is automatically closed at the end of the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =red> instead of manually calling the initializer for ever single variable, you can use the 'global_varaibles_initializer()' funciton. Not that it does not actually perform initializtion immediately, but rater creates a node in the graph that will initialize all variables when it is run:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() #prepare an init node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #initialize all the variables\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inisde jupyter or Python shell you may perfer to create an **InteractiveSession**, the onyl differnt from a regular Session is that when and InteractiveSession is created it **automatically sets itself as the default session,** so you dont need a with block (but you do need to close the session manually when you are done with it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess =tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A TF program is typically split into two parts:\n",
    "1. First part builds a computaiton graph (this is called constructrion phase). \n",
    "2. Second part runs it (execution phase). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Graphs\n",
    "\n",
    "Any node you create is automatically added to the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In most cases this is fine, but sometimes you many want to manage multiple graphs. You can create new graphs and make it trhe default graph with a new with block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "new_graph = tf.Graph()\n",
    "with new_graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "print(x2.graph is new_graph)\n",
    "\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see from the above code, you use the **with block** to temporarily reset the default_graph so you can assign the variable to the new graph! \n",
    "\n",
    "<font color=red> **To avoide duplicated nodes, you can also run: tf.reset_default_graph()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifecycle of a Node Value\n",
    "\n",
    "When you evaluate a node, TF determines set of nodes that it depends on and it evaluates these first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) # 10\n",
    "    print(z.eval()) # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this code defines a very simple graph. Then it starts a session and runs the graph to evaluate y:\n",
    "TensorFlow automatically detects that y depends on w, which depends on x, so it first evaluates w,\n",
    "then x, then y, and returns the value of y. Finally, the code runs the graph to evaluate z. \n",
    "\n",
    "Once again,\n",
    "TensorFlow detects that it must first evaluate w and x. <font color =red>It is important to note that it will not reuse the\n",
    "result of the previous evaluation of w and x. **In short, the preceding code evaluates w and x twice BECAUSE WE RAN THE EVAL() FUNCTION IN TWO DIFFERENT LINES ON EACH VARIABLE INDEPENDENTLY.**</font>\n",
    "\n",
    "**All node values are dropped between graph runs, except variable values, which are maintained by the session across graph runs (queues and readers also maintain some state, as we will see in Chapter 12).**\n",
    "\n",
    "<font color=red> A variable starts its life when its initializer is run, and it ends when the session is closed.\n",
    "\n",
    "**If you want to evaluate y and z efficiently, without evaluating w and x twice as in the previous code,\n",
    "you must ask TensorFlow to evaluate both y and z in just one graph run, as shown in the following\n",
    "code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val) # 10\n",
    "    print(z_val) # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:** In single-process TensorFlow, multiple sessions do not share any state, even if they reuse the same graph (each session\n",
    "would have its own copy of every variable). In distributed TensorFlow (see Chapter 12), variable state is stored on the\n",
    "servers, not in the sessions, so multiple sessions can share the same variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with TF\n",
    "\n",
    "TF Operations (called ops for short) can take any number of inputs and produce any number of outputs. \n",
    "\n",
    "<font color = red size=5> The inputs are mutlidimensional arrays, called tensors. Just like numpy arrays, tesnros have a type and a shape. In the Python API tensors are simply represneted by NumPy ndarrays. They typically contain floats. </font>\n",
    "\n",
    "In the examples so far, the tensors contained a single scalar value, but you can perform computations on arrays of any shape. Following code manipulates 2D arrays to perform Linear Reg. \n",
    "\n",
    "- It starts by fetching the dataset; \n",
    "- then it adds an extra bias input feature (x0 = 1) to all training instances (it does so using NumPy so it runs immediately); \n",
    "- then it creates **two TensorFlow constant nodes, X and y, to hold this data and the targets, and it uses some of the matrix operations provided by TensorFlow to define theta.** These matrix functions — transpose(), matmul(), and matrix_inverse() — are selfexplanatory, but as usual they do not perform any computations immediately; instead, they create nodes in the graph that will perform them when the graph is run. You may recognize that the definition of theta corresponds to the Normal Equation = (XT · X)–1 · XT · y; see Chapter 4). \n",
    "- Finally, the code creates a session and uses it to evaluate theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.74651413e+01]\n",
      " [  4.35734153e-01]\n",
      " [  9.33829229e-03]\n",
      " [ -1.06622010e-01]\n",
      " [  6.44106984e-01]\n",
      " [ -4.25131839e-06]\n",
      " [ -3.77322501e-03]\n",
      " [ -4.26648885e-01]\n",
      " [ -4.40514028e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "#create constants because these numbers dont change... I think?!?!\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Compare with pure numpy\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Compare with scikit learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =blue size =4>The main benefit of this code versus computing the Normal Equation directly using NumPy is that\n",
    "**TensorFlow will automatically run this on your GPU card if you have one (provided you installed\n",
    "TensorFlow with GPU support**, of course; see Chapter 12 for more details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Gradient Descent\n",
    "\n",
    "Let’s try using Batch Gradient Descent (introduced in Chapter 4) instead of the Normal Equation.\n",
    "First we will do this by manually computing the gradients, then we will use TensorFlow’s autodiff\n",
    "feature to let TensorFlow compute the gradients automatically, and finally we will use a couple of\n",
    "TensorFlow’s out-of-the-box optimizers.\n",
    "\n",
    "**WARNING: ** <font color=red>When using Gradient Descent, remember that it is important to first normalize the input feature vectors, or else training may\n",
    "be much slower. </font>You can do this using TensorFlow, NumPy, Scikit-Learn’s StandardScaler, or any other solution you\n",
    "prefer. The following code assumes that this normalization has already been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Computing Gradients\n",
    "\n",
    "Following code should be fairly self explanatory except for a few new elements:\n",
    "\n",
    "- random_uniform(): gives node in graph that will generate tensor containin random values (much like NumPy's rand() funciton)\n",
    "- assign(): creates node that will assign new vlaue to a variable. **In this case it implements the Batch Gradient Descent step θ(next step) = θ – η∇θMSE(θ).**\n",
    "- Main loop executes training step over and over (n_epoch times), and every 100 iterations it prints current MSE, you should see MSE go down after every iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  2.34476576,  0.98214266,  0.62855945, -0.15375759,\n",
       "        -0.9744286 , -0.04959654,  1.05254828, -1.32783522],\n",
       "       [ 1.        ,  2.33223796, -0.60701891,  0.32704136, -0.26333577,\n",
       "         0.86143887, -0.09251223,  1.04318455, -1.32284391],\n",
       "       [ 1.        ,  1.7826994 ,  1.85618152,  1.15562047, -0.04901636,\n",
       "        -0.82077735, -0.02584253,  1.03850269, -1.33282653],\n",
       "       [ 1.        ,  0.93296751,  1.85618152,  0.15696608, -0.04983292,\n",
       "        -0.76602806, -0.0503293 ,  1.03850269, -1.33781784],\n",
       "       [ 1.        , -0.012881  ,  1.85618152,  0.3447108 , -0.03290586,\n",
       "        -0.75984669, -0.08561576,  1.03850269, -1.33781784]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale data first, ALWAYS NORMALIZE X BEFORE TRAINING WHEN DOING GS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)),scaled_housing_data]\n",
    "scaled_housing_data_plus_bias[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 11.1272\n",
      "Epoch 100 MSE = 0.820041\n",
      "Epoch 200 MSE = 0.620612\n",
      "Epoch 300 MSE = 0.591767\n",
      "Epoch 400 MSE = 0.572972\n",
      "Epoch 500 MSE = 0.559444\n",
      "Epoch 600 MSE = 0.549682\n",
      "Epoch 700 MSE = 0.542636\n",
      "Epoch 800 MSE = 0.53755\n",
      "Epoch 900 MSE = 0.533878\n",
      "\n",
      " Best theta vals:  \n",
      " [[ 2.06855249]\n",
      " [ 0.7698015 ]\n",
      " [ 0.14014694]\n",
      " [-0.09151634]\n",
      " [ 0.13498099]\n",
      " [ 0.0040568 ]\n",
      " [-0.03986073]\n",
      " [-0.80009288]\n",
      " [-0.76083553]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "# create vector of random theta variables n+1 values (1 for each feature)\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "#calculate gradients at each iteration\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X),error)\n",
    "\n",
    "#assign updates gradients (assign updates tf.variable() values!)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "#initialize all variables (just theta in this instance)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# run da thing!\n",
    "with tf.Session() as sess:\n",
    "    #first run initializer\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval()) #print every 100 runs\n",
    "        sess.run(training_op) #run the model for every n_epoch times!\n",
    "    #This pulls out most recentl theta... not necessarily BEST theta. I think?\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print('\\n',\"Best theta vals: \",'\\n', best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Autodiff\n",
    "\n",
    "**The preceding code works fine, but requires mathematically deriving the gradients from the cost function (MSE)... **\n",
    "\n",
    "In the case of Linear Regression, the gradients are easy to derive and calculate... BUT if you had to do this with a deep neural network this would be a pain in the ass.***You COULD use symbolic differentiaion to automatically find the equations for the partial derivatives for you, but the resulting code would not necessarily be very efficient.***\n",
    "\n",
    "To understand why, consider the function f(x)= exp(exp(exp(x))). If you know calculus, you can figure out its derivative f′(x) = exp(x) × exp(exp(x)) × exp(exp(exp(x))). If you code f(x) and f′(x) separately\n",
    "and exactly as they appear, your code will not be as efficient as it could be. A more efficient solution would be to write a function that first computes exp(x), then exp(exp(x)), then exp(exp(exp(x))), and\n",
    "returns all three. This gives you f(x) directly (the third term), and if you need the derivative you can just multiply all three terms and you are done. With the naïve approach you would have had to call the\n",
    "exp function nine times to compute both f(x) and f′(x). With this approach you just need to call it three times.\n",
    "\n",
    "<font color=red><br>**It gets worse when your function is defined by some arbitrary code. Can you find the equation (or the code) to compute the partial derivatives of the following function? Hint: don’t even try. ** </font>\n",
    "\n",
    "    def my_func(a, b):\n",
    "        z = 0\n",
    "        for i in range(100):\n",
    "            z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "        return z\n",
    "\n",
    "<font color = blue size=4> Forutunately, TF's **autodiff** feature comes to the rescue: it can automatically and efficiently compute the gradients for you. </font> Simply replace the 'gradients = ' line in the Gradient Descent code in the previous seciton, comme ca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 8.04903\n",
      "[[-3.75065756]\n",
      " [-2.22404027]\n",
      " [ 1.87810469]\n",
      " [-2.79127312]\n",
      " [-2.27046943]\n",
      " [-0.37205476]\n",
      " [ 1.22339511]\n",
      " [ 0.88830465]\n",
      " [-0.60488385]]\n",
      "Epoch 50 MSE = 1.5338\n",
      "[[-1.36587632]\n",
      " [-0.55734181]\n",
      " [ 0.42228666]\n",
      " [-0.29444665]\n",
      " [-0.31611839]\n",
      " [-0.04471397]\n",
      " [ 0.45046836]\n",
      " [ 0.30207318]\n",
      " [ 0.10403683]]\n",
      "Epoch 100 MSE = 0.928238\n",
      "[[-0.49741107]\n",
      " [-0.17276448]\n",
      " [ 0.12586282]\n",
      " [ 0.04735763]\n",
      " [-0.10181199]\n",
      " [ 0.01067678]\n",
      " [ 0.16459675]\n",
      " [ 0.20067784]\n",
      " [ 0.18626732]]\n",
      "Epoch 150 MSE = 0.816813\n",
      "[[-0.18114249]\n",
      " [-0.06663112]\n",
      " [ 0.05456889]\n",
      " [ 0.08639927]\n",
      " [-0.08751009]\n",
      " [ 0.01530829]\n",
      " [ 0.05916692]\n",
      " [ 0.17607491]\n",
      " [ 0.18177451]]\n",
      "Epoch 200 MSE = 0.769569\n",
      "[[-0.06596741]\n",
      " [-0.03268528]\n",
      " [ 0.03373747]\n",
      " [ 0.08390116]\n",
      " [-0.08612089]\n",
      " [ 0.01275524]\n",
      " [ 0.02050991]\n",
      " [ 0.16306704]\n",
      " [ 0.16883875]]\n",
      "Epoch 250 MSE = 0.735273\n",
      "[[-0.02402345]\n",
      " [-0.02024526]\n",
      " [ 0.02609884]\n",
      " [ 0.07642988]\n",
      " [-0.08165816]\n",
      " [ 0.01035774]\n",
      " [ 0.00644491]\n",
      " [ 0.15214403]\n",
      " [ 0.15674564]]\n",
      "Epoch 300 MSE = 0.706643\n",
      "[[-0.00874934]\n",
      " [-0.01482995]\n",
      " [ 0.0224474 ]\n",
      " [ 0.06896003]\n",
      " [-0.07544062]\n",
      " [ 0.00877058]\n",
      " [ 0.00138908]\n",
      " [ 0.14206746]\n",
      " [ 0.14590622]]\n",
      "Epoch 350 MSE = 0.682149\n",
      "[[-0.00318703]\n",
      " [-0.01187732]\n",
      " [ 0.02018152]\n",
      " [ 0.06208664]\n",
      " [-0.06886381]\n",
      " [ 0.00772043]\n",
      " [-0.00038452]\n",
      " [ 0.13268445]\n",
      " [ 0.13602221]]\n",
      "Epoch 400 MSE = 0.661098\n",
      "[[-0.0011605 ]\n",
      " [-0.00986954]\n",
      " [ 0.01847833]\n",
      " [ 0.05582426]\n",
      " [-0.06251757]\n",
      " [ 0.00696417]\n",
      " [-0.00097011]\n",
      " [ 0.12394894]\n",
      " [ 0.12690508]]\n",
      "Epoch 450 MSE = 0.642983\n",
      "[[-0.00042242]\n",
      " [-0.00828395]\n",
      " [ 0.01705102]\n",
      " [ 0.05012231]\n",
      " [-0.05659704]\n",
      " [ 0.00636703]\n",
      " [-0.00113005]\n",
      " [ 0.11582049]\n",
      " [ 0.11845344]]\n",
      "Epoch 500 MSE = 0.627381\n",
      "[[-0.00015386]\n",
      " [-0.00693193]\n",
      " [ 0.01578997]\n",
      " [ 0.04493095]\n",
      " [-0.05114464]\n",
      " [ 0.00586205]\n",
      " [-0.00114032]\n",
      " [ 0.10825658]\n",
      " [ 0.1106032 ]]\n",
      "Epoch 550 MSE = 0.613932\n",
      "[[ -5.66790986e-05]\n",
      " [ -5.74210705e-03]\n",
      " [  1.46476412e-02]\n",
      " [  4.02049646e-02]\n",
      " [ -4.61499803e-02]\n",
      " [  5.41698793e-03]\n",
      " [ -1.10024342e-03]\n",
      " [  1.01217903e-01]\n",
      " [  1.03305891e-01]]\n",
      "Epoch 600 MSE = 0.602327\n",
      "[[ -2.23534007e-05]\n",
      " [ -4.68248269e-03]\n",
      " [  1.36009380e-02]\n",
      " [  3.59042287e-02]\n",
      " [ -4.15853225e-02]\n",
      " [  5.01546077e-03]\n",
      " [ -1.04520214e-03]\n",
      " [  9.46660712e-02]\n",
      " [  9.65193883e-02]]\n",
      "Epoch 650 MSE = 0.592306\n",
      "[[ -1.18623002e-05]\n",
      " [ -3.73546593e-03]\n",
      " [  1.26364063e-02]\n",
      " [  3.19927000e-02]\n",
      " [ -3.74186784e-02]\n",
      " [  4.64893971e-03]\n",
      " [ -9.87558276e-04]\n",
      " [  8.85660052e-02]\n",
      " [  9.02066976e-02]]\n",
      "Epoch 700 MSE = 0.583644\n",
      "[[ -1.13018996e-05]\n",
      " [ -2.88973842e-03]\n",
      " [  1.17453877e-02]\n",
      " [  2.84368005e-02]\n",
      " [ -3.36183757e-02]\n",
      " [  4.31206962e-03]\n",
      " [ -9.31434624e-04]\n",
      " [  8.28849971e-02]\n",
      " [  8.43328610e-02]]\n",
      "Epoch 750 MSE = 0.576148\n",
      "[[ -1.13673996e-05]\n",
      " [ -2.13510334e-03]\n",
      " [  1.09208273e-02]\n",
      " [  2.52066888e-02]\n",
      " [ -3.01543269e-02]\n",
      " [  4.00140882e-03]\n",
      " [ -8.78159772e-04]\n",
      " [  7.75933266e-02]\n",
      " [  7.88660124e-02]]\n",
      "Epoch 800 MSE = 0.569657\n",
      "[[ -1.13797996e-05]\n",
      " [ -1.46327866e-03]\n",
      " [  1.01571195e-02]\n",
      " [  2.22745240e-02]\n",
      " [ -2.69986205e-02]\n",
      " [  3.71445995e-03]\n",
      " [ -8.27994780e-04]\n",
      " [  7.26627260e-02]\n",
      " [  7.37767741e-02]]\n",
      "Epoch 850 MSE = 0.564029\n",
      "[[ -1.12898997e-05]\n",
      " [ -8.66860908e-04]\n",
      " [  9.44947824e-03]\n",
      " [  1.96147598e-02]\n",
      " [ -2.41256505e-02]\n",
      " [  3.44907166e-03]\n",
      " [ -7.80930626e-04]\n",
      " [  6.80673271e-02]\n",
      " [  6.90377876e-02]]\n",
      "Epoch 900 MSE = 0.559144\n",
      "[[ -1.13160004e-05]\n",
      " [ -3.38384503e-04]\n",
      " [  8.79355334e-03]\n",
      " [  1.72043890e-02]\n",
      " [ -2.15117559e-02]\n",
      " [  3.20346584e-03]\n",
      " [ -7.36693211e-04]\n",
      " [  6.37830123e-02]\n",
      " [  6.46236613e-02]]\n",
      "Epoch 950 MSE = 0.554899\n",
      "[[ -1.14292998e-05]\n",
      " [  1.27892301e-04]\n",
      " [  8.18533730e-03]\n",
      " [  1.50220990e-02]\n",
      " [ -1.91347543e-02]\n",
      " [  2.97605968e-03]\n",
      " [ -6.95168390e-04]\n",
      " [  5.97877949e-02]\n",
      " [  6.05111942e-02]]\n",
      "\n",
      " Best theta vals:  \n",
      " [[ 2.06855249]\n",
      " [ 0.89036942]\n",
      " [ 0.17608295]\n",
      " [-0.2943781 ]\n",
      " [ 0.29176521]\n",
      " [ 0.01556478]\n",
      " [-0.04547139]\n",
      " [-0.42491141]\n",
      " [-0.39848056]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "# create vector of random theta variables n+1 values (1 for each feature)\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "#calculate gradients MAKE CHANGE HERE USING TENSORFLOW THIS TIME\n",
    "#tf.gradients(mse,[theta])\n",
    "\n",
    "#[0] is becase tf.gradient instructions are stored in a list... I think\n",
    "gradients = tf.gradients(mse,[theta])[0] \n",
    "\n",
    "#assign updates gradients\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "#initialize all variables (just theta in this instance)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# run da thing!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval()) #print every 100 runs\n",
    "            #THIS IS JUST ME PRINTING THE GRADIENTS IN ROUNDED FORM SO ITS EASIER TO READ!!! \n",
    "            #ITS SO COOL!!! After each epoch you can see the gradients converging towards 0!!!\n",
    "            grads = []\n",
    "            for g in gradients.eval():\n",
    "                grads.append(g[0])\n",
    "            print(np.array([round(gr,10) for gr in grads]).reshape(9,1))\n",
    "        sess.run(training_op) #run the model for every n_epoch times!           \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print('\\n',\"Best theta vals: \",'\\n', best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEAT! I printed the gradients.eval() at each epoch, and you can see the calculated gradients for each coefficient decreasing at each step!!! SOO COOOOOLLLLL!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5> Below is a list of the main approaches to computing gradients automatically. Tensor flow useses reverse-mode autodiff, which is perfect (efficient and accurate) when there are many inputs and few outputs, as is often the case with neural networks. **It computes all the partial derivatives of the outputs with regards to all the inputs in just n outputs + 1  graph traversals.** \n",
    "\n",
    "![](Pictures/Picture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an Optimizer\n",
    "So TF computes the gradients for you. But it gets even easier, it also provides a number of optimizers out of the box, including a Gradient Descent optimizer. You can simply replace the preceding 'gradients = ...' and 'training_op = ...' lines with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.87982\n",
      "Epoch 100 MSE = 0.721473\n",
      "Epoch 200 MSE = 0.565347\n",
      "Epoch 300 MSE = 0.552013\n",
      "Epoch 400 MSE = 0.545321\n",
      "Epoch 500 MSE = 0.540453\n",
      "Epoch 600 MSE = 0.536786\n",
      "Epoch 700 MSE = 0.534003\n",
      "Epoch 800 MSE = 0.53188\n",
      "Epoch 900 MSE = 0.530254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.86707652],\n",
       "       [ 0.14191604],\n",
       "       [-0.3061046 ],\n",
       "       [ 0.32597232],\n",
       "       [ 0.00343692],\n",
       "       [-0.04208428],\n",
       "       [-0.69584179],\n",
       "       [-0.66939598]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "#################### previous code ####################\n",
    "# gradients = tf.gradients(mse,[theta])[0] \n",
    "# training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "#################### optimized TF code ####################\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "#initialize all variables (just theta in this instance)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# run da thing!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval()) #print every 100 runs\n",
    "        sess.run(training_op) #run the model for every n_epoch times!           \n",
    "    best_theta = theta.eval()\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use a different type of optimizer, you just need to change one line. **For example, you can\n",
    "use a momentum optimizer (which often converges much faster than Gradient Descent) by defining the optimizer like this:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 5.65754\n",
      "Epoch 100 MSE = 0.530181\n",
      "Epoch 200 MSE = 0.524699\n",
      "Epoch 300 MSE = 0.524364\n",
      "Epoch 400 MSE = 0.524327\n",
      "Epoch 500 MSE = 0.524322\n",
      "Epoch 600 MSE = 0.524321\n",
      "Epoch 700 MSE = 0.524321\n",
      "Epoch 800 MSE = 0.52432\n",
      "Epoch 900 MSE = 0.524321\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "#################### previous code ####################\n",
    "# gradients = tf.gradients(mse,[theta])[0] \n",
    "# training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "#################### optimized TF code ####################\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "#initialize all variables (just theta in this instance)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# run da thing!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval()) #print every 100 runs\n",
    "        sess.run(training_op) #run the model for every n_epoch times!           \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding Data to the Training Algorithm\n",
    "Lets's try to modify previous code to implement Mini-batch Gradient Descent. \n",
    "\n",
    "For this, **we need to replace x and y at every iteration with the next mini-batch. The simplist way to do this is to use placeholder nodes.**\n",
    "- These nodes are special because they dont actually perform any computation, they just output data you tell them ot output at any time.\n",
    "- They are typically used to pass training dat to TensorFlow during training. If you dont specify a value at runtime for a placeholder, you get an exception.\n",
    "\n",
    "To create a placeholder node: \n",
    "- you must call the placeholder() function and specify the output tensor's data type. \n",
    "- Optionally, you can also specify its shape, if you want to enforce it (use 'None' you dont want to specify a shape).  \n",
    "\n",
    "For example, the following code creates a placeholder node A, and also a node B = A + 5. \n",
    "- **When we evaluate B, we pass a feed_dict to the eval() method that specifies the value of A.** \n",
    "- Note that A must have rank 2 (i.e., it must be two-dimensional) and there must be three columns (or else an exception is raised), but it can have any number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n",
      "[[ 6.  7.  8.]\n",
      " [ 6.  7.  8.]\n",
      " [ 6.  7.  8.]]\n"
     ]
    }
   ],
   "source": [
    "#reset the graph\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()\n",
    "\n",
    "# Set # of cols to 3!!! You could also leave it as None\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "    # gotta have 3 columns but unlimited rows!!!!\n",
    "    B_val_3 = B.eval(feed_dict={A: [[1, 2, 3],\n",
    "                                   [1, 2, 3],\n",
    "                                   [1, 2, 3]]})\n",
    "\n",
    "print(B_val_1)\n",
    "print(B_val_2)\n",
    "print(B_val_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch Gradient Descent\n",
    "To implement Mini-batch Gradient Descent, we tweak the code slightly. First, change the x and y in the construction phase to make them placeholder nodes:\n",
    "\n",
    "Then define the batch size and compute the total number of batches. \n",
    "\n",
    "Then in the execution phase, fetch the mini-batches one by one, then provide the value of x and y via the feed_dict parameter when evaluating a node thta depends on either of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.07033372]\n",
      " [ 0.86371452]\n",
      " [ 0.12255151]\n",
      " [-0.31211874]\n",
      " [ 0.38510373]\n",
      " [ 0.00434168]\n",
      " [-0.01232954]\n",
      " [-0.83376896]\n",
      " [-0.80304712]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "reset_graph()\n",
    "\n",
    "##################################### SET UP LINREG #####################################\n",
    "\n",
    "#OLD CODE\n",
    "#X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "#y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "#set up placeholder nodes for mini batch GS\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "#set up LinReg codes\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\") #initialize first rand theta\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\") #set cost function graph\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) #set GS optimizer\n",
    "training_op = optimizer.minimize(mse) #initialize the training op to train GS on MSE\n",
    "\n",
    "init = tf.global_variables_initializer() #initialize all variables\n",
    "\n",
    "##################################### RUN MINIBATCH GS #####################################\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 5}\n",
      "{1, 2, 3, 4, 5, 6}\n",
      "{1, 2}\n"
     ]
    }
   ],
   "source": [
    "s= set([1,2,3,1,1,1])\n",
    "s.add(5)\n",
    "print(s)\n",
    "\n",
    "s.update({1,2,3,4,5,6})\n",
    "print(s)\n",
    "\n",
    "s.intersection_update({1,2})\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red size=5> NOTE: We do not need to pass the value of X and y when evaluating 'theta' since it does not depend on either of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Resorting Models\n",
    "\n",
    "**Saving a mdoel: **TF makes saving a restoring a model very easy. Just create a saver ne at the end of the construction phase (after all variables nodes are created); then, in the execution phase, just call its save() method whenever you want to save the model, passing it the session and the path of the checkpoint file.\n",
    "\n",
    "**Moreover, you\n",
    "probably want to save checkpoints at regular intervals during training so that if your computer crashes\n",
    "during training you can continue from the last checkpoint rather than start over from scratch.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       # not shown in the book\n",
    "learning_rate = 0.01                                                                  # not shown\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            # not shown\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            # not shown\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      # not shown\n",
    "error = y_pred - y                                                                    # not shown\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    # not shown\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            # not shown\n",
    "training_op = optimizer.minimize(mse)                                                 # not shown\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval()) \n",
    "            # Save at regular intervals!\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    # save final model!\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")\n",
    "\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restoring a mdoel: ** create a Saver at the end of the construction phase just like before, but then at the begining of the execution phase, instead of initializing the variables using the init node, you call the restore() method of the Saver object. \n",
    "\n",
    "By default a Saver saves and restores all variables under their own name, but if you need more\n",
    "control, you can specify which variables to save or restore, and what names to use. For example, the\n",
    "following Saver will save or restore only the theta variable under the name weights:\n",
    "\n",
    "    saver = tf.train.Saver({\"weights\": theta})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() # not shown in the book\n",
    "print(best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** You can also restore meta graphs, or trained graphs!!!***\n",
    "\n",
    "<font color=red size=5>This means that you can import a pretrained model without having to have the corresponding Python code to build the graph. This is very handy when you keep tweaking and saving your model: you can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# notice that we start with an empty graph.\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\") # not shown in the book\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")  # this restores the graph's state\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Graph and Training Curves Using TensorBoard\n",
    "So now we have a computation graph that trains a Linear Regression model using Mini-batch\n",
    "Gradient Descent, and we are saving checkpoints at regular intervals. Sounds sophisticated, doesn’t\n",
    "it? However, we are still relying on the print() function to visualize progress during training. \n",
    "\n",
    "There\n",
    "is a better way: enter TensorBoard. If you feed it some training stats, it will display nice interactive visualizations of these stats in your web browser (e.g., learning curves). You can also provide it the\n",
    "graph’s definition and it will give you a great interface to browse through it. This is very useful to identify errors in the graph, to find bottlenecks, and so on.\n",
    "\n",
    "- The first step is to tweak your program a bit so it writes the graph definition and some training stats — for example, the training error (MSE) — to a log directory that TensorBoard will read from. \n",
    "- You need to use a different log directory every time you run your program, or else TensorBoard will merge stats from different runs, which will mess up the visualizations. \n",
    "- **The simplest solution for this is to include a timestamp in the log directory name. Add the following code at the beginning of the program:**\n",
    "\n",
    "You also need to add these lines to the end of the constructor code: \n",
    "\n",
    "    mse_summary = tf.summary.scalar('MSE', mse)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "   \n",
    "The first line creates a node in the graph that will evaluate the MSE value and write it to a\n",
    "TensorBoard-compatible binary log string called a summary. The second line creates a FileWriter\n",
    "that you will use to write summaries to logfiles in the log directory. The first parameter indicates the\n",
    "path of the log directory (in this case something like tf_logs/run-20160906091959/, relative to the\n",
    "current directory). The second (optional) parameter is the graph you want to visualize. Upon creation,\n",
    "**the FileWriter creates the log directory if it does not already exist (and its parent directories if\n",
    "needed), and writes the graph definition in a binary logfile called an events file.**\n",
    "\n",
    "Next you need to update the execution phase to evaluate the mse_summary node regularly during\n",
    "training (e.g., every 10 mini-batches). This will output a summary that you can then write to the events\n",
    "file using the file_writer. Then you can close the FileWriter at the end of the program.\n",
    "\n",
    "Here is the updated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reset default graph\n",
    "reset_graph()\n",
    "\n",
    "#set up logdir to save model in\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "#################### set up graph: construciton phase ####################\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "# Add lines to write model to file\n",
    "mse_summary = tf.summary.scalar('MSE', mse) \n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "#################### train model: execution phase ####################\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()                                                     # not shown\n",
    "\n",
    "file_writer.close()\n",
    "\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Avoid logging training stats at every single training step, as this would significantly slow down training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tf_log file, and you can see the runs!\n",
    "\n",
    "Put this code into the command line (make sure you are in the files directory first):\n",
    "    \n",
    "    dir tf_logs\n",
    "    \n",
    "And it will give you a record of the times you ran the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WANRING: In windows, you have to try to avoid the colon as it is TF treats it as something else... not exactly sure, but this means that you can't include full file path in the --logdir command. https://github.com/tensorflow/tensorflow/issues/7856\n",
    "\n",
    "Command line code to run tensorboard:\n",
    "\n",
    "    (py36) C:\\Users\\mciniello>cd C:\\Users\\mciniello\\Desktop\\Data Science Fundementals\\Data Mining and Advanced Analytics\\Text book code\n",
    "\n",
    "    (py36) C:\\Users\\mciniello\\Desktop\\Data Science Fundementals\\Data Mining and Advanced Analytics\\Text book code>tensorboard --logdir tf_logs/\n",
    "    Starting TensorBoard b'54' at http://CA47496-mcini04:6006\n",
    "\n",
    "    (Press CTRL+C to quit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Scopes\n",
    "\n",
    "When dealing with more complex models such as neural networks, the graph can easily become\n",
    "cluttered with thousands of nodes. To avoid this, you can create name scopes to group related nodes.\n",
    "For example, let’s modify the previous code to define the error and mse ops within a name scope\n",
    "called \"loss\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reset default graph\n",
    "reset_graph()\n",
    "\n",
    "#set up logdir to save model in\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "#################### set up graph: construciton phase ####################\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "# Adjust model to include name scopes\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "mse_summary = tf.summary.scalar('MSE', mse) \n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "#################### train model: execution phase ####################\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()                                                     # not shown\n",
    "\n",
    "file_writer.close()\n",
    "\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mse and error nodes now appear inside the loss namespace, which appears collapse.\n",
    "\n",
    "BEFORE:\n",
    "![](pictures/tb1.png)\n",
    "\n",
    "AFTER:\n",
    "![](pictures/tb2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity\n",
    "Suppose you want to create a graph that adds the output of two rectified linear units (ReLU). A ReLU\n",
    "computes a linear function of the inputs, and outputs the result if it is positive, and 0 otherwise, as\n",
    "shown in Equation 9-1.\n",
    "\n",
    "![](pictures/relu.emf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does the job, but it’s quite repetitive:\n",
    "    \n",
    "    n_features = 3\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "    w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "    w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "    b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "    b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "    z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "    z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "    relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "    relu2 = tf.maximum(z1, 0., name=\"relu2\")\n",
    "    output = tf.add(relu1, relu2, name=\"output\")\n",
    "    \n",
    "Such repetitive code is hard to maintain and error-prone (in fact, this code contains a cut-and-paste\n",
    "error; did you spot it?). It would become even worse if you wanted to add a few more ReLUs.\n",
    "Fortunately, TensorFlow lets you stay DRY (Don’t Repeat Yourself): simply create a function to build\n",
    "a ReLU. **The following code creates five ReLUs and outputs their sum (note that add_n() creates an operation that will compute the sum of a list of tensors):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when you create a node, TensorFlow checks whether its name already exists, and if it does\n",
    "it appends an underscore followed by an index to make the name unique. So the first ReLU contains\n",
    "nodes named \"weights\", \"bias\", \"z\", and \"relu\" (plus many more nodes with their default name,\n",
    "such as \"MatMul\"); the second ReLU contains nodes named \"weights_1\", \"bias_1\", and so on; the\n",
    "third ReLU contains nodes named \"weights_2\", \"bias_2\" and so on. TensorBoard identifies such series and collapses them together to reduce clutter. \n",
    "\n",
    "Using name scopes, you can make the graph much clearer. Simply move all the content of the relu()\n",
    "function inside a name scope. Figure 9-7 shows the resulting graph. **Notice that TensorFlow also\n",
    "gives the name scopes unique names by appending _1, _2, and so on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### even better code using name_scope\n",
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                             # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      # not shown\n",
    "        return tf.maximum(z, 0., name=\"max\")                          # not shown\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"tf_logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing Variables\n",
    "... TensorFlow offers another option, which may lead to slightly cleaner and more modular code than the\n",
    "previous solutions.5 This solution is a bit tricky to understand at first, but since it is used a lot in\n",
    "TensorFlow it is worth going into a bit of detail. The idea is to use the get_variable() function to\n",
    "create the shared variable if it does not exist yet, or reuse it if it already exists. The desired behavior\n",
    "(creating or reusing) is controlled by an attribute of the current variable_scope(). For example, the\n",
    "following code will create a variable named \"relu/threshold\" (as a scalar, since shape=(), and\n",
    "using 0.0 as the initial value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the variable has already been created by an earlier call to get_variable(), this code\n",
    "will raise an exception. This behavior prevents reusing variables by mistake. If you want to reuse a\n",
    "variable, you need to explicitly say so by setting the variable scope’s reuse attribute to True (in\n",
    "which case you don’t have to specify the shape or the initializer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will fetch the existing \"relu/threshold\" variable, or raise an exception if it does not\n",
    "exist or if it was not created using get_variable(). Alternatively, you can set the reuse attribute to\n",
    "True inside the block by calling the scope’s reuse_variables() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:**\n",
    "Once reuse is set to True, it cannot be set back to False within the block. Moreover, if you define other variable scopes\n",
    "inside this one, they will automatically inherit reuse=True. \n",
    "\n",
    "<font color=blue size 56> Lastly, only variables created by get_variable() can be reused this way. **This why when you run a loop that keeps defininf threshold, X, y, etc, only theshold gets reused because it is the only variable you have called using 'get_variable()', and X, Y,etc all have _1, _2, _3 etc appended to them!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have all the pieces you need to make the relu() function access the threshold variable\n",
    "without having to pass it as a parameter:\n",
    "\n",
    "<FONT COLOR = RED SIZE=4>  This code first defines the relu() function, then creates the relu/threshold variable (as a scalar\n",
    "that will later be initialized to 0.0) and builds five ReLUs by calling the relu() function. **The\n",
    "relu() function reuses the relu/threshold variable, and creates the other ReLU nodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"tf_logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue size=4> **Variables created using get_variable() are always named using the name of their variable_scope as a prefix (e.g.,\n",
    "\"relu/threshold\"),** but for all other nodes (including variables created with tf.Variable()) the **variable scope acts like a\n",
    "new name scope.** In particular, if a name scope with an identical name was already created, then a suffix is added to make the name unique. **For example, all nodes created in the preceding code (except the threshold variable) have a name prefixed with \"relu_1/\" to \"relu_5/\", as shown in Figure 9-8.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is somewhat unfortunate that the threshold variable must be defined outside the relu() function, where all the rest of the ReLU code resides. \n",
    "To fix this, **the following code creates the threshold variable within the relu() function upon the first call, then reuses it in subsequent calls.**\n",
    "\n",
    "Now the relu() function does not have to worry about name scopes or variable sharing: it just calls get_variable(), which will create or reuse the threshold variable (it does not need to know which is the case). **The rest of the code calls relu() five times, making sure to set reuse=False on\n",
    "the first call, and reuse=True for the other calls.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"tf_logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "\n",
    "**1. What are the main benefits of creating a computation graph rather than directly executing the computations? What are the main drawbacks?**\n",
    "\n",
    "Main benefits:\n",
    "- TensorFlow can automatically compute the gradients for you (using reverse-mode autodiff). **Though so does sklearn right?!?!**\n",
    "- TensorFlow can take care of running the operations in parallel in different threads.\n",
    "- It makes it easier to run the same model across different devices.\n",
    "- It simplifies introspection — for example, **to view the model in TensorBoard**.\n",
    "\n",
    "Main drawbacks:\n",
    "- It makes the learning curve steeper.\n",
    "- It makes step-by-step debugging harder. **VERY TRUE!!! But there are more simply implementations for neural nets being released everyday it seems!!!***\n",
    "\n",
    "**2. Is the statement a_val = a.eval(session=sess) equivalent to a_val = sess.run(a)?**\n",
    "\n",
    "Yes!\n",
    "\n",
    "**3. Is the statement a_val, b_val = a.eval(session=sess), b.eval(session=sess) equivalent to a_val, b_val = sess.run([a, b])?**\n",
    "\n",
    "No! Indeed, the first statement runs the graph twice (once to compute a, once to compute b), while the second statement runs the graph only once. If any of these operations (or the ops they depend on) have side effects\n",
    "(e.g., a variable is modified, an item is inserted in a queue, or a reader reads a file), then the\n",
    "effects will be different. **If they don’t have side effects, both statements will return the same\n",
    "result, but the second statement will be faster than the first.**\n",
    "\n",
    "**4. Can you run two graphs in the same session?**\n",
    "<font color=red><br>No, you cannot run two graphs in the same session. You would have to merge the graphs into a single graph first.</font>\n",
    "\n",
    "**5. If you create a graph g containing a variable w, then start two threads and open a session in each thread, both using the same graph g, will each session have its own copy of the variable w or will it be shared?**\n",
    "\n",
    "In local TensorFlow, sessions manage variable values, so if you create a graph g containing\n",
    "a variable w, then start two threads and open a local session in each thread, both using the\n",
    "same graph g, then each session will have its own copy of the variable w. However, in\n",
    "distributed TensorFlow, variable values are stored in containers managed by the cluster, so\n",
    "if both sessions connect to the same cluster and use the same container, then they will share\n",
    "the same variable value for w.\n",
    "\n",
    "**6. When is a variable initialized? When is it destroyed?**\n",
    "\n",
    "<font color=red> A variable is initialized when you call its initializer, and it is destroyed when the session\n",
    "ends.</font> In distributed TensorFlow, variables live in containers on the cluster, so closing a\n",
    "session will not destroy the variable. To destroy a variable, you need to clear its container.\n",
    "\n",
    "**7. What is the difference between a placeholder and a variable?**\n",
    "\n",
    "Variables and placeholders are very differnet:\n",
    "- A variable is an operation that holds a value. If you run the variable, it returns that value, and before you run it you need to initialize it. You can also change the variables value by using the assignment operation. Variables are stateful, meaning that they keep the same value upon succesive runs of the graph (unless you change it with the assignment operation). **Variables are typically used to hold model parameters but also for other purposes (e.g. to count the global training step).**\n",
    "- Placeholders technically do not do very much: they just hold information about the type of shape of the tensor they represent, but they have no value. In fact, if you try to evaluation an operation that depends on a placeholder, you must feed TensorFlow the value of the placeholder (using the feed_dict argument) or else you will get an exception. **Placeholders are typically used to feed training or test data to TensorFlow during the EXECUTION PHASE, or to pass a value to an assignment node to change the value of a variable.**\n",
    "\n",
    "**8. What happens when you run the graph to evaluate an operation that depends on a placeholder but you don’t feed its value? What happens if the operation does not depend on the placeholder?**\n",
    "\n",
    "If you run the graph to evaluate an operation that depends on a placeholder but you don’t feed its value, you get an exception. If the operation does not depend on the placeholder, then no exception is raised.\n",
    "\n",
    "**9. When you run a graph, can you feed the output value of any operation, or just the value of placeholders?**\n",
    "\n",
    "When you run a graph, you can feed the output value of any operation, not just the value of\n",
    "placeholders. **In practice, however, this is rather rare.**\n",
    "\n",
    "**10. How can you set a variable to any value you want (during the execution phase)?**\n",
    "\n",
    "You can specify a variable’s initial value when constructing the graph, and it will be initialized later when you run the variable’s initializer during the execution phase. **If you want to change that variable’s value to anything you want during the execution phase, then the simplest option is to create an assignment node (during the graph construction phase) using the tf.assign() function, passing the variable and a placeholder as parameters.** During the execution phase, you can run the assignment operation and feed the variable’s new value using the placeholder.\n",
    "\n",
    "            import tensorflow as tf\n",
    "            x = tf.Variable(tf.random_uniform(shape=(), minval=0.0, maxval=1.0))\n",
    "            x_new_val = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "            x_assign = tf.assign(x, x_new_val)\n",
    "            with tf.Session():\n",
    "                x.initializer.run() # random number is sampled *now*\n",
    "                print(x.eval()) # 0.646157 (some random number)\n",
    "                x_assign.eval(feed_dict={x_new_val: 5.0})\n",
    "                print(x.eval()) # 5.0\n",
    "\n",
    "**11. How many times does reverse-mode autodiff need to traverse the graph in order to compute the gradients of the cost function with regards to 10 variables? What about forward-mode autodiff? And symbolic differentiation?**\n",
    "\n",
    "- Reverse-mode autodiff (implemented by TensorFlow) needs to traverse the graph only twice in order to compute the gradients of the cost function with regards to any number of variables. \n",
    "- Forward-mode autodiff would need to run once for each variable (so 10 times if we want the gradients with regards to 10 different variables). \n",
    "- As for symbolic differentiation, it would build a different graph to compute the gradients, so it would not traverse the original graph at all (except when building the new gradients graph). A highly optimized symbolic differentiation system could potentially run the new gradients graph only once to compute the gradients with regards to all variables, but that new graph may be horribly complex and inefficient compared to the original graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
