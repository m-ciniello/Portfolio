{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Nets - Part 1\n",
    "\n",
    "At some point you will need to train a much deeper DNN, perhaps with (say) 10 layers, each containing hundreds of neurons, connected by hundreds of thousands of connections. This would not be a walk in the park:\n",
    "- First, you would be faced with the tricky vanishing gradients problem (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train.\n",
    "- Second, with such a large network, training would be extremely slow.\n",
    "- Third, a model with millions of parameters would severely risk overfitting the training set.\n",
    "\n",
    "Here we will go through each of these problems in turn and present techniques to solve them. We will start by explaining the vanishing gradients problem and exploring some of the most popular solutions to this problem. Next we will look at various optimizers that can speed up training large models tremendously compared to plain Gradient Descent. Finally, we will go through a few popular regularization techniques for large neural networks. With these tools, you will be able to train very deep nets: welcome to Deep Learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradient Problems\n",
    "\n",
    "**Backpropogation review:** Backpropogation algo works by going from output layer to the input layer, propogating the input gradient on the way. Once the algo has computed the gradient of the cost function with regards to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step. \n",
    "\n",
    "<font color=red> This is pretty easy so far... the catch is, the gradients often get smaller and smaller as the algorithm progresses down to the lower layers. **As a result, the Gradient Descent update leaves the lower layer connection weights vitrually unchanged, and training never converges**. This is called **vanishing gradients problem. ** In some cases, the opposite can happen: the gradients get so big the algo diverges. **This is called exploding gradients, which mostly occurs in recurrent neural networks**. More Generally, DNNs suffer from unstable gradients: **different layers may learn at widely different rates**.  </font>\n",
    "\n",
    "However, significant progress on this problem was made wiht the 2010 paper \"Understanding the Difficulty of Training Deep Feedforward Neural Networks\": http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf:\n",
    "\n",
    "Xavier Glorot and Yoshua Bengio showed that  the **main problems were mainly:**\n",
    "- the popular logistic sigmoid activation function \n",
    "- and the weight initialization technique that was most popular at the time (which was a random initialization using normal disytribution with a mean of 0 and a standard deviation of 1).\n",
    "\n",
    "<font color=red size=4>In short they showed that with this scheme the **variance of the outputs of each layer is MUCH greater than the variance of its inputs**... das not good dawg! Gotta control dat variance!!! SOOOO ESSENTIALLY, as you go forward through the network, the larger variance saturates the numbers, and the gradients get superrrr small. BECAUSE AS YOU REMEMBER FROM YOUR NOTES, the slope of the loss function with respect to each weights (each weight's gradient) is equal to the product of:</font>\n",
    "\n",
    "1. the slope of the loss function with respect to the node the weight feeds into\n",
    "2. the slope of the activation function with respect to the node the weight feeds into\n",
    "3. Value of the node that feeds into the weight\n",
    "\n",
    "<font color=green> so this means that if the gradients in the VERY DEEP NODES are close to zero (which will be the case if the outputs are saturated), then as back prop goes back across the network, it will start with VERY low gradients, and this will not allow the model to train effectively!</font>\n",
    "\n",
    "\n",
    "Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 like the tanh function. \n",
    "\n",
    "**Saturation:** Looking at the logistic activation fuction, you can see that when inputs become large (negative or positive), the function saturates at 0 or 1 (becomes really close to either one), <font color=red size=4>with a derivative (or gradient) extremely close to 0</font>. ***Thus when backrpop kicks in, it has virtually no gradient to propogate back through the network, and what little gradient exists keeps getting diluted as backprop progresses through the top layers, so there is really nothing left for the lower layers.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEJCAYAAAB8Pye7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2149907ed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier and He Initialization\n",
    "\n",
    "In their paper, Glorot and Bengio propose a way to significantly alleviate the **Vanishin/Exploding Gradient Problem.** The signla needs to flow properly in both directions: in the forward direction when making predictions, and in the reverse when back propogating gradients. **We dont want the signal to die out, nor do we want it to explode and saturate.** <Font size=5 color=red> To do this, we need the variance of the outputs of each layer to be equal to the variance of its inputs... AND we also need the gradients to have equal variance before and after flowing through a layer in reverse direction. </font> It is actually NOT POSSIBLE to guaruntee both unless the layer has an equal number of input and output connections, but they proposed a good compromise that has proven to work very well in practice: **the connection weights must be initialized randomly as described in the equation below, where n_inputs and n_outputs are the number of input and output connections for the layer whose weights are being initialized (also called fan-in and fan-out).** This is called Xavier initialization or Glorot initialization.\n",
    "![](Pictures/homl_ch11_xavier.jpg)\n",
    "![](Pictures/homl_ch11_xavier2.jpg)\n",
    "\n",
    "Using the Xavier initialization strategy can speed up training considerably, and it is one of the tricks\n",
    "that led to the current success of Deep Learning. Some recent papers4 have provided similar\n",
    "strategies for different activation functions, as shown in Table 11-1. The initialization strategy for the\n",
    "ReLU activation function (and its variants, including the ELU activation described shortly) is\n",
    "sometimes called He initialization (after the last name of its author).\n",
    "\n",
    "![](Pictures/homl_ch11_picture123.jpg)\n",
    "\n",
    "<font color=red size=6> By default, the fully_connected() function introduced in chapter 10 uses the Xavier initialization (with a uniform distribution). You can change this to He Initializaiton by using the variance_scaling_initializer() funciton like this:</font>\n",
    "\n",
    "\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    hidden1 = fully_connected(X, n_hidden1, weights_initializer=he_init, scope=\"h1\")\n",
    "    \n",
    "NOTE: \n",
    "He initialization considers only the fan-in, not the average between fan-in and fan-out like in Xavier initialization. This is also\n",
    "the default for the variance_scaling_initializer() function, but you can change this by setting the argument\n",
    "mode=\"FAN_AVG\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions\n",
    "\n",
    "One of the inishgts in the 2010 paper was also that vanishing/exploding gradients problems were in part due a poor choice of activation function. Other activation functions behave much better in deep neural nets, in particular ReLU activation funciton, **mostly because it does not saturate for positive values** and also because its super fast to copmute. \n",
    "\n",
    "HOWEVER, ReLU activation is not eprfect, as it suffers from **dying ReLU problem: during trianing some neurons effectively die, meaning they stop otputting anything other than 0.** <font color =red>In some cases, you may find that half of your network's neurons are dead, especially if you used a large learning rate.\n",
    "- during training, if a neurons weights get updated such that the weighted sum of the neurons inputs is negative, it wil start outputting 0. When this happens, the neuron is unlikely to come back to life, since the gradient of the ReLU funciton is 0 when its input is negative. (<font size=4> this means that the GRADIENT OF THE WEIGHT WILL ALSO BE ZERO, SO THE WEIGHT WONT CHANGE... AND THUS THE OUTPUT WONT CHANGE</FONT>)\n",
    "\n",
    "<font color=GREEN> To solve this problem you may want to use a variant of the ReLU function called the **leaky ReLU function**.\n",
    "- Leaky ReLU is defined as LeakyReLUα(z) = max(αz, z)\n",
    "- hyperparam α defines how much the function \"leaks\": it is the slope of the function for z greater than 0 and is typically set to 0.01\n",
    "- this small slope ensures that leaky ReLUs NEVER die; they can go into a long coma, but they have a chance to eventually wake up. \n",
    "- **Generally, leaky ReLUs outperform strict ReLU functions**\n",
    "- **Someitmes setting α = 0.2 (A HUGE LEAK) can even result in better results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEJCAYAAAC0U81tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW0QkxSi4RURSjIJbRCTFKLhFRFLM/wf74jU5HDqbzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2149a174710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set params\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTRUCTION PHASE\n",
    "\n",
    "<font color=red> NOTE: TensorFlow does not have a predefined function for leaky ReLUs, but it is easy enough to define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "#DEFINE ACTIVATION FUNCTION: LEAKY RELU\n",
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "# set input placeholders\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# define model layers variables\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "# define loss function variables\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# define GS optimizer variables\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "# define evaluation variables\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "# define initialization variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EVALUATION PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9044\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.951\n",
      "10 Batch accuracy: 0.96 Validation accuracy: 0.9666\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.972\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9748\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9764\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.978\n",
      "35 Batch accuracy: 0.96 Validation accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "#EVALUATE MODEL\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE EXPONENTIAL LINEAR UNIT (ELU)\n",
    "\n",
    "The Exponential Linear Unit (ELU) outperformed the ReLU variants in many examples: training time was reduced AND the neural network performed better in terms of accuracy. \n",
    "![](pictures/ELU.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.81606028, -0.93233236, -0.99084218,  5.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha*np.exp(z)-1,z)\n",
    "\n",
    "elu(np.array([-1,-2,-4, 5]),alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAELCAYAAADN4q16AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x214a103d358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z, alpha=1), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ELU funciton has a few major differences from the ReLU activation function:\n",
    "1. it takes on negative values when z is less than 0, which allows the unit to have an average output closer to 0... THIS HELPS ALLEVIATE VANISHING GRADUENTS PROBLEM EVEN MORE THAN LEAKY RELU... I THINK. α IS USUALLY SET TO 1\n",
    "2. IT HAS A NONZERO GRADIENT WHEN Z IS LESS THAN 0, WHICH AGAIN HELPS ADDRESS THE DYING NEURON ISSUE.\n",
    "3. The funciton is smooth everywhere, which helps speed of gradient descent, since it DOES NOT BOUNCE AS MUCH LEFT AND RIGHT.\n",
    "\n",
    "**Main drawbacks of the ELU are that it is slower to copmute than ReLU due to use the exponential function... HOWEVER THIS IS OFTEN COMPENSATED FOR FASTER CONVERGANCE AND TRAINING!!! But obviously at test time the ELU will still work a little slower.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consturciton phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set params\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# set input placeholders\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# define model layers variables\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "# define loss function variables\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# define GS optimizer variables\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "# define evaluation variables\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "# define initialization variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.88 Validation accuracy: 0.9036\n",
      "5 Batch accuracy: 0.92 Validation accuracy: 0.9356\n",
      "10 Batch accuracy: 0.96 Validation accuracy: 0.9522\n",
      "15 Batch accuracy: 0.98 Validation accuracy: 0.9626\n",
      "20 Batch accuracy: 0.98 Validation accuracy: 0.9658\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.969\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.9722\n",
      "35 Batch accuracy: 0.96 Validation accuracy: 0.9734\n"
     ]
    }
   ],
   "source": [
    "#EVALUATE MODEL\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>Which Activation Functions to Use?!?</font>\n",
    "\n",
    "So, which activation functions should I use for the **hidden layers** of my deep neural network?!?!? It will generally vary. **HOWEVER, this trend generally holds:**\n",
    "\n",
    "<font color=green size =6> ELU > leaky ReLU > ReLU > tanh > logistic </font>\n",
    "\n",
    "If you don’t want to tweak yet another hyperparameter, you may just use the default α values suggested earlier **(0.01 for the leaky ReLU, and 1 for ELU)**. If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "Although using He Initialization along with ELU can reduce vanishing/exploding gradient issues at the begining of training, it doesnt guaruntee that they wont come back during training... oh dear.\n",
    "\n",
    "**Batch Normalization was created to addresses vanishing/exploding gradients, more generally it addresses the problem that the distribution of each layers inputs changes during the training, as the parameters of the previous layers change (WHICH THEY CALL THE INTERNAL COVARIATE SHIFT PROBLEM).**\n",
    "\n",
    "The technique consists of adding an operation in the model just before the activation function of each layer:\n",
    "- simply zero-centering the inputs\n",
    "- then scaling and shifting the result using two new adjustable parameters per layer (ONE FOR SCLAING AND ONE FOR SHIFTING)... the model adjusts these paramaters...<font color=red> **In other words this operation lets the model learn the optimal scale and mean of the inputs fo each layer...**</font>\n",
    "\n",
    "In order to zero-center and normalize the inputs **the algorithm needs to estimate the inputs’ mean and standard deviation.** It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name “Batch Normalization”).\n",
    "\n",
    "![](Pictures/batchnorm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At test time, there is NO MINI BATCH to compute the empircal mean and standard dev, so instead you simply use the WHOLE training set's mean and standard deviation. That makes sense, as you want to scale and shift the testing data the exact same way you scale and shift the training data.**\n",
    "\n",
    "The training data means and st.dev is typically calced efficiently during training using a moving average. \n",
    "\n",
    "<font color=red size=4> So, in total, four parameters are learned for **EACH** Batch normalized layer:\n",
    "- γ (scale), \n",
    "- β (offset), \n",
    "- μ (mean), \n",
    "- σ (standard deviation)</font><font color=blue size=2>\n",
    "\n",
    "**The authors demonstrated that this technique considerably improved all the deep neural networks they experimented with.** \n",
    "- The vanishing gradients problem was strongly reduced, to the point that they could use saturating activation functions such as the tanh and even the logistic activation function. \n",
    "- The networks were also much less sensitive to the weight initialization.\n",
    "- They were able to use much larger learning rates, significantly speeding up the learning process. \n",
    "- ***Specifically, they note that “Applied to a state-of-the-art image classification model, Batch Normalization achieves the same\n",
    "accuracy with 14 times fewer training steps, and beats the original model by a significant margin.[…] Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.” ***\n",
    "\n",
    "**Finally, like a gift that keeps on giving, Batch Normalization also acts like a regularizer, reducing the need for other regularization techniques (such as dropout, described laterin the chapter).**\n",
    "\n",
    "Batch Normalization does, however, add some complexity to the model (although it removes the need for normalizing the input data since the first hidden layer will take care of that, provided it is batchnormalized). Moreover, there is a runtime penalty: the neural network makes slower predictions due\n",
    "to the extra computations required at each layer. **So if you need predictions to be lightning-fast, you may want to check how well plain ELU + He initialization perform before playing with Batch Normalization.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses tensorflow.contrib.layers.batch_norm() rather than tf.layers.batch_normalization() (which did not exist when this chapter was written). It is now preferable to use tf.layers.batch_normalization(), because anything in the contrib module may change or be deleted without notice. Instead of using the batch_norm() function as a regularizer parameter to the fully_connected() function, we now use batch_normalization() and we explicitly create a distinct layer. The parameters are a bit different, in particular:\n",
    "\n",
    "- decay is renamed to momentum,\n",
    "- is_training is renamed to training,\n",
    "- updates_collections is removed: the update operations needed by batch normalization are added to the UPDATE_OPS collection and you need to explicity run these operations during training (see the execution phase below),\n",
    "- we don't need to specify scale=True, as that is the default.\n",
    "\n",
    "Also note that in order to run batch norm just before each hidden layer's activation function, we apply the ELU activation function manually, right after the batch norm layer.\n",
    "\n",
    "Note: since the tf.layers.dense() function is incompatible with tf.contrib.layers.arg_scope() (which is used in the book), we now use python's functools.partial() function instead. It makes it easy to create a my_dense_layer() function that just calls tf.layers.dense() with the desired parameters automatically set (unless they are overridden when calling my_dense_layer()). As you can see, the code remains very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets walk through whats happening here:\n",
    "- training: Set to Either true or false. Will be used to tell the batch_normalization() function whether it should use the current mini-batch's mean and standard deviation (during training) or the running averages that it keeps track of (during testing). \n",
    "- bn1:  momentum is decay, which is used in the exponential decay formula. Exponential decay is used to calculated the running averages, and follows the following forumula, where Given a new value v, the running average $\\hat{v}$ is  updated through the equation\n",
    "![](pictures/homl_ch11_2expdec.jpg)\n",
    "***A good value to pic for momentum (or decay) is close to 1... like 0.999999. More 9s the larger the datasets and smaller minibatches.*** \n",
    "\n",
    "**Note that by default batch_norm() (AND MAYBE TRUE FOR batch_normalization() as well?!?) only centres, normalizes and shifts the inputs; IT DOES NOT SCALE. This makes sense for layers with no activation function or with the ReLU activation function, since the next layers weights can take of scaling.... BUT FOR ANY OTHER ACTIVATION FUNCTION YOU SHOULD ADD \"scale=True\" to the params of the batch_normalization funciton. **\n",
    "\n",
    "\n",
    "**To avoid repeating the same parameters over and over again, we can use Python's partial() function:**\n",
    "The partial() is used for partial function application which “freezes” some portion of a function’s arguments and/or keywords resulting in a new object with a simplified signature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "#reset graph and set network params\n",
    "reset_graph()\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "#set inputs and training placeholders\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "#define partial function so we dont have to repeat same code\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, \n",
    "                              training=training, \n",
    "                              momentum=0.9)\n",
    "\n",
    "#first layer\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "#second layer\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "#output layer\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------#\n",
    "#################### CONSTRUCITON PHASE ######################\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#reset graph and set network params\n",
    "reset_graph()\n",
    "n_inputs = 28*28 #MNIST data\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "batch_norm_momentum = 0.99\n",
    "learning_rate = 0.01\n",
    "\n",
    "#set placeholders for inputs\n",
    "X = tf.placeholder(tf.float32, shape=(None,n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "#define network architechture\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    #set He initialization param \n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    #defin batch normailization partial function\n",
    "    mikes_batch_norm_layer = partial(tf.layers.batch_normalization, \n",
    "                                  training=training, \n",
    "                                  momentum=batch_norm_momentum)\n",
    "    #def desne layer partial func\n",
    "    mikes_dense_layer = partial(tf.layers.dense, \n",
    "                                kernel_initializer=he_init)\n",
    "    \n",
    "    #first layer\n",
    "    hl1_pre = mikes_dense_layer(X, n_hidden1, name=\"hidden1\") #pre normalization and activation\n",
    "    hl1_post = tf.nn.elu(mikes_batch_norm_layer(hl1_pre)) #post normalization and activation\n",
    "    \n",
    "    #second layer\n",
    "    hl2_pre = mikes_dense_layer(hl1_post, n_hidden2, name=\"hidden2\")\n",
    "    hl2_post = tf.nn.elu(mikes_batch_norm_layer(hl2_pre))\n",
    "    \n",
    "    #output layer\n",
    "    logits_pre = mikes_dense_layer(hl2_post, n_outputs, name=\"outputs\")\n",
    "    logits_post = mikes_batch_norm_layer(logits_pre)\n",
    "    \n",
    "#define loss variable\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits_post)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "#minimize loss variable (single iteration)\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "#evaluation using in_top_k\n",
    "#in_top_k says whether the targets are in the top K predictions.\n",
    "#used for multi class classification\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits_post, y, 1) #logits in top 1 (so we want exact predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\") #tf.cast used to change type of outputs(in this case to floats)\n",
    "\n",
    "#initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.757729, accuracy: 0.8693, \n",
      "Epoch 2 loss: 0.558846, accuracy: 0.8976, \n",
      "Epoch 3 loss: 0.468253, accuracy: 0.9128, \n",
      "Epoch 4 loss: 0.405583, accuracy: 0.9219, \n",
      "Epoch 5 loss: 0.357831, accuracy: 0.9289, \n",
      "Epoch 6 loss: 0.321598, accuracy: 0.9367, \n",
      "Epoch 7 loss: 0.28966, accuracy: 0.939, \n",
      "Epoch 8 loss: 0.269497, accuracy: 0.9437, \n",
      "Epoch 9 loss: 0.247739, accuracy: 0.9472, \n",
      "Epoch 10 loss: 0.229867, accuracy: 0.9513, \n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------#\n",
    "###################### EVALUATION PHASE ######################\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "#NOTE: we are using tf.layers.batch_normalization() rather than tf.contrib.layers.batch_norm()\n",
    "#So we need to explicitly run the extra update operations needed by batch normalization \n",
    "#namely (sess.run([training_op, extra_update_ops],...).\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "#set model run params\n",
    "n_epochs = 10\n",
    "batch_size = 200\n",
    "\n",
    "#run dat model\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            #set training=True to make sure batch normalizatio works properly!\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True,\n",
    "                                X: X_batch,\n",
    "                                y: y_batch})\n",
    "        loss_test, acc_test = sess.run([loss, accuracy],feed_dict={X: mnist.test.images, y: mnist.test.labels}) \n",
    "        print(\"Epoch {0} loss: {1}, accuracy: {2}, \".format(epoch+1,str(loss_test),str(acc_test)))\n",
    "        \n",
    "    save_path = saver.save(sess, \"./elu_nn_model2.ckpt\") # refers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, pretty crappy results! We have done much better using much simpler models.\n",
    "\n",
    "While our model will improve if we run it for longer, it is also good to note that **Batch Norm and ELU work best with very deep networks, with many layers and nodes.**\n",
    "\n",
    "Additionally, notice that the **list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables.** If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Clipping\n",
    "\n",
    "A popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold (this is mostly useful for recurrent neural networks). This is called Gradient Clipping.** In general people now prefer Batch Normalization, but it’s still useful to know about Gradient Clipping and how to implement it.**\n",
    "\n",
    "In TensorFlow, the optimizer’s minimize() function takes care of both computing the gradients and applying them, so you must instead:\n",
    "- call the optimizer’s compute_gradients() method first, \n",
    "- then create an operation to clip the gradients using the clip_by_value() function, \n",
    "- and finally create an operation to apply the clipped gradients using the optimizer’s apply_gradients() method\n",
    "\n",
    "        threshold = 1.0\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "        for grad, var in grads_and_vars]\n",
    "        training_op = optimizer.apply_gradients(capped_gvs)\n",
    "- You would then run this training_op at every training step, as usual. It will compute the gradients,clip them between –1.0 and 1.0, and apply them. The threshold is a hyperparameter you can tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create a simple neural net for MNIST and add gradient clipping. The first part is the same as earlier (except we added a few more layers to demonstrate reusing pretrained models, see below):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0 #gradient clipping threshold\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "#gradient clipping\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "                  for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_gvs)\n",
    "              \n",
    "#evaluation variables\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32), name=\"accuracy\")\n",
    "\n",
    "#initialization variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver =tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.90288, accuracy: 0.3079, \n",
      "Epoch 2 loss: 0.789285, accuracy: 0.7928, \n",
      "Epoch 3 loss: 0.44974, accuracy: 0.8784, \n",
      "Epoch 4 loss: 0.343765, accuracy: 0.9003, \n",
      "Epoch 5 loss: 0.29559, accuracy: 0.9141, \n",
      "Epoch 6 loss: 0.27228, accuracy: 0.9178, \n",
      "Epoch 7 loss: 0.248331, accuracy: 0.9258, \n",
      "Epoch 8 loss: 0.229343, accuracy: 0.9323, \n",
      "Epoch 9 loss: 0.216602, accuracy: 0.9344, \n",
      "Epoch 10 loss: 0.206523, accuracy: 0.9387, \n"
     ]
    }
   ],
   "source": [
    "#run model\n",
    "n_epochs = 10\n",
    "batch_size=200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        loss_test, acc_test = sess.run([loss, accuracy],feed_dict={X: mnist.test.images, y: mnist.test.labels}) \n",
    "        print(\"Epoch {0} loss: {1}, accuracy: {2}, \".format(epoch+1,str(loss_test),str(acc_test)))\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor 'gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1:0' shape=(784, 300) dtype=float32>,\n",
       "  <tf.Variable 'hidden1/kernel:0' shape=(784, 300) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(300,) dtype=float32>,\n",
       "  <tf.Variable 'hidden1/bias:0' shape=(300,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1:0' shape=(300, 50) dtype=float32>,\n",
       "  <tf.Variable 'hidden2/kernel:0' shape=(300, 50) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(50,) dtype=float32>,\n",
       "  <tf.Variable 'hidden2/bias:0' shape=(50,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1:0' shape=(50, 50) dtype=float32>,\n",
       "  <tf.Variable 'hidden3/kernel:0' shape=(50, 50) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1:0' shape=(50,) dtype=float32>,\n",
       "  <tf.Variable 'hidden3/bias:0' shape=(50,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1:0' shape=(50, 50) dtype=float32>,\n",
       "  <tf.Variable 'hidden4/kernel:0' shape=(50, 50) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1:0' shape=(50,) dtype=float32>,\n",
       "  <tf.Variable 'hidden4/bias:0' shape=(50,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1:0' shape=(50, 50) dtype=float32>,\n",
       "  <tf.Variable 'hidden5/kernel:0' shape=(50, 50) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1:0' shape=(50,) dtype=float32>,\n",
       "  <tf.Variable 'hidden5/bias:0' shape=(50,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1:0' shape=(50, 10) dtype=float32>,\n",
       "  <tf.Variable 'outputs/kernel:0' shape=(50, 10) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>,\n",
       "  <tf.Variable 'outputs/bias:0' shape=(10,) dtype=float32_ref>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notice how the shape of the tensors matches the n_inputs and layer size (n_hidden1 for example)\n",
    "grads_and_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'hidden3/kernel:0',\n",
       " 'hidden3/bias:0',\n",
       " 'hidden4/kernel:0',\n",
       " 'hidden4/bias:0',\n",
       " 'hidden5/kernel:0',\n",
       " 'hidden5/bias:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there should be 12 variables\n",
    "[v.name for v in tf.trainable_variables()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
