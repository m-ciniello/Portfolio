{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88psfTJ3WO3F"
   },
   "source": [
    "# NLP Learning Series Section 2 - POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPGjEW4gWcGX"
   },
   "source": [
    "# Table of Contents\n",
    "0. Notebook Setup\n",
    "\n",
    "1. What is Parts-of-Speech (POS) tagging?\n",
    "    - 1.1 Basic Introduction and use cases\n",
    "    - 1.2 Experimenting with NLTK's built in POS tagger\n",
    "    - 1.3 Building a very simple probabilistic POS tagger with NLTK\n",
    "    \n",
    "\n",
    "2. Feature Extraction for POS tagging\n",
    "    - 2.1 shape funciton\n",
    "    - 2.2 POS_features function\n",
    "    \n",
    "\n",
    "3. Build a custom POS tagger\n",
    "    - Tagging and evaluating\n",
    "    - Putting it all together\n",
    "    - Training  and tuning the model\n",
    "    \n",
    "\n",
    "4. Build a POS tagger using NLTK\n",
    "\n",
    "\n",
    "5. Basic sentiment anlaysis (progressively add more complex features)\n",
    "    - Tweet sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCDIdLkT--vZ"
   },
   "source": [
    "\n",
    "# 0\\. Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3oDUb2qiEHZl"
   },
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihxw5CVg_C00"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import nltk\n",
    "\n",
    "# Download nltk packages\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtX9gNMDHn50"
   },
   "source": [
    "**Load Data**\n",
    "\n",
    "In this exercise, we will use the 'English Universal Dependencies' dataset for trianing and testing:\n",
    "- Train data:  12544 sentences, runtime=~60s \n",
    "- Test data: 2078 sentences, runtime=~10s \n",
    "\n",
    "The data is loaded as a list of lists, where each individual list item represents a sentence, and each sentence is broken down into a set of token/tag tuples as follows:\n",
    "\n",
    "```[('What', 'WP'),\n",
    " ('if', 'IN'),\n",
    " ('Google', 'NNP'),\n",
    " ('Morphed', 'VBD'),\n",
    " ('Into', 'IN'),\n",
    " ('GoogleOS', 'NNP'),\n",
    " ('?', '.')]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "n3-3AqP7EUht",
    "outputId": "8b3563de-5016-488c-ab95-843bb14397af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 12.6M  100 12.6M    0     0  9715k      0  0:00:01  0:00:01 --:--:-- 9708k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1622k  100 1622k    0     0  1756k      0 --:--:-- --:--:-- --:--:-- 1754k\n"
     ]
    }
   ],
   "source": [
    "# Run bash commands to get data from course site\n",
    "!curl -o train_raw.conllu http://vsarangian.com/NLPLearningSeries/datasets/en_ewt-ud-train.conllu\n",
    "!curl -o test_raw.conllu http://vsarangian.com/NLPLearningSeries/datasets/en_ewt-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "eswmCiYvFJ2-",
    "outputId": "16c5ca03-6a4a-4094-b6ff-86fc4a3d7562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 0.3187830448150635\n",
      "Runtime: 0.03853797912597656\n",
      "Train data size: 12543\n",
      "Test data size: 2077\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk import conlltags2tree\n",
    "from time import time\n",
    "def read_pos_data(filename):\n",
    "    \"\"\"\n",
    "    Iterate through the Universal Dependencies Corpus Part-Of-Speech data\n",
    "    Yield sentences one by one, don't load all the data in memory\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    current_sentence = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # ignore comments\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            # empty line indicates end of sentence\n",
    "            if not line:\n",
    "                yield current_sentence\n",
    "                # Reset current_sentence each time so yield works!\n",
    "                current_sentence = []\n",
    "                continue\n",
    "            annotations = line.split('\\t')\n",
    "            # Get only the word and the part of speech\n",
    "            current_sentence.append((annotations[1], annotations[4]))\n",
    "    print(\"Runtime:\", time()-t0)\n",
    "\n",
    "train_data = read_pos_data('train_raw.conllu')\n",
    "train_list = list(train_data) # ignore last empty value\n",
    "\n",
    "test_data = read_pos_data('test_raw.conllu')\n",
    "test_list = list(test_data) # ignore last empty value\n",
    "\n",
    "print(\"Train data size:\", len(train_list))\n",
    "print(\"Test data size:\", len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "bmtFxwOlp8oB",
    "outputId": "5a33f702-96e6-49ea-f3bd-8403c341a8b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', '-LRB-'),\n",
       " ('This', 'DT'),\n",
       " ('killing', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('respected', 'JJ'),\n",
       " ('cleric', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('causing', 'VBG'),\n",
       " ('us', 'PRP'),\n",
       " ('trouble', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('years', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('come', 'VB'),\n",
       " ('.', '.'),\n",
       " (']', '-RRB-')]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a sneak peak at the data\n",
    "train_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-_D5fFrbpQG"
   },
   "source": [
    "# 1\\. What is Parts-of-Speech (POS) Tagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Ofhtw5ZcHjk"
   },
   "source": [
    "## 1.1 POS Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYB1AU4J7F9W"
   },
   "source": [
    "\n",
    "Part of Speech Tagging (or POS Tagging, for short) is probably the most popular challenge in the history of NLP. The process basically involves **assigning a grammatical label to every word in a sequence (usually a sentence).** When I say grammatical label, I mean: *Noun, Verb, Preposition, Pronoun, etc.* \n",
    "\n",
    "In NLP, a collection of labels is called a **tag set**. There are multiple version of POS tag sets for various languages, but for the enlgish language the most widley used one is the **Penn Treebank Tag Set**. Below is a subset of the alphabetical list of part-of-speech tags used in the Penn Treebank Project (the full list can be found [here](https://www.clips.uantwerpen.be/pages/mbsp-tags)):\n",
    "\n",
    "![alt text](http://vsarangian.com/NLPLearningSeries/images/nlp2_pic1.jpg)\n",
    "\n",
    "\n",
    "**TO BE INCORPORATED: Your instinct now might be to run to your high school grammar book but don’t\n",
    "worry, you don’t really need to know what all those POS tags mean. In fact, not even\n",
    "all corpora implement this exact tag set, but rather a subset of it. For example, I’ve\n",
    "never encountered the LS (list item marker) or the PDT (predeterminer) anywhere.\n",
    "Part-Of-Speech tagging also serves as a base of deeper NLP analyses. There are just\n",
    "a few cases when you’ll work directly with the tagged sentence. A scenario that\n",
    "comes to mind is keyword extraction, when usually you only want to extract the\n",
    "adjectives and nouns. In this case, you use the tags to filter out those words that\n",
    "can’t be a keyword.  NER IS A MAJOR USE CASE. Named Entity Recognition (NER for short) is almost as well-known and studied as\n",
    "POS tagging. NER implies extracting named entities and their classes from a given\n",
    "text. The usual named entities we’re dealing with stand for: People, Organizations,\n",
    "Locations, Events, etc. Sometimes, things like currencies, numbers, percents, dates\n",
    "and time expressions can be considered named entities even though they technically\n",
    "aren’t. The entities are used in information extraction tasks and usually, these\n",
    "entities can be attributed to a real-life object or concept.**\n",
    "\n",
    "\n",
    "As you can imagine, it would be very difficult - and probably pretty ineffective - to assign a POS tag to each word in the english language. In fact, even if we could do that, some words have different tags based the context they are found in. \n",
    "\n",
    "To address this problem,  **we use machine learning to learn the important patterns associated with each POS tag.** The approach looks something like this:\n",
    "1. Get some humans to annotate some texts with POS tags (we’ll call this the gold standard)\n",
    "2. Extract a series of features from each word in the gold standard corpus\n",
    "3. Build some mathematical models to predict tags based on the extracted features\n",
    "3. Assess how well the model is performing on data the model hasn’t seen yet \n",
    "\n",
    "As you could probably tell, POS tagging is really just a simple a classification task. The most complicated part is often the process of extracting features from each word. A word's feature can include things like:\n",
    "- position in sentence\n",
    "- shape (capitalized, non-capitalized, CamelCase, etc.)\n",
    "- previous word\n",
    "- next word in sentence\n",
    "- end of sentence\n",
    "- begining of sentence\n",
    "\n",
    "There are various other types of features that can be extract from a single word, and we will go over these in the next section. For now, we will test out the **pre-trained POS tagger** from NLTK, just to get a sense of the kinds of outputs we should expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PJaVF7fcMKO"
   },
   "source": [
    "## 1.2 Experimenting with the NLTK POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4wo0OpVKu8c"
   },
   "source": [
    "Using the pretrained POS tagger from NLTK is as simple as passing a tokenized sentence into the POS tagging function, just as in the code below. The output should be a list of tuples, each with the individual token and the corresponding (predicted)  POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "Rbykv3JPJhN7",
    "outputId": "8206ac94-defa-4d7a-bb65-f7ad7e2c9251"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'NNP'),\n",
       " ('my', 'PRP$'),\n",
       " ('name', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('Mike', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('smartest', 'JJS'),\n",
       " ('man', 'NN'),\n",
       " ('alive', 'JJ'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import nltk\n",
    "\n",
    "# Create test sentence and tokenize\n",
    "sentence = \"Hey my name is Mike, and I am the smartest man alive!\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Use nltk's pos_tag pre trained model to tag the words \n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6OJagW2loYd"
   },
   "source": [
    "Note that NLTK's pretrianed POS tagger is not perfect, as it is trained on a general dataset that may be slightly different from the dataset in your specific use case.  \n",
    "\n",
    "To showcase this, let's try using **NLTK's POS tagger to tag all of the data in the English Universal Dependencies test dataset** we imported at the begining of this notebook. This should give us a good bench mark for our future, more customized models. We will use the 'test_list' data for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "raUo-jvKlhWg",
    "outputId": "c9abcd72-5daa-442a-88f8-ff7a35a346f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8495437701717337\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = []\n",
    "pos_predict = []\n",
    "pos_actual = []\n",
    "\n",
    "for sent in test_list:\n",
    "    # Get words/tags from sentences\n",
    "    words = [token[0] for token in sent]\n",
    "    tags = [token[1] for token in sent]\n",
    "    # Predict tags using nltk.pos_tag\n",
    "    predictions = [tag for word, tag in nltk.pos_tag(words)]\n",
    "    # Join all predictions and actuals into a single list\n",
    "    pos_predict.extend(predictions)\n",
    "    pos_actual.extend(tags)\n",
    "    \n",
    "# Calculate nltk.pos_tag prediction accuracy\n",
    "correct = [a==b for a,b in zip(pos_predict, pos_actual)]\n",
    "print(\"Accuracy:\", sum(correct)/len(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VwoVJEANXlX"
   },
   "source": [
    "Approximately 85% accuracy isn't bad! Especially for a pretrained model that has supposedly never seen our data before. That said, we can do much better by trained our own model with the English Universal Dependencies training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDKvCKuScMUI"
   },
   "source": [
    "## 1.3 Building a very simple n-gram POS tagger with NLTK\n",
    "Before we jump into building a POS tagger from scratch, we are going to build one more simple tagger to add another benchmark when evaluating our own models. The model we will build will just be a simple **n-grams** tagger, in which each tagger chooses a token's label based on the preceding `n` words' tags.  The model works as follows:\n",
    "\n",
    "1. **Train a trigram tagger with NLTK's `TrigramTagger()`:** When looking for a word w3, if we have already encountered a trigram of form: w1, w2, w3, with computed tags t1, t2, t3, we will output tag t3 for word w3. Otherwise, we fallback on the bigram tagger.\n",
    "2. **Train a bigram tagger with  NLTK's `BigramTagger()`:** When looking for a word w2, if we have already encountered a bigram of form: w1, w2, with computed tags t1, t2, we will output tag t2 for word w2. Otherwise, we fallback on the unigram tagger.\n",
    "3. **Train a unigram tagger with  NLTK's  `UnigramTagger()`:** When looking for a word w1, if we have already encountered that word and computed its tags t1, we will output tag t1 for word w1. Otherwise, we fallback on a default choice.\n",
    "4. **Train a default tagger with  NLTK's  `DefaultTagger()`:** Since all the above methods failed, we can output the most common tag in the dataset. \n",
    "\n",
    "Note that the methodology used for training and testing the n-grams tagger below is the same for NLTK tagger classes:\n",
    "- **Training:** the model is trained as soon as the class is initialized\n",
    "- **Tagging:** a trained model can tag new sentences by passing a list of tokens through the `tag` method\n",
    "- **Testing:** a trained model can be evaluated by passing unseen test data through the `evaluate` method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "KzmgQ2EO7a5P",
    "outputId": "8c8d7889-8b0d-4e6f-8a31-fe95374482f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 tags: [('NN', 26906), ('IN', 20715), ('DT', 16819), ('NNP', 12449), ('PRP', 12198)]\n",
      "Most Common Tag is:  NN\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from collections import Counter\n",
    "\n",
    "# First establish most common tag to train the 'default tagger'\n",
    "tag_counter = Counter() \n",
    "for sentence in train_list:\n",
    "    tag_counter.update([t for _, t in sentence])\n",
    "    \n",
    "# Take a look at most common tags\n",
    "print(\"Top 5 tags:\", tag_counter.most_common(5))\n",
    "\n",
    "most_common_tag = tag_counter.most_common()[0][0]\n",
    "print(\"Most Common Tag is: \", most_common_tag) # NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "colab_type": "code",
    "id": "-Jb4VjUo_njl",
    "outputId": "094d8207-b76b-422d-d8cf-50e465764d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "Training complete. Time=4.22s\n",
      "0.8630912061202534\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hey', 'UH'),\n",
       " ('everyone', 'NN'),\n",
       " ('!', '.'),\n",
       " ('My', 'PRP$'),\n",
       " ('name', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('Mike', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('best', 'JJS'),\n",
       " ('at', 'IN'),\n",
       " ('everything', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the 4 taggers\n",
    "print(\"Starting training ...\")\n",
    "t0 = time()\n",
    "tag0 = nltk.DefaultTagger(most_common_tag)\n",
    "tag1 = nltk.UnigramTagger(train_list, backoff=tag0)\n",
    "tag2 = nltk.BigramTagger(train_list, backoff=tag1)\n",
    "tag3 = nltk.TrigramTagger(train_list, backoff=tag2)\n",
    "t1 = time()\n",
    "print(\"Training complete. Time={0:.2f}s\".format(t1 - t0))\n",
    "\n",
    "# Compute test set accuracy\n",
    "print(tag3.evaluate(test_list))\n",
    "print()\n",
    "\n",
    "# Here's how to use our new tagger\n",
    "sentence = \"Hey everyone! My name is Mike and I am the best at everything!\"\n",
    "sent_tokens = nltk.word_tokenize(sentence)\n",
    "tag3.tag(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQ404Z_xBht7"
   },
   "source": [
    "Approximately 86% accuracy! **That's even better than our pretrained NLTK POS tagger!** However that is still not that great, especially if we want high accuracy to be able to parse entities (names, organization, places, etc) as we will see in the Named Entity Recognition section. That said, this is another useful benchmark. Next we will build our own model from scratch, and hopefully try to improve on this accuracy level! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrWMNhxrJgaQ"
   },
   "source": [
    "# 2\\. Feature Extraction for POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vzgIGceJuwF"
   },
   "source": [
    "In this section, we will create a series of functions that will extract features from set of tokens and tags, and convert those features into a vectorized format that can be ingested by any machine learning algorithm. The three functions are:\n",
    "1. `shape`: helper function that uses regex to extract information about individual word\n",
    "2. `pos_features`: helper function that **slides over the sentence extracting features (including the `shape` funciton output) for every word, taking into account the the tags already assigned (the history parameter).** The function returns a dictionary with the keys and values as the feature names and feature values, respectively. \n",
    "3. `tagged_to_dataset`: using pos_features function to take in training data and returns vectorized X, y data (if you want vectorized data, you can also just return the list of feature dictionaries by keeping 'vectorize' set to false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLSQntqzuUlZ"
   },
   "source": [
    "## 2.1 Extracting 'word shapes'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryC5lMy8Jn2x"
   },
   "source": [
    "One of the most powerful features for POS tagging is the **shape of the word.** Our implementation uses regex (which we covered in the last section) to extract a word's shapes and classify it into one of 10 unique word shape groups:\n",
    "- Numbers (1, 1.25, 100000)\n",
    "- Punctuation (. , ;)\n",
    "- Capitalized\n",
    "- UPPERCASE\n",
    "- lowercase\n",
    "- CamelCase\n",
    "- MiXEdCaSE\n",
    "- EndingWithADot.\n",
    "- ABB.RE.VI.ATION.\n",
    "- Contains-Hyphen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "P6cgtTfcN1B7",
    "outputId": "53a42f92-324e-4aba-895e-b43b99dc248a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This : capitalized\n",
      "is : lowercase\n",
      "the : lowercase\n",
      "1st : number\n",
      "test : lowercase\n",
      "of : lowercase\n",
      "Mike : capitalized\n",
      "'s : other\n",
      "CrAzY : camelcase\n",
      "word-shape : contains-hyphen\n",
      "FUNCTION : uppercase\n",
      ". : punct\n"
     ]
    }
   ],
   "source": [
    "# Import regex\n",
    "import re\n",
    "\n",
    "# Define shape function\n",
    "def shape(word):\n",
    "    if re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word):\n",
    "        return 'number'\n",
    "    elif re.match('\\W+$', word):\n",
    "        return 'punct'\n",
    "    elif re.match('[A-Z][a-z]+$', word):\n",
    "        return 'capitalized'\n",
    "    elif re.match('[A-Z]+$', word):\n",
    "        return 'uppercase'\n",
    "    elif re.match('[a-z]+$', word):\n",
    "        return 'lowercase'\n",
    "    elif re.match('[A-Z][a-z]+[A-Z][a-z]+[A-Za-z]*$', word):\n",
    "        return 'camelcase'\n",
    "    elif re.match('[A-Za-z]+$', word):\n",
    "        return 'mixedcase'\n",
    "    elif re.match('__.+__$', word):\n",
    "        return 'wildcard'\n",
    "    elif re.match('[A-Za-z0-9]+\\.$', word):\n",
    "        return 'ending-dot'\n",
    "    elif re.match('[A-Za-z0-9]+\\.[A-Za-z0-9\\.]+\\.$', word):\n",
    "        return 'abbreviation'\n",
    "    elif re.match('[A-Za-z0-9]+\\-[A-Za-z0-9\\-]+.*$', word):\n",
    "        return 'contains-hyphen'\n",
    "    return 'other'\n",
    "\n",
    "# Test it out\n",
    "sentence = \"This is the 1st test of Mike's CrAzY word-shape FUNCTION.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "for token in tokens:\n",
    "    print(token,\":\", shape(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvmYpivgDKjy"
   },
   "source": [
    "## 2.2 Extraction contextual features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bs18wdJEO12a"
   },
   "source": [
    "In addition to word shapes, we need to extract features that correspond to a given words' context (eg. the previous/next word in the sentence). These types of features are useful for words which may have multiple tags based on the context that they appear in. For instance, in the sentences below, we can see that the word 'work' can be either a noun ('NN') or a verb ('VBP') based on the context it is found in! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "gTkx0MqpRnp4",
    "outputId": "316f21de-3acb-4578-c31b-d34ab77099d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Work' as a NOUN: [('Mike', 'NN'), ('does', 'VBZ'), ('his', 'PRP$'), ('work', 'NN'), ('!', '.')]\n",
      "'Work' as a VERB: [('Mike', 'NNP'), (',', ','), ('you', 'PRP'), ('work', 'VBP'), ('so', 'RB'), ('hard', 'JJ'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "sent1 = nltk.word_tokenize(\"Mike does his work!\")\n",
    "sent2 = nltk.word_tokenize(\"Mike, you work so hard!\")\n",
    "\n",
    "print(\"'Work' as a NOUN:\",nltk.pos_tag(sent1))\n",
    "print(\"'Work' as a VERB:\",nltk.pos_tag(sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNnZ-35WUlmA"
   },
   "source": [
    "**THIS SECTION NEEDS SOME WORK:**\n",
    "To address this issue, for any given word in the sentence we will extract: \n",
    "- a) the two preceding and following words \n",
    "- b) the tags of the two preceding words (note that we only use the preceeding word tags, as we will NOT have the following word tags when we need to make predictions)\n",
    "\n",
    "We will create a function that loops through each word, stores the contextual features, and stores them into a dictionary. There are a few nuances that we will need to address here:\n",
    "- To handle the first and second words in the sentence, we will need to pad the sentence with `__START__` and `__END__` tokens.  \n",
    "- We will also need to pad the tags associated with each of the preceeding words. Note that we do not directly feed the tags in, but create a `history` variable that grows one tag at a time as the function loops through the words in the sentence. This is because when we need to make predictions (tag an untagged sentence) we will be predicting one tag at a time, and then adding that tag to the history variable. In effect we will be using our tag predictions for the first couple words to predict tags of tags of the following words. A basic implementaion of this idea is as follow: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "GZ2i-LhnT68w",
    "outputId": "f1c51170-001c-4a7f-9b87-74997607f7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike {'prev-word': '__START1__', 'prev-prev-word': '__START2__', 'next-word': 'is', 'next-next-word': 'the', 'prev-pos': '__START1__', 'prev-prev-pos': '__START2__'}\n",
      "is {'prev-word': 'Mike', 'prev-prev-word': '__START1__', 'next-word': 'the', 'next-next-word': 'smartest', 'prev-pos': 'NNP', 'prev-prev-pos': '__START1__'}\n",
      "the {'prev-word': 'is', 'prev-prev-word': 'Mike', 'next-word': 'smartest', 'next-next-word': 'and', 'prev-pos': 'NN', 'prev-prev-pos': 'NNP'}\n",
      "smartest {'prev-word': 'the', 'prev-prev-word': 'is', 'next-word': 'and', 'next-next-word': 'most', 'prev-pos': '.', 'prev-prev-pos': 'NN'}\n",
      "and {'prev-word': 'smartest', 'prev-prev-word': 'the', 'next-word': 'most', 'next-next-word': 'handsome', 'prev-pos': 'PRP$', 'prev-prev-pos': '.'}\n",
      "most {'prev-word': 'and', 'prev-prev-word': 'smartest', 'next-word': 'handsome', 'next-next-word': 'man', 'prev-pos': 'NN', 'prev-prev-pos': 'PRP$'}\n",
      "handsome {'prev-word': 'most', 'prev-prev-word': 'and', 'next-word': 'man', 'next-next-word': 'on', 'prev-pos': 'VBZ', 'prev-prev-pos': 'NN'}\n",
      "man {'prev-word': 'handsome', 'prev-prev-word': 'most', 'next-word': 'on', 'next-next-word': 'the', 'prev-pos': 'NNP', 'prev-prev-pos': 'VBZ'}\n",
      "on {'prev-word': 'man', 'prev-prev-word': 'handsome', 'next-word': 'the', 'next-next-word': 'planet', 'prev-pos': 'CC', 'prev-prev-pos': 'NNP'}\n",
      "the {'prev-word': 'on', 'prev-prev-word': 'man', 'next-word': 'planet', 'next-next-word': '!', 'prev-pos': 'PRP', 'prev-prev-pos': 'CC'}\n",
      "planet {'prev-word': 'the', 'prev-prev-word': 'on', 'next-word': '!', 'next-next-word': '__END1__', 'prev-pos': 'VBP', 'prev-prev-pos': 'PRP'}\n",
      "! {'prev-word': 'planet', 'prev-prev-word': 'the', 'next-word': '__END1__', 'next-next-word': '__END2__', 'prev-pos': 'DT', 'prev-prev-pos': 'VBP'}\n"
     ]
    }
   ],
   "source": [
    "# Create context extractor\n",
    "def extract_context(tokens, tag_history, index):\n",
    "  # Pad sentence with start and end tokens\n",
    "  tokens = ['__START2__', '__START1__'] + list(tokens) + ['__END1__', '__END2__']\n",
    "\n",
    "  # We will be looking two words back in history, so need to make sure we do not go out of bounds\n",
    "  tag_history = ['__START2__', '__START1__'] + list(tag_history)\n",
    "  # shift the index with 2, to accommodate the padding\n",
    "  index += 2\n",
    "  # Store features in a dictionary\n",
    "  context_features = {# Context words\n",
    "                      'prev-word': tokens[index - 1],\n",
    "                      'prev-prev-word': tokens[index - 2],\n",
    "                      'next-word': tokens[index + 1],\n",
    "                      'next-next-word': tokens[index + 2],\n",
    "                      # Historical tags\n",
    "                      'prev-pos': tag_history[-1],\n",
    "                      'prev-prev-pos': tag_history[-2]}\n",
    "  return context_features\n",
    "\n",
    "# Create tagged sentence (pretend this is our human tagged training data!)\n",
    "raw_sentence = \"Mike is the smartest and most handsome man on the planet!\"\n",
    "tokens = nltk.word_tokenize(raw_sentence)\n",
    "tags = [tag for token, tag in nltk.pos_tag(sent_tokens)]\n",
    "\n",
    "# Loop through each word and extract context\n",
    "tag_history = []\n",
    "for i, token in enumerate(tokens):\n",
    "  # Add another historical tag on each iteration\n",
    "  tag_history = tags[0:i]\n",
    "  # Extract features (add 2 to index to adjust for start/end tokens)\n",
    "  feats = extract_context(tokens, tag_history, i)\n",
    "  print(token, feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUN7qsiIjiXm"
   },
   "source": [
    "## 2.3 Creating the Feature Extraction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgd2kCQNjn0X"
   },
   "source": [
    "Now that we know how to extract word shapes as well as contextual features for each word in a sentence, we are going to put these things together to create our `pos_features` function. We will also add some additional features like the stem of words (we discussed word stemming and lemmatizing in the last section), as well as word suffixes, just to capute as much information as possible. The below implementation is very similar to the POS feature extractor used by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "id": "9EfUN6IAWyOi",
    "outputId": "c36f40bf-fb78-4850-ee58-9d7f9dd3a0a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike {'word': 'Mike', 'stem': 'mike', 'shape': 'capitalized', 'suffix-1': 'e', 'suffix-2': 'ke', 'suffix-3': 'ike', 'prev-word': '__START1__', 'prev-stem': '__start1__', 'prev-prev-word': '__START2__', 'prev-prev-stem': '__start2__', 'next-word': 'is', 'next-stem': 'is', 'next-next-word': 'the', 'next-next-stem': 'the', 'prev-pos': '__START1__', 'prev-prev-pos': '__START2__', 'prev-word+word': '__start1__+Mike'}\n",
      "is {'word': 'is', 'stem': 'is', 'shape': 'lowercase', 'suffix-1': 's', 'suffix-2': 'is', 'suffix-3': 'is', 'prev-word': 'Mike', 'prev-stem': 'mike', 'prev-prev-word': '__START1__', 'prev-prev-stem': '__start1__', 'next-word': 'the', 'next-stem': 'the', 'next-next-word': 'smartest', 'next-next-stem': 'smartest', 'prev-pos': 'NNP', 'prev-prev-pos': '__START1__', 'prev-word+word': 'mike+is'}\n",
      "the {'word': 'the', 'stem': 'the', 'shape': 'lowercase', 'suffix-1': 'e', 'suffix-2': 'he', 'suffix-3': 'the', 'prev-word': 'is', 'prev-stem': 'is', 'prev-prev-word': 'Mike', 'prev-prev-stem': 'mike', 'next-word': 'smartest', 'next-stem': 'smartest', 'next-next-word': 'and', 'next-next-stem': 'and', 'prev-pos': 'NN', 'prev-prev-pos': 'NNP', 'prev-word+word': 'is+the'}\n",
      "smartest {'word': 'smartest', 'stem': 'smartest', 'shape': 'lowercase', 'suffix-1': 't', 'suffix-2': 'st', 'suffix-3': 'est', 'prev-word': 'the', 'prev-stem': 'the', 'prev-prev-word': 'is', 'prev-prev-stem': 'is', 'next-word': 'and', 'next-stem': 'and', 'next-next-word': 'most', 'next-next-stem': 'most', 'prev-pos': '.', 'prev-prev-pos': 'NN', 'prev-word+word': 'the+smartest'}\n",
      "and {'word': 'and', 'stem': 'and', 'shape': 'lowercase', 'suffix-1': 'd', 'suffix-2': 'nd', 'suffix-3': 'and', 'prev-word': 'smartest', 'prev-stem': 'smartest', 'prev-prev-word': 'the', 'prev-prev-stem': 'the', 'next-word': 'most', 'next-stem': 'most', 'next-next-word': 'handsome', 'next-next-stem': 'handsom', 'prev-pos': 'PRP$', 'prev-prev-pos': '.', 'prev-word+word': 'smartest+and'}\n",
      "most {'word': 'most', 'stem': 'most', 'shape': 'lowercase', 'suffix-1': 't', 'suffix-2': 'st', 'suffix-3': 'ost', 'prev-word': 'and', 'prev-stem': 'and', 'prev-prev-word': 'smartest', 'prev-prev-stem': 'smartest', 'next-word': 'handsome', 'next-stem': 'handsom', 'next-next-word': 'man', 'next-next-stem': 'man', 'prev-pos': 'NN', 'prev-prev-pos': 'PRP$', 'prev-word+word': 'and+most'}\n",
      "handsome {'word': 'handsome', 'stem': 'handsom', 'shape': 'lowercase', 'suffix-1': 'e', 'suffix-2': 'me', 'suffix-3': 'ome', 'prev-word': 'most', 'prev-stem': 'most', 'prev-prev-word': 'and', 'prev-prev-stem': 'and', 'next-word': 'man', 'next-stem': 'man', 'next-next-word': 'on', 'next-next-stem': 'on', 'prev-pos': 'VBZ', 'prev-prev-pos': 'NN', 'prev-word+word': 'most+handsome'}\n",
      "man {'word': 'man', 'stem': 'man', 'shape': 'lowercase', 'suffix-1': 'n', 'suffix-2': 'an', 'suffix-3': 'man', 'prev-word': 'handsome', 'prev-stem': 'handsom', 'prev-prev-word': 'most', 'prev-prev-stem': 'most', 'next-word': 'on', 'next-stem': 'on', 'next-next-word': 'the', 'next-next-stem': 'the', 'prev-pos': 'NNP', 'prev-prev-pos': 'VBZ', 'prev-word+word': 'handsome+man'}\n",
      "on {'word': 'on', 'stem': 'on', 'shape': 'lowercase', 'suffix-1': 'n', 'suffix-2': 'on', 'suffix-3': 'on', 'prev-word': 'man', 'prev-stem': 'man', 'prev-prev-word': 'handsome', 'prev-prev-stem': 'handsom', 'next-word': 'the', 'next-stem': 'the', 'next-next-word': 'planet', 'next-next-stem': 'planet', 'prev-pos': 'CC', 'prev-prev-pos': 'NNP', 'prev-word+word': 'man+on'}\n",
      "the {'word': 'the', 'stem': 'the', 'shape': 'lowercase', 'suffix-1': 'e', 'suffix-2': 'he', 'suffix-3': 'the', 'prev-word': 'on', 'prev-stem': 'on', 'prev-prev-word': 'man', 'prev-prev-stem': 'man', 'next-word': 'planet', 'next-stem': 'planet', 'next-next-word': '!', 'next-next-stem': '!', 'prev-pos': 'PRP', 'prev-prev-pos': 'CC', 'prev-word+word': 'on+the'}\n",
      "planet {'word': 'planet', 'stem': 'planet', 'shape': 'lowercase', 'suffix-1': 't', 'suffix-2': 'et', 'suffix-3': 'net', 'prev-word': 'the', 'prev-stem': 'the', 'prev-prev-word': 'on', 'prev-prev-stem': 'on', 'next-word': '!', 'next-stem': '!', 'next-next-word': '__END1__', 'next-next-stem': '__end1__', 'prev-pos': 'VBP', 'prev-prev-pos': 'PRP', 'prev-word+word': 'the+planet'}\n",
      "! {'word': '!', 'stem': '!', 'shape': 'punct', 'suffix-1': '!', 'suffix-2': '!', 'suffix-3': '!', 'prev-word': 'planet', 'prev-stem': 'planet', 'prev-prev-word': 'the', 'prev-prev-stem': 'the', 'next-word': '__END1__', 'next-stem': '__end1__', 'next-next-word': '__END2__', 'next-next-stem': '__end2__', 'prev-pos': 'DT', 'prev-prev-pos': 'VBP', 'prev-word+word': 'planet+!'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def pos_features(tokens, index, tag_history):\n",
    "    \"\"\"\n",
    "    sentence = list of words: [word1, word2, ...]\n",
    "    index = the index of the word we want to extract features for\n",
    "    history = the list of predicted tags of the previous tokens\n",
    "    \"\"\"\n",
    "    # Pad the sequence with placeholders\n",
    "    # We will be looking at two words back and forward, so need to make sure we do not go out of bounds\n",
    "    tokens = ['__START2__', '__START1__'] + list(tokens) + ['__END1__', '__END2__']\n",
    "    # We will be looking two words back in history, so need to make sure we do not go out of bounds\n",
    "    tag_history = ['__START2__', '__START1__'] + list(tag_history)\n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    "    return {\n",
    "        # Intrinsic features\n",
    "        'word': tokens[index],\n",
    "        'stem': stemmer.stem(tokens[index]),\n",
    "        'shape': shape(tokens[index]),\n",
    "        # Suffixes\n",
    "        'suffix-1': tokens[index][-1],\n",
    "        'suffix-2': tokens[index][-2:],\n",
    "        'suffix-3': tokens[index][-3:],\n",
    "        # Context\n",
    "        'prev-word': tokens[index - 1],\n",
    "        'prev-stem': stemmer.stem(tokens[index - 1]),\n",
    "        'prev-prev-word': tokens[index - 2],\n",
    "        'prev-prev-stem': stemmer.stem(tokens[index - 2]),\n",
    "        'next-word': tokens[index + 1],\n",
    "        'next-stem': stemmer.stem(tokens[index + 1]),\n",
    "        'next-next-word': tokens[index + 2],\n",
    "        'next-next-stem': stemmer.stem(tokens[index + 2]),\n",
    "        # Historical features\n",
    "        'prev-pos': tag_history[-1],\n",
    "        'prev-prev-pos': tag_history[-2],\n",
    "        # Composite\n",
    "        'prev-word+word': tokens[index - 1].lower() + '+' + tokens[index]}\n",
    "\n",
    "# Create tagged sentence (pretend this is our human tagged training data!)\n",
    "raw_sentence = \"Mike is the smartest and most handsome man on the planet!\"\n",
    "tokens = nltk.word_tokenize(raw_sentence)\n",
    "tags = [tag for token, tag in nltk.pos_tag(sent_tokens)]\n",
    "\n",
    "# Loop through each word and extract context\n",
    "tag_history = []\n",
    "for i, token in enumerate(tokens):\n",
    "  # Add another historical tag on each iteration\n",
    "  tag_history = tags[0:i]\n",
    "  # Extract features (add 2 to index to adjust for start/end tokens)\n",
    "  feats = pos_features(tokens, i, tag_history)\n",
    "  print(token, feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qY6_MWP0JTGq"
   },
   "source": [
    "## 2.3 Vecotizer the data for ML ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEYxzfSuu_6R"
   },
   "source": [
    "Now that we have built our POS feature extractor, we need to build two functions to extract the features from an entire corpus of training data, and then vectorize the feature dictionaries to convert them to a format that can be ingested by our machine learning algorithm!\n",
    "\n",
    "To start, we will build a tags_to_dataset function, which will do the following:\n",
    "1. Take in a list of tokenized and tagged sentences as well as a 'feature_detector' (which will be our `pos_features` function)\n",
    "2. Loop through each sentence in the corpus\n",
    "3. Loop through each word in each sentence and extract the features\n",
    "\n",
    "The result should be a list of 204507 tuples which contain the features of the individual word (stored in a dictionary) and its corresponding tag (our classifier label, or y value). Note there are 204507 NON-UNIQUE tokens in the corpus, and we treat each one as a unique observation to be fed into our classifier!\n",
    "\n",
    "To start we will only use **1000 sentences** just to speed up the process. We will use the whole training set when we actually want to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "A2tXEX731E0r",
    "outputId": "6f44f4dc-0acc-4fec-91dd-7bd62d195d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences in subset: 1000\n",
      "Total Words: 21859\n",
      "[('Al', 'NNP'), ('-', 'HYPH'), ('Zaman', 'NNP'), (':', ':'), ('American', 'JJ'), ('forces', 'NNS'), ('killed', 'VBD'), ('Shaikh', 'NNP'), ('Abdullah', 'NNP'), ('al', 'NNP'), ('-', 'HYPH'), ('Ani', 'NNP'), (',', ','), ('the', 'DT'), ('preacher', 'NN'), ('at', 'IN'), ('the', 'DT'), ('mosque', 'NN'), ('in', 'IN'), ('the', 'DT'), ('town', 'NN'), ('of', 'IN'), ('Qaim', 'NNP'), (',', ','), ('near', 'IN'), ('the', 'DT'), ('Syrian', 'JJ'), ('border', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of non-unique words\n",
    "n_sents = 1000\n",
    "total_words = 0\n",
    "for i in train_list[:n_sents]:\n",
    "  total_words+=len(i)\n",
    "\n",
    "print(\"Total Sentences in subset:\", len(train_list[:n_sents]))\n",
    "print(\"Total Words:\", total_words)\n",
    "\n",
    "# As a reminder, print out what the first sentence in the trainin set looks like\n",
    "print(train_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBRU21ctUNH9"
   },
   "source": [
    "For our purposes, we will only use **1000 sentences** just to speed up the process. We will use the whole training set when we actually want to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "colab_type": "code",
    "id": "u8stF0oNwRr2",
    "outputId": "1e1f98e4-67b0-4845-b0d8-36834a99e085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'next-next-stem': 'zaman',\n",
       "   'next-next-word': 'Zaman',\n",
       "   'next-stem': '-',\n",
       "   'next-word': '-',\n",
       "   'prev-pos': '__START1__',\n",
       "   'prev-prev-pos': '__START2__',\n",
       "   'prev-prev-stem': '__start2__',\n",
       "   'prev-prev-word': '__START2__',\n",
       "   'prev-stem': '__start1__',\n",
       "   'prev-word': '__START1__',\n",
       "   'prev-word+word': '__start1__+Al',\n",
       "   'shape': 'capitalized',\n",
       "   'stem': 'al',\n",
       "   'suffix-1': 'l',\n",
       "   'suffix-2': 'Al',\n",
       "   'suffix-3': 'Al',\n",
       "   'word': 'Al'},\n",
       "  'NNP'),\n",
       " ({'next-next-stem': ':',\n",
       "   'next-next-word': ':',\n",
       "   'next-stem': 'zaman',\n",
       "   'next-word': 'Zaman',\n",
       "   'prev-pos': 'NNP',\n",
       "   'prev-prev-pos': '__START1__',\n",
       "   'prev-prev-stem': '__start1__',\n",
       "   'prev-prev-word': '__START1__',\n",
       "   'prev-stem': 'al',\n",
       "   'prev-word': 'Al',\n",
       "   'prev-word+word': 'al+-',\n",
       "   'shape': 'punct',\n",
       "   'stem': '-',\n",
       "   'suffix-1': '-',\n",
       "   'suffix-2': '-',\n",
       "   'suffix-3': '-',\n",
       "   'word': '-'},\n",
       "  'HYPH')]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the function to convert tags sentence to (features, tag) tuples\n",
    "def tags_to_dataset(tagged_sentences, feature_detector):\n",
    "  \"\"\"\n",
    "  Helper function:\n",
    "    Take in train data (tagged sentences) and return feature dict (pre-vectorized dataset)\n",
    "    eg. tagged_sentences = [[(word1, tag1),(word2, tag2)],[(word1, tag1),(word2, tag2)]]\n",
    "  \"\"\"\n",
    "  # Initialize empty featursets list\n",
    "  classifier_corpus = []\n",
    "  for sentence in tagged_sentences:\n",
    "      # Initialize empty history list (will be updated as loop through each word in sent)\n",
    "      tag_history = []\n",
    "      # Use zip(* ) to zip tokens & tags into two separate lists\n",
    "      sentence_tokens, sentence_tags = zip(*sentence)\n",
    "      # Loop through each word in sentence\n",
    "      # Duplicate words are kept because contexts (prev/post words) may differ\n",
    "      for index in range(len(sentence)):\n",
    "          # Use the feature detector (eg. pos_features) initialized with the class\n",
    "          featureset = feature_detector(sentence_tokens, index, tag_history)\n",
    "          # Update featursets list with tuple (featureset, tag)\n",
    "          classifier_corpus.append((featureset, sentence_tags[index]))\n",
    "          # Update history for next index word\n",
    "          tag_history.append(sentence_tags[index])\n",
    "  return classifier_corpus\n",
    "\n",
    "# Try extracting the features from a subset of the training data\n",
    "prevec_data = tags_to_dataset(train_list[:n_sents], pos_features)\n",
    "\n",
    "# Print the first two words!\n",
    "print(len(prevec_data))\n",
    "prevec_data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ehjm2sRi4rq6"
   },
   "source": [
    "Looks great! Now we can convert our whole training corpus int a list of features/tag tuples! The next step is to take this data and vectorize it so it can be ingested by a machine algorith. To do this, we will use sklearn's DictVectorizer (which we discussed in the last section). Note that when we want to make predictions, we need to use 'transform' and not 'fit_transform',  as we want our prediction data matrixes to have the same shape as the data we trained the model on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IkPIx-zq0nzI",
    "outputId": "5fb28db9-fd2b-4d9a-9636-05d174177d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (21859, 56941)\n",
      "X type: <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize sklearn DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dictvect = DictVectorizer()\n",
    "\n",
    "# Extract featursets and tags\n",
    "featuresets, y = zip(*prevec_data)\n",
    "\n",
    "# Vectorize the data\n",
    "X = dictvect.fit_transform(featuresets)\n",
    "print(\"X shape:\",X.shape)\n",
    "print(\"X type:\",type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDwp8Hppbytb"
   },
   "source": [
    "# 3\\. Build a custom POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJvQ2IQJTOAe"
   },
   "source": [
    "## 3.1 Tagging and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVLldMVfEo_9"
   },
   "source": [
    "Once we vectorized our data (the X values) and extracted the labels (the y values), all we really have to do is feed this data into a machine learning model. We assume you have a strong understanding of ML algos, and know how to propely train and fine tune a model, so we will not go over that here. \n",
    "\n",
    "The more challenging component of POS models is making predictions, which we will go over now. But first, lets train a simple model so we have something to make predictions with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "0r-Dj00iGJIS",
    "outputId": "38efb3ec-30c5-49a4-bd7a-f5ca6046bf4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use X and y values generated above to train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VLODI4zF0IM"
   },
   "source": [
    "Great, now that we have a model, let's figure out how to make predictions and evaluate the accuracy of our model. To do this, we will need four functions:\n",
    "- `tag`: tag all words in a sentence\n",
    "- `tag_sents`: tag multiple sentences\n",
    "- `accuracy`: calculate the accuracy (correct predictions / total observations)\n",
    "-  `evaluate`: use above functions to predict tags of test set and then return accuracy value\n",
    "\n",
    "The trickiest component of this process involves the `tag` function. After we predict a new tag, we append that tag to the `tag_history` argument. This argument is then fed back into the `pos_features` function for the next word in the sentenace. So when we train a model, we use actual human-tagged POS features to fill the `prev-pos` and `prev-prev-pos` elements of our feature dictionary. But when we predict tags, the predicted previous tags fill those feature slots!\n",
    "\n",
    "Note that because we have to tag each word individually, we also have to apply to the pos_features and vectorizing functions to each word individually, which can make the tagging process extremely slow. In the final version of the model, we will implement a option to turn the 'tag_history' parameter off. But for now we will stick with the basic implementation.\n",
    "\n",
    "Lets start by building the `tag` and `tag_sents` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "kpYsN2YkMnRh",
    "outputId": "0f1c362b-437e-4568-927e-dcabd3387aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tagger: [('Mike', 'IN'), ('is', 'VBZ'), ('the', 'DT'), ('smartest', 'NN'), ('and', 'CC'), ('most', 'JJS'), ('handsome', 'DT'), ('man', 'NN'), ('on', 'IN'), ('the', 'DT'), ('planet', 'NN'), ('!', '.')]\n",
      "NLTK tagger: [('Mike', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('smartest', 'JJS'), ('and', 'CC'), ('most', 'RBS'), ('handsome', 'JJ'), ('man', 'NN'), ('on', 'IN'), ('the', 'DT'), ('planet', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Create function to tag single word\n",
    "def tag(tokens):\n",
    "    \"\"\"\n",
    "    Tag untagged tokens, use predictions from previous tags as features. \n",
    "    \"\"\"\n",
    "    tags_predicted = []\n",
    "    for i in range(len(tokens)):\n",
    "        # Extract features from single word\n",
    "        single_feats_dict = pos_features(tokens=tokens, index=i, tag_history=tags_predicted)\n",
    "        # Vectorize single word feats\n",
    "        single_feats_vector = dictvect.transform(single_feats_dict)\n",
    "        # Use trained model to predict new tag\n",
    "        pred_tag = model.predict(single_feats_vector)[0]\n",
    "        # Append predicted tag to 'tags_predicted' arg to be fed into next feature set\n",
    "        tags_predicted.append(pred_tag)\n",
    "    return list(zip(tokens,tags_predicted))\n",
    "\n",
    "# Create function to tag multiple sentences\n",
    "def tag_sents(sent_tokens):\n",
    "    return [tag(tokens) for tokens in sent_tokens]\n",
    "  \n",
    "\n",
    "# Test out the tag function and compare the results to nltk's pos tagger!\n",
    "raw_sentences = [\"Mike is the smartest and most handsome man on the planet!\",\n",
    "                \"It is also true that Mike is the best Data Scientist of all time!\",\n",
    "                \"Oh and don't forget about how athletic and strong he is, obviously\"]\n",
    "tokens = [nltk.word_tokenize(sent) for sent in raw_sentences]\n",
    "\n",
    "print(\"Custom tagger:\", tag_sents(tokens)[0])\n",
    "print(\"NLTK tagger:\", nltk.pos_tag(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrXxd77VT2C-"
   },
   "source": [
    "Not bad! Remember, we are using a model trained on only 1000 sentences (which is not a lot of data in an NLP context). Let's now build the model accuracy and evaluate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "YSzs6ayy-9MB",
    "outputId": "9838742a-25eb-4879-dc87-7bd04fdaa5d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8048780487804879, Eval Runtime: 0.09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8048780487804879"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.util import untag\n",
    "from itertools import chain\n",
    "\n",
    "# Create accuracy function \n",
    "def accuracy(reference, test):\n",
    "    \"\"\"\n",
    "    Given a list of reference values and a corresponding list of test\n",
    "    values, return the fraction of corresponding values that are equal.\n",
    "    \"\"\"\n",
    "    if len(reference) != len(test):\n",
    "        raise ValueError(\"Lists must have the same length.\")\n",
    "    return sum(x == y for x, y in zip(reference, test)) / len(test)\n",
    "\n",
    "# Create evaluate function \n",
    "def evaluate(test_set, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluation function:\n",
    "      Returns 'accuracy' metric using sklearn built-in 'score' method.\n",
    "    \"\"\"\n",
    "    t0=time()\n",
    "    untagged_sents = [untag(sent) for sent in test_set]\n",
    "    # Use tagging function created above to retag (predcit)\n",
    "    retagged_sents = tag_sents(untagged_sents)\n",
    "    preds = list(chain(*retagged_sents))\n",
    "    actual = list(chain(*test_set))\n",
    "    score = accuracy(actual, preds)\n",
    "    if verbose:\n",
    "      print(\"Accuracy: {}, Eval Runtime: {:.2f}\".format(score, time()-t0))\n",
    "    return score\n",
    "\n",
    "tokens_test = [nltk.pos_tag(sent) for sent in tokens]\n",
    "evaluate(tokens_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhsTnpjWXV0_"
   },
   "source": [
    "## 3.2 Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMRrrA2EXZKP"
   },
   "source": [
    "Now that we have learned all the building blocks for POS tagging, lets put everything together into one big glorious `Custom_POS_Tagger` class! We have done so below using the functions we created above, with a few minor changes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWoUSC4ZaAD8"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.tag.util import untag\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "class Custom_POS_Tagger:\n",
    "    def __init__(self, \n",
    "                 feature_detector,\n",
    "                 classifier,\n",
    "                 vectorizer=None,\n",
    "                 sparse=True,\n",
    "                 scaler=None):\n",
    "   \n",
    "        # Save params\n",
    "        self.feature_detector = feature_detector\n",
    "        self.classifier = classifier\n",
    "        if vectorizer is None:\n",
    "          self.vectorizer = DictVectorizer(sparse=sparse)\n",
    "        else:\n",
    "          self.vectorizer = vectorizer \n",
    "        self.fitted = False\n",
    "        self.scaler=None\n",
    "        \n",
    "    def _tags_to_dataset(self, tagged_sentences):\n",
    "        \"\"\"\n",
    "        Helper function:\n",
    "          Take in train data (tagged sentences) and return feature dict (pre-vectorized dataset)\n",
    "          eg. tagged_sentences = [[(word1, tag1),(word2, tag2)],[(word1, tag1),(word2, tag2)]]\n",
    "        \"\"\"\n",
    "        # Initialize empty featursets list\n",
    "        classifier_corpus = []\n",
    "        for sentence in tagged_sentences:\n",
    "            # Initialize empty history list (will be updated as loop through each word in sent)\n",
    "            tag_history = []\n",
    "            # Use zip(* ) to zip tokens & tags into two separate lists\n",
    "            sentence_tokens, sentence_tags = zip(*sentence)\n",
    "            # Loop through each word in sentence\n",
    "            # Duplicate words are kept because contexts (prev/post words) may differ\n",
    "            for index in range(len(sentence)):\n",
    "                # Use the feature detector (eg. pos_features) initialized with the class\n",
    "                featureset = self.feature_detector(sentence_tokens, index, tag_history)\n",
    "                # Update featursets list with tuple (featureset, tag)\n",
    "                classifier_corpus.append((featureset, sentence_tags[index]))\n",
    "                # Update history for next index word\n",
    "                tag_history.append(sentence_tags[index])\n",
    "        return classifier_corpus\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        \"\"\"\n",
    "        Training function:\n",
    "          Uses fit method of self.classifier to fit model.          \n",
    "        \"\"\"\n",
    "        t0_model = time()\n",
    "        print(\"Commencing feature extraction...\")\n",
    "        # Transform batch into features dictionary\n",
    "        features_dict = self._tags_to_dataset(train_data)\n",
    "        featuresets, y = zip(*features_dict)\n",
    "        # Extract X and y values from features_dictionary\n",
    "        X = self.vectorizer.fit_transform(featuresets)\n",
    "        # Run fit method on model\n",
    "        print(\"Training model on features set size {}\".format(X.shape))\n",
    "        self.classifier.fit(X, y)\n",
    "        print(\"Training complete. Total runtime: {:.2f} sec\".format(time()-t0_model))\n",
    "        self.fitted = True\n",
    "        \n",
    "    def _is_trained(self):\n",
    "        if self.fitted == False:\n",
    "          raise ValueError(\"Train the model first dummy!\")\n",
    "          \n",
    "    def tag(self, tokens):\n",
    "        \"\"\"\n",
    "        Tag untagged tokens, use predictions from previous tags as features. \n",
    "        \"\"\"\n",
    "        self._is_trained() # Make sure model is trained\n",
    "        tags_predicted = []\n",
    "        for i in range(len(tokens)):\n",
    "            single_feats_dict = self.feature_detector(tokens, i, tags_predicted)\n",
    "            single_feats_vector = self.vectorizer.transform(single_feats_dict)\n",
    "            tags_predicted.append(self.classifier.predict(single_feats_vector)[0])\n",
    "        return list(zip(tokens,tags_predicted))\n",
    "    \n",
    "    def tag_sents(self, sent_tokens):\n",
    "        return [self.tag(tokens) for tokens in sent_tokens]\n",
    "    \n",
    "    def accuracy(self, reference, test):\n",
    "        \"\"\"\n",
    "        Given a list of reference values and a corresponding list of test\n",
    "        values, return the fraction of corresponding values that are equal.\n",
    "        \"\"\"\n",
    "        if len(reference) != len(test):\n",
    "            raise ValueError(\"Lists must have the same length.\")\n",
    "        return sum(x == y for x, y in zip(reference, test)) / len(test)\n",
    "    \n",
    "    def evaluate(self, test_set, verbose=True):\n",
    "        \"\"\"\n",
    "        Evaluation function:\n",
    "          Returns 'accuracy' metric using sklearn built-in 'score' method.\n",
    "        \"\"\"\n",
    "        t0=time()\n",
    "        untagged_sents = [untag(sent) for sent in test_set]\n",
    "        retagged_sents = self.tag_sents(untagged_sents)\n",
    "        preds = list(chain(*retagged_sents))\n",
    "        actual = list(chain(*test_set))\n",
    "        score = self.accuracy(actual, preds)\n",
    "        if verbose:\n",
    "            print(\"Accuracy: {}, Eval Runtime: {:.2f}\".format(score, time()-t0))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2UqeoxYBw--"
   },
   "source": [
    "## 3.4 Training and tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOBCRfw9cUCJ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.util import untag\n",
    "from itertools import chain\n",
    "from time import time\n",
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "_gL_r2dFhlMY",
    "outputId": "a9b535a9-bb6f-4f52-9a93-6412eecd8247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on features set size (204607, 256137)\n",
      "Training complete. Total runtime: 345.97 sec\n",
      "Accuracy: 0.8852850938359167, Eval Runtime: 110.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8852850938359167"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build RandomForest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = Custom_POS_Tagger(feature_detector=pos_features, classifier=RandomForestClassifier())\n",
    "model_rf.fit(train_list)\n",
    "model_rf.evaluate(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "ceZw8g8DjSSK",
    "outputId": "afdf19bb-54ec-4ae0-b8a9-42ce05f7da96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing feature extraction...\n",
      "Training model on features set size (204607, 256137)\n",
      "Training complete. Total runtime: 220.65 sec\n",
      "Accuracy: 0.8717775032872455, Eval Runtime: 108.51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8717775032872455"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Regularized RandomForest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = Custom_POS_Tagger(feature_detector=pos_features, classifier=RandomForestClassifier(max_depth=200)) \n",
    "model_rf.fit(train_list)\n",
    "model_rf.evaluate(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "xYLFr6sTmKGu",
    "outputId": "43c585ab-edef-4f9a-9705-97fe31f750db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing feature extraction...\n",
      "Training model on features set size (204607, 256137)\n",
      "Training complete. Total runtime: 54.81 sec\n",
      "Accuracy: 0.9373630314380205, Eval Runtime: 7.60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9373630314380205"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build SVC model\n",
    "from sklearn.svm import LinearSVC\n",
    "model_svc = Custom_POS_Tagger(feature_detector=pos_features, classifier=LinearSVC())\n",
    "model_svc.fit(train_list)\n",
    "model_svc.evaluate(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "SZwMdbmxyp45",
    "outputId": "aa54f4a3-2cc5-47fc-b9fb-9e11010a7901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing feature extraction...\n",
      "Training model on features set size (204607, 256137)\n",
      "Training complete. Total runtime: 21.81 sec\n",
      "Accuracy: 0.8730127106825517, Eval Runtime: 1376.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8730127106825517"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_mnb = Custom_POS_Tagger(feature_detector=pos_features, classifier=MultinomialNB())\n",
    "model_mnb.fit(train_list)\n",
    "model_mnb.evaluate(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhcOzNGYb6og",
    "toc-hr-collapsed": false
   },
   "source": [
    "# 4\\. Build a POS tagger using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxy_d7_Qcfko"
   },
   "source": [
    "In this sectoin, we are going to create our own POS tagger, just as we did above, however we are going to use NLTK's built in POS tagger training functionality to do so. We will then compare our custom model with the NLTK implementation and see how the accuracy levels and runtime values comapre!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PbYiWunreyS"
   },
   "source": [
    "**Here is the documentation:** \n",
    "\n",
    "`Init signature: \n",
    "ClassifierBasedPOSTagger(feature_detector=None, train=None, classifier_builder=<bound method NaiveBayesClassifier.train of <class 'nltk.classify.naivebayes.NaiveBayesClassifier'>>, classifier=None, backoff=None, cutoff_prob=None, verbose=False)\n",
    "\n",
    "Docstring:      A classifier based part of speech tagger.\n",
    "File:           /usr/local/lib/python3.6/dist-packages/nltk/tag/sequential.py\n",
    "Type:           ABCMeta`\n",
    "\n",
    "*NOTE: that its base class is the `ClassiferBasedTagger`, so it has all the same arguments with the addition of the `feature_detector`)*\n",
    "\n",
    "\n",
    "**Parameters**:\t\n",
    "- feature_detector – A function used to generate the featureset input for the classifier:: feature_detector(tokens, index, history) -> featureset *(ClassifierBasedPOSTagger has a default tagger that is very similar to the one we constructed above)*\n",
    "- train – A tagged corpus consisting of a list of tagged sentences, where each sentence is a list of (word, tag) tuples.\n",
    "- backoff – A backoff tagger, to be used by the new tagger if it encounters an unknown context.\n",
    "- classifier_builder – A function used to train a new classifier based on the data in train. It should take one argument, a list of labeled featuresets (i.e., (featureset, label) tuples).\n",
    "- classifier – The classifier that should be used by the tagger. This is only useful if you want to manually construct the classifier; normally, you would use train instead.\n",
    "- backoff – A backoff tagger, used if this tagger is unable to determine a tag for a given token.\n",
    "- cutoff_prob – If specified, then this tagger will fall back on its backoff tagger if the probability of the most likely tag is less than cutoff_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "V1yxDm3dhLVA",
    "outputId": "e7e8c20e-3043-4ce2-e3b0-0740a54eeb3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 7.268903017044067\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "t0 = time()\n",
    "model_nltk = ClassifierBasedPOSTagger(train=train_list)\n",
    "print(\"Runtime:\", time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "T4KQDL2FiARm",
    "outputId": "a640037b-9c8c-4001-8836-becb61168d31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 16.696187019348145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8973832344439373"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time()\n",
    "score = model_nltk.evaluate(test_list[0:1000])\n",
    "print(\"Runtime:\", time()-t0)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UP5AMtaZb96f"
   },
   "source": [
    "# 5\\. POS use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6npnbKtkH-p"
   },
   "source": [
    "As you can imagine, POS tagging serves many uses in NLP, and is the building block for many higher level concepts, particularily:\n",
    "- **Named Entity Recognition (NER)**: extracting important entities from text (like names, conmpanies, locations, etc.) by essentially grouping together POS tags\n",
    "- **Dependency Parsing**: parsing dependencies and parent-child relationships between words in a sentence\n",
    "\n",
    "We will learn about these more advanced concepts in later sections. For now, take a look at the outputs below using these pre-trained NLTK models to get a sense of what the outputs look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "colab_type": "code",
    "id": "GRAyRuSaDDic",
    "outputId": "c4aebef9-c935-4e0e-b301-5ac8fc52320b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Sentence:\n",
      " Hi! My name is Mike Ciniello and I am the smartest man alive. I work at KPMG Canada.\n",
      "\n",
      "Tokens:\n",
      " ['Hi', '!', 'My', 'name', 'is', 'Mike', 'Ciniello', 'and', 'I', 'am', 'the', 'smartest', 'man', 'alive', '.', 'I', 'work', 'at', 'KPMG', 'Canada', '.']\n",
      "\n",
      "POS Tokens:\n",
      " [('Hi', 'NN'), ('!', '.'), ('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Mike', 'NNP'), ('Ciniello', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('the', 'DT'), ('smartest', 'JJS'), ('man', 'NN'), ('alive', 'JJ'), ('.', '.'), ('I', 'PRP'), ('work', 'VBP'), ('at', 'IN'), ('KPMG', 'NNP'), ('Canada', 'NNP'), ('.', '.')]\n",
      "\n",
      "NER Tree:\n",
      " (S\n",
      "  (GPE Hi/NN)\n",
      "  !/.\n",
      "  My/PRP$\n",
      "  name/NN\n",
      "  is/VBZ\n",
      "  (PERSON Mike/NNP Ciniello/NNP)\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  the/DT\n",
      "  smartest/JJS\n",
      "  man/NN\n",
      "  alive/JJ\n",
      "  ./.\n",
      "  I/PRP\n",
      "  work/VBP\n",
      "  at/IN\n",
      "  (ORGANIZATION KPMG/NNP Canada/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Create sentence\n",
    "sentence = \"\"\"Hi! My name is Mike Ciniello and I am the smartest man alive. I work at KPMG Canada.\"\"\"\n",
    "print(\"Raw Sentence:\\n\", sentence)\n",
    "\n",
    "# Tokenize and pos tag\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(\"\\nTokens:\\n\", tokens)\n",
    "\n",
    "# Tag tokenized data\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(\"\\nPOS Tokens:\\n\", tagged_tokens)\n",
    "\n",
    "# Create NER tree\n",
    "ner_annotated_tree = nltk.ne_chunk(tagged_tokens)\n",
    "print(\"\\nNER Tree:\\n\",ner_annotated_tree)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Learning Series Section 2 - POS Tagging",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
