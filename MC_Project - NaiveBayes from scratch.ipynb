{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaiveBayes from Scratch\n",
    "\n",
    "SECTION 1: Guassian Naive Bayes\n",
    "- Leveraged code from: https://chrisalbon.com/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch/\n",
    "\n",
    "SECTION 2: Multinomial Naive Bayes\n",
    "- Leveraged code from: https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67\n",
    "\n",
    "Quick notes on the difference between the 3 main NB approaches:\n",
    "- **Bernoulli Naive Bayes**: It assumes that all our features are binary such that they take only two values. Means 0s can represent “word does not occur in the document” and 1s as \"word occurs in the document\" .\n",
    "\n",
    "- **Multinomial Naive Bayes** : Its is used when we have discrete data (e.g. movie ratings ranging 1 and 5 as each rating will have certain frequency to represent). In text learning we have the count of each word to predict the class or label.\n",
    "\n",
    "- **Gaussian Naive Bayes** : Because of the assumption of the normal distribution, Gaussian Naive Bayes is used in cases when all our features are continuous. For example in Iris dataset features are sepal width, petal width, sepal length, petal length. So its features can have different values in data set as width and length can vary. We can’t represent features in terms of their occurrences. This means data is continuous. Hence we use Gaussian Naive Bayes here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Guassian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes is simple classifier known for doing well when only a small number of observations is available. In this tutorial we will create a gaussian naive bayes classifier from scratch and use it to predict the class of a previously unseen data point. This tutorial is based on an example on Wikipedia’s naive bayes classifier page, I have implemented it in Python and tweaked some notation to improve explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>fs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>6.00</td>\n",
       "      <td>180</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>5.92</td>\n",
       "      <td>190</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>5.58</td>\n",
       "      <td>170</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>5.92</td>\n",
       "      <td>165</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>5.10</td>\n",
       "      <td>159</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female</td>\n",
       "      <td>5.10</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female</td>\n",
       "      <td>5.00</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>5.50</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female</td>\n",
       "      <td>5.42</td>\n",
       "      <td>130</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female</td>\n",
       "      <td>5.75</td>\n",
       "      <td>150</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender     h    w  fs\n",
       "0    male  6.00  180  12\n",
       "1    male  5.92  190  11\n",
       "2    male  5.58  170  12\n",
       "3    male  5.92  165  10\n",
       "4    male  5.10  159   9\n",
       "5  female  5.10  150   8\n",
       "6  female  5.00  100   6\n",
       "7  female  5.50  150   8\n",
       "8  female  5.42  130   7\n",
       "9  female  5.75  150   9"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#crete dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty dataframe\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Create our target variable\n",
    "data['Gender'] = ['male','male','male','male','male','female','female','female','female','female',]\n",
    "\n",
    "# Create our feature variables\n",
    "data['h'] = [6,5.92,5.58,5.92,5.10,5.10,5,5.5,5.42,5.75]\n",
    "data['w'] = [180,190,170,165,159,150,100,150,130,150]\n",
    "data['fs'] = [12,11,12,10,9,8,6,8,7,9]\n",
    "\n",
    "# View the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>fs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>130</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h    w  fs\n",
       "0  6  130   8"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create data for a new row entry \n",
    "# Create an empty dataframe\n",
    "person = pd.DataFrame()\n",
    "\n",
    "# Create some feature values for this single row\n",
    "person['h'] = [6]\n",
    "person['w'] = [130]\n",
    "person['fs'] = [8]\n",
    "\n",
    "# View the data \n",
    "person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "Bayes theorem is a famous equation that allows us to make predictions based on data. Here is the classic version of the Bayes theorem:\n",
    "\n",
    "$\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)\\,P(A)}{P(B)}}$\n",
    "\n",
    "This might be too abstract, so let us replace some of the variables to make it more concrete. In a bayes classifier, we are interested in finding out the class (e.g. male or female, spam or ham) of an observation given the data:\n",
    "\n",
    "$p(\\text{class} \\mid \\mathbf {\\text{data}} )={\\frac {p(\\mathbf {\\text{data}} \\mid \\text{class}) * p(\\text{class})}{p(\\mathbf {\\text{data}} )}}$\n",
    "\n",
    "where:\n",
    "- class is a particular class (e.g. male)\n",
    "- data is an observation’s data\n",
    "- p(class|data) is called the posterior\n",
    "- p(data|class) is called the likeliehood\n",
    "- p(class) is called the prior\n",
    "- p(data) is called the marginal probability\n",
    "\n",
    "In a bayes classifier:\n",
    "- **we calculate the posterior (technically we only calculate the numerator of the posterior, but ignore that for now) for every class for each observation.** \n",
    "- Then, classify the observation based on the class with the largest posterior value. \n",
    "\n",
    "#### In our example, we have one observation to predict and two possible classes (e.g. male and female), therefore we will calculate two posteriors: one for male and one for female.\n",
    "\n",
    "$p(\\text{person is male} \\mid \\mathbf {\\text{person’s data}} )={\\frac {p(\\mathbf {\\text{person’s data}} \\mid \\text{person is male}) * p(\\text{person is male})}{p(\\mathbf {\\text{person’s data}} )}}$\n",
    "\n",
    "$p(\\text{person is female} \\mid \\mathbf {\\text{person’s data}} )={\\frac {p(\\mathbf {\\text{person’s data}} \\mid \\text{person is female}) * p(\\text{person is female})}{p(\\mathbf {\\text{person’s data}} )}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Classifier\n",
    "A gaussian naive bayes is probably the most popular type of bayes classifier. To explain what the name means, let us look at what the bayes equations looks like when we apply our:\n",
    "- two classes (male and female) and \n",
    "- three feature variables (height, weight, and footsize):\n",
    "\n",
    "${\\displaystyle {\\text{posterior (male)}}={\\frac {P({\\text{male}})\\,p({\\text{height}}\\mid{\\text{male}})\\,p({\\text{weight}}\\mid{\\text{male}})\\,p({\\text{foot size}}\\mid{\\text{male}})}{\\text{marginal probability}}}}$\n",
    "\n",
    "${\\displaystyle {\\text{posterior (female)}}={\\frac {P({\\text{female}})\\,p({\\text{height}}\\mid{\\text{female}})\\,p({\\text{weight}}\\mid{\\text{female}})\\,p({\\text{foot size}}\\mid{\\text{female}})}{\\text{marginal probability}}}}$\n",
    "\n",
    "Now let us unpack the top equation a bit:\n",
    "- **PRIOR:** P(male) is the prior probabilities. It is, as you can see, simply the probability an observation is male. **This is just the number of males in the dataset divided by the total number of people in the dataset.**\n",
    "- **LIKELIHOOD:**  $p({\\text{height}}\\mid{\\text{female}})\\,p({\\text{weight}}\\mid{\\text{female}})\\,p({\\text{foot size}}\\mid{\\text{female}})$ is the likelihood. Notice that we have unpacked person’s data so it is now every feature in the dataset. We essentially multiply the probability of each variable occuring independently at the same time. The **“gaussian” and “naive”** come from two assumptions present in this likelihood:\n",
    "    - **ASSUMES INDEPENDENT FEATURES:** If you look each term in the likelihood you will notice that we assume each feature is uncorrelated from each other. That is, foot size is independent of weight or height etc.. This is obviously not true, and is a “naive” assumption - hence the name “naive bayes.”\n",
    "    - **ASSUMES FEATURE DATA IS NORMALLY DISTRIBUTED:** Second, we assume have that the value of the features (e.g. the height of women, the weight of women) **are normally (gaussian) distributed.** This means that p(height|female) is calculated by inputing the required parameters into the probability density function of the normal distribution:\n",
    "\n",
    "$p(\\text{height}\\mid\\text{female})=\\frac{1}{\\sqrt{2\\pi\\text{variance of female height in the data}}}\\,e^{ -\\frac{(\\text{observation’s height}-\\text{average height of females in the data})^2}{2\\text{variance of female height in the data}} }$\n",
    "- **MARGINAL PROBABILITY:** is probably one of the most confusing parts of bayesian approaches. In toy examples (including ours) it is completely possible to calculate the marginal probability. **However, in many real-world cases, it is either extremely difficult or impossible to find the value of the marginal probability (explaining why is beyond the scope of this tutorial).** This is not as much of a problem for our classifier as you might think. Why? Because *we don’t care what the true posterior value is, we only care which class has a the highest posterior value.* And because the marginal probability is the same for all classes:\n",
    "    - 1) we can ignore the denominator, \n",
    "    - 2) calculate only the posterior’s numerator for each class, and \n",
    "    - 3) pick the largest numerator. That is, we can ignore the posterior’s denominator and make a prediction solely on the relative values of the posterior’s numerator.\n",
    "\n",
    "Okay! Theory over. Now let us start calculating all the different parts of the bayes equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Priors\n",
    "Priors can be either:\n",
    "- constants (like in this lil example) or\n",
    "- probability distributions (like in MCMC models?!?!)\n",
    "\n",
    "In our example, this is simply the probability of being a gender. Calculating this is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male: 0.5\n",
      "Female: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Number of males\n",
    "n_male = data['Gender'][data['Gender'] == 'male'].count()\n",
    "\n",
    "# Number of males\n",
    "n_female = data['Gender'][data['Gender'] == 'female'].count()\n",
    "\n",
    "# Total rows\n",
    "total_ppl = data['Gender'].count()\n",
    "\n",
    "# Number of males divided by the total rows\n",
    "P_male = n_male/total_ppl\n",
    "\n",
    "# Number of females divided by the total rows\n",
    "P_female = n_female/total_ppl\n",
    "print(\"Male:\", P_male)\n",
    "print(\"Female:\", P_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Likelihood\n",
    "Remember that each term (eg. p(height|female)) in our likelehood is assumed to be a normal probability distribution function. For example:\n",
    "\n",
    "$p(\\text{height}\\mid\\text{female})=\\frac{1}{\\sqrt{2\\pi\\text{variance of female height in the data}}}\\,e^{ -\\frac{(\\text{observation’s height}-\\text{average height of females in the data})^2}{2\\text{variance of female height in the data}} }$\n",
    "\n",
    "This means that for each class (e.g. female) and feature (e.g. height) combination we need to calculate the variance and mean value from the data. Pandas makes this easy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>fs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>5.354</td>\n",
       "      <td>136.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>5.704</td>\n",
       "      <td>172.8</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            h      w    fs\n",
       "Gender                    \n",
       "female  5.354  136.0   7.6\n",
       "male    5.704  172.8  10.8"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data by gender and calculate the means of each feature\n",
    "data_means = data.groupby('Gender').mean()\n",
    "\n",
    "# View the values\n",
    "data_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>fs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>0.09308</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.14028</td>\n",
       "      <td>151.7</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              h      w   fs\n",
       "Gender                     \n",
       "female  0.09308  480.0  1.3\n",
       "male    0.14028  151.7  1.7"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data by gender and calculate the variance of each feature\n",
    "data_variance = data.groupby('Gender').var()\n",
    "\n",
    "# View the values\n",
    "data_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.36000000000001"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or you could do it manually for funsies\n",
    "#remember, this is the SAMPLE VARIANCE/STD DEV... so you do n-1 for denominator\n",
    "# HOWEVER, sklearns implementation DOES NOT USE DDOF, so we wont use it in our implementation\n",
    "feat = 'w'\n",
    "gend = 'male'\n",
    "degrees_of_freedom = 0\n",
    "\n",
    "select_data = data[data['Gender']==gend][feat]\n",
    "sum_of_squares = sum([(x-select_data.mean())**2 for x in select_data])\n",
    "variance = sum_of_squares/(select_data.count()-degrees_of_freedom)\n",
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h       0.112224\n",
       "w     121.360000\n",
       "fs      1.360000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mask = data.Gender =='female'\n",
    "m_mask = data.Gender =='male'\n",
    "\n",
    "data[m_mask].var(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can create all the variables we need. The code below might look complex but all we are doing is creating a variable out of each cell in both of the tables above.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_h_mean, m_w_mean, m_fs_mean = data[m_mask].mean()\n",
    "m_h_var, m_w_var, m_fs_var = data[m_mask].var(ddof=0)\n",
    "\n",
    "f_h_mean, f_w_mean, f_fs_mean = data[f_mask].mean()\n",
    "f_h_var, f_w_var, f_fs_var = data[f_mask].var(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alright, now we need to create a function to calculate the probability density of each of the terms of the likelehood (eg $p(\\text{height}\\mid\\text{female})$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>fs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>6.00</td>\n",
       "      <td>180</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>5.92</td>\n",
       "      <td>190</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>5.58</td>\n",
       "      <td>170</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>5.92</td>\n",
       "      <td>165</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>5.10</td>\n",
       "      <td>159</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female</td>\n",
       "      <td>5.10</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female</td>\n",
       "      <td>5.00</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>5.50</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female</td>\n",
       "      <td>5.42</td>\n",
       "      <td>130</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female</td>\n",
       "      <td>5.75</td>\n",
       "      <td>150</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender     h    w  fs\n",
       "0    male  6.00  180  12\n",
       "1    male  5.92  190  11\n",
       "2    male  5.58  170  12\n",
       "3    male  5.92  165  10\n",
       "4    male  5.10  159   9\n",
       "5  female  5.10  150   8\n",
       "6  female  5.00  100   6\n",
       "7  female  5.50  150   8\n",
       "8  female  5.42  130   7\n",
       "9  female  5.75  150   9"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\text{height}\\mid\\text{female})=\\frac{1}{\\sqrt{2\\pi\\text{variance of female height in the data}}}\\,e^{ -\\frac{(\\text{observation’s height}-\\text{average height of females in the data})^2}{2\\text{variance of female height in the data}} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a function that calculates p(x | y):\n",
    "def p_x_given_y(obs, mean_y, variance_y):\n",
    "    #input the argument into a probability desnsity function\n",
    "    denom = (np.sqrt(2*np.pi*variance_y))\n",
    "    exp = np.exp((-(obs-mean_y)**2)/(2*variance_y))\n",
    "    return 1/denom*exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy: 0.13088130319179492 Custom: 0.13088130319179492\n",
      "Scipy: 0.9893330841735296 Custom: 0.9893330841735296\n",
      "Scipy: 0.8060000587621732 Custom: 0.8060000587621733\n"
     ]
    }
   ],
   "source": [
    "# Check scipy.stats to make sure its working!\n",
    "from scipy.stats import norm\n",
    "dist = norm(loc=m_h_mean, scale=np.sqrt(m_h_var))\n",
    "for i in [5,5.5,6]:\n",
    "    print('Scipy:', dist.pdf(i), 'Custom:', p_x_given_y(i, m_h_mean, m_h_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Male height dist\n",
    "m_probs = []\n",
    "m_heights = np.arange(m_h_mean-1,m_h_mean+1.1,0.01)\n",
    "for h in m_heights:\n",
    "    m_probs.append(p_x_given_y(h, m_h_mean, m_h_var))\n",
    "\n",
    "# Female height dist\n",
    "f_probs = []\n",
    "f_heights = np.arange(f_h_mean-1,f_h_mean+1.1,0.01)\n",
    "for h in f_heights:\n",
    "    f_probs.append(p_x_given_y(h, f_h_mean, f_h_var))\n",
    "\n",
    "# ALL height dist\n",
    "all_h_mean = data['h'].mean()\n",
    "all_h_var = data['h'].var()\n",
    "all_probs = []\n",
    "all_heights = np.arange(all_h_mean-1,all_h_mean+1.1,0.01)\n",
    "for h in all_heights:\n",
    "    all_probs.append(p_x_given_y(h, all_h_mean, all_h_var))\n",
    "    \n",
    "plt.scatter(x=m_heights, y=m_probs, label=\"Male\")\n",
    "plt.scatter(x=f_heights, y=f_probs, label=\"Female\")\n",
    "plt.scatter(x=all_heights, y=all_probs, label=\"ALL\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Bayes Classifier To A New Data Point\n",
    "\n",
    "OKIE DOKE! Our bayes classifier is ready to go. Remember that since we can ignore the marginal probability (the denominator), what we are actually calculating is this: \n",
    "\n",
    "${\\displaystyle {\\text{numerator of the posterior}}={P({\\text{female}})\\,p({\\text{height}}\\mid{\\text{female}})\\,p({\\text{weight}}\\mid{\\text{female}})\\,p({\\text{foot size}}\\mid{\\text{female}})}{}}$\n",
    "\n",
    "${\\displaystyle {\\text{denominator of the posterior}}=p(X)={p({\\text{height}})\\,p({\\text{weight}})p({\\text{footsize}})}{}}$\n",
    "\n",
    "To do this, we just need to plug in the values of the unclassified person (height = 6), the variables of the dataset (e.g. mean of the female heigh, and the function (p_x_given_y) we made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>fs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>6.00</td>\n",
       "      <td>180</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>5.92</td>\n",
       "      <td>190</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>5.58</td>\n",
       "      <td>170</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>5.92</td>\n",
       "      <td>165</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>5.10</td>\n",
       "      <td>159</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female</td>\n",
       "      <td>5.10</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female</td>\n",
       "      <td>5.00</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>5.50</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female</td>\n",
       "      <td>5.42</td>\n",
       "      <td>130</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female</td>\n",
       "      <td>5.75</td>\n",
       "      <td>150</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender     h    w  fs\n",
       "0    male  6.00  180  12\n",
       "1    male  5.92  190  11\n",
       "2    male  5.58  170  12\n",
       "3    male  5.92  165  10\n",
       "4    male  5.10  159   9\n",
       "5  female  5.10  150   8\n",
       "6  female  5.00  100   6\n",
       "7  female  5.50  150   8\n",
       "8  female  5.42  130   7\n",
       "9  female  5.75  150   9"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>fs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>130</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h    w  fs\n",
       "0  6  130   8"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability male:  0.0004724061915678271\n",
      "Probability female:  0.9995275938084321\n"
     ]
    }
   ],
   "source": [
    "#Numerator of the posterior if the unclassified observation is a male\n",
    "posterior_numerator_male = P_male * p_x_given_y(person['h'], m_h_mean, m_h_var) *\\\n",
    "p_x_given_y(person['w'], m_w_mean, m_w_var) *\\\n",
    "p_x_given_y(person['fs'], m_fs_mean, m_fs_var)\n",
    "\n",
    "#Numerator of the posterior if the unclassified observation is a female\n",
    "posterior_numerator_female = P_female * p_x_given_y(person['h'], f_h_mean, f_h_var) *\\\n",
    "p_x_given_y(person['w'], f_w_mean, f_w_var) *\\\n",
    "p_x_given_y(person['fs'], f_fs_mean, f_fs_var)\n",
    "\n",
    "denominator = posterior_numerator_male + posterior_numerator_female\n",
    "print(\"Probability male: \", (posterior_numerator_male/denominator)[0])\n",
    "print(\"Probability female: \", (posterior_numerator_female/denominator)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because the numerator of the posterior for female is greater than male, then we predict that the person is female.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.drop('Gender', axis=1)\n",
    "y = data['Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995276020928201 0.0004723979071794123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "predictions = model.predict_proba(person)[0]\n",
    "print(predictions[0],predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NICE! The sklearn prediction probabilities match the probabilities produced by our custom model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> Section 2: Multinomial Naive Bayes </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will cover how you can implement a Multinomial Naive Bayes Classifier for the 20 Newsgroups dataset. The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: \n",
    "- one for training (or development)\n",
    "- the other one for testing (or for performance evaluation). \n",
    "\n",
    "The split between the train and test set is based upon a messages posted before and after a specific date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of main steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets go over the basics of a Multinomial Naive Bayes Classifier! Remember, this basic equation still holds:\n",
    "\n",
    "${\\displaystyle {\\text{posterior (classA)}}={\\frac {P({\\text{classA}})\\,p({\\text{feat1}}\\mid{\\text{classA}})\\,p({\\text{feat2}}\\mid{\\text{classA}})\\,p({\\text{feat3}}\\mid{\\text{classA}})}{\\text{marginal probability}}}}$\n",
    "\n",
    "However we will be replace the feature probabilities (in the numerator):\n",
    "This replaces the gaussian formula used with continuous variables:\n",
    "$p(\\text{feat_n}\\mid\\text{classA})=\\frac{1}{\\sqrt{2\\pi\\text{var of feat_n for classA}}}\\,e^{ -\\frac{(\\text{feat_n}-\\text{average of feat_n for classA})^2}{2\\text{variance of feat_n for classA}} }$\n",
    "\n",
    "### Step 1. Calclate probabilities of each word per class\n",
    "Since words are categorical features, the probability of each word per class is simply the count of the word in the class, divided by the total words in the class. \n",
    "\n",
    "So for **class j** and **word i**, the probability is given by:\n",
    "\n",
    "![](images/mnb_1.png)\n",
    "\n",
    "- where `word_ij` is the count of the word_i in class_j\n",
    "- and `word_j` is the count of ALL words in class_j\n",
    "\n",
    "However since some words have 0 counts, we will perform a Laplace Smoothing with a very smalle alpha term ɑ:\n",
    "\n",
    "![](images/mnb_2.png)\n",
    "\n",
    "- where **V** is an array of all the words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Calculate likliehood\n",
    "This is the exact same as how we calculated likliehood in the Gaussian example. All we do is multiply the conditional probabilities of each feature together (again, we assume independence of the features).\n",
    "\n",
    "![](images/mnb_3.png)\n",
    "- ∝ = proportional to\n",
    "- ∏ = product of all values in a range of series\n",
    "\n",
    "For example, in the example below, where we are trying to determine the likeliehood of a phrase belonging to the 'Sports' class, is simply the product of the conditional probilities of all the words in the phrase:\n",
    "\n",
    "![](images/mnb_4.png)\n",
    "![](images/mnb_5.png)\n",
    "\n",
    "These conditional probablities are simply the probabilities we calculated in the first step (we use these instead of the Gausian distribution probabilties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Laplace Smoothing\n",
    "\n",
    "In the final step we calculate the probabilities and compare which has a higher probability: **P(a very beautiful goal | Sports)** or **P(a very beautiful goal | Not Sports).**\n",
    "\n",
    "But we have a problem! The word “goal” does not exist in the category 'Not Sports', thus P(goal| Sports) = 0, leading to P(a very beautiful goal | Sports)=0. It is problematic when a **frequency-based probability is zero, because it will wipe out all the information in the other probabilities,** and we need to find a solution for this.\n",
    "\n",
    "A solution would be **Laplace smoothing**, which is a technique for smoothing categorical data:\n",
    "- A small-sample correction, or pseudo-count, will be incorporated in every probability estimate. Consequently, no probability will be zero.\n",
    "- Note: In statistics, additive smoothing, is also called Laplace smoothing\n",
    "\n",
    "#### Here's how Laplace Smoothing works:\n",
    "\n",
    "Given an observation x = (x1, …, xd) from a multinomial distribution with N trials and parameter vector θ = (θ1, …, θd), a \"smoothed\" version of the data gives the estimator:\n",
    "![](images/mnb_6.png)\n",
    "\n",
    "   - where the pseudo-count α > 0 is the smoothing parameter (α = 0 corresponds to no smoothing). \n",
    "   - and `d` is the number of words in the corpus\n",
    "\n",
    "In other words, we add alpha to every count so it’s never zero. To balance this, we add (alpha \\* the number of possible words) to the divisor, so the product will never be greater than 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution\n",
    "\n",
    "First we calculate the fraction of documents in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_obs: 11269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.04259472890229834,\n",
       " 2: 0.05155736977549028,\n",
       " 3: 0.05075871860857219,\n",
       " 4: 0.05208980388676901,\n",
       " 5: 0.051024935664211554,\n",
       " 6: 0.052533498979501284,\n",
       " 7: 0.051646108794036735,\n",
       " 8: 0.052533498979501284,\n",
       " 9: 0.052888455053687104,\n",
       " 10: 0.0527109770165942,\n",
       " 11: 0.05306593309078002,\n",
       " 12: 0.0527109770165942,\n",
       " 13: 0.05244475996095483,\n",
       " 14: 0.0527109770165942,\n",
       " 15: 0.052622237998047744,\n",
       " 16: 0.05315467210932647,\n",
       " 17: 0.04836276510781791,\n",
       " 18: 0.05004880646020055,\n",
       " 19: 0.04117490460555506,\n",
       " 20: 0.033365870973467035}"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "root = 'datasets/MNB/20news-bydate/matlab'\n",
    "train_label = open(os.path.join(root,'train.label'))\n",
    "\n",
    "# Extract labels\n",
    "lines = train_label.readlines()\n",
    "n_obs = len(lines)\n",
    "print(\"n_obs:\", n_obs)\n",
    "\n",
    "# Clean labels\n",
    "labels = [int(x.replace('\\n','')) for x in lines]\n",
    "\n",
    "# Extract p for each label\n",
    "label_proba = {}\n",
    "for i in range(1,21):\n",
    "    label_proba[i] = 0\n",
    "\n",
    "# Count the occurence of each class\n",
    "counter = Counter(labels)\n",
    "\n",
    "# Divide each element by total\n",
    "for key in counter:\n",
    "    counter[key] = counter[key]/n_obs\n",
    "\n",
    "pi = dict(counter.items())\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Distribution over Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docIdx</th>\n",
       "      <th>wordIdx</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1467340</th>\n",
       "      <td>11269</td>\n",
       "      <td>47387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467341</th>\n",
       "      <td>11269</td>\n",
       "      <td>48339</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467342</th>\n",
       "      <td>11269</td>\n",
       "      <td>48919</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467343</th>\n",
       "      <td>11269</td>\n",
       "      <td>51544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467344</th>\n",
       "      <td>11269</td>\n",
       "      <td>53958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         docIdx  wordIdx  count\n",
       "1467340   11269    47387      1\n",
       "1467341   11269    48339      1\n",
       "1467342   11269    48919      1\n",
       "1467343   11269    51544      1\n",
       "1467344   11269    53958      1"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "train_data = open(os.path.join(root,'train.data'))\n",
    "cols = ['docIdx', 'wordIdx', 'count']\n",
    "df = pd.read_csv(train_data, delimiter=' ', names=cols)\n",
    "\n",
    "# Data should be sparse matrix format!\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1467345, 4)\n",
      "docIdx      9166920894\n",
      "wordIdx     9271195849\n",
      "count          2765300\n",
      "classIdx      16684449\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docIdx</th>\n",
       "      <th>wordIdx</th>\n",
       "      <th>count</th>\n",
       "      <th>classIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    docIdx  wordIdx  count  classIdx\n",
       "0        1        1      4         1\n",
       "1        1        2      2         1\n",
       "2        1        3     10         1\n",
       "3        1        4      4         1\n",
       "4        1        5      2         1\n",
       "5        1        6      1         1\n",
       "6        1        7      1         1\n",
       "7        1        8      1         1\n",
       "8        1        9      3         1\n",
       "9        1       10      9         1\n",
       "10       1       11      2         1\n",
       "11       1       12     58         1\n",
       "12       1       13      1         1\n",
       "13       1       14      4         1\n",
       "14       1       15      2         1\n",
       "15       1       16     13         1\n",
       "16       1       17      6         1\n",
       "17       1       18      2         1\n",
       "18       1       19      4         1\n",
       "19       1       20      6         1\n",
       "20       1       21      1         1\n",
       "21       1       22      1         1\n",
       "22       1       23     54         1\n",
       "23       1       24      3         1\n",
       "24       1       25      3         1\n",
       "25       1       26      1         1\n",
       "26       1       27     11         1\n",
       "27       1       28      2         1\n",
       "28       1       29     90         1\n",
       "29       1       30     20         1\n",
       "..     ...      ...    ...       ...\n",
       "70       1       71      2         1\n",
       "71       1       72      2         1\n",
       "72       1       73      3         1\n",
       "73       1       74      2         1\n",
       "74       1       75      1         1\n",
       "75       1       76      1         1\n",
       "76       1       77      1         1\n",
       "77       1       78      1         1\n",
       "78       1       79      1         1\n",
       "79       1       80      1         1\n",
       "80       1       81     19         1\n",
       "81       1       82      2         1\n",
       "82       1       83      3         1\n",
       "83       1       84      1         1\n",
       "84       1       85      1         1\n",
       "85       1       86      1         1\n",
       "86       1       87      1         1\n",
       "87       1       88      5         1\n",
       "88       1       89      8         1\n",
       "89       1       90      2         1\n",
       "90       1       91      4         1\n",
       "91       1       92      3         1\n",
       "92       1       93     11         1\n",
       "93       1       94      1         1\n",
       "94       1       95      8         1\n",
       "95       1       96      1         1\n",
       "96       1       97      1         1\n",
       "97       1       98      2         1\n",
       "98       1       99      3         1\n",
       "99       1      100      3         1\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join labels to df\n",
    "df_y = pd.DataFrame({'classIdx':labels,'docIdx':range(1,len(labels)+1)})\n",
    "df = pd.merge(df, df_y)\n",
    "print(df.shape)\n",
    "print(df.sum())\n",
    "df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>wordIdx</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>53966</th>\n",
       "      <th>53967</th>\n",
       "      <th>53968</th>\n",
       "      <th>53969</th>\n",
       "      <th>53970</th>\n",
       "      <th>53971</th>\n",
       "      <th>53972</th>\n",
       "      <th>53973</th>\n",
       "      <th>53974</th>\n",
       "      <th>53975</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classIdx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.855542e-05</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>1.661627e-03</td>\n",
       "      <td>5.438638e-05</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>3.625960e-05</td>\n",
       "      <td>6.048302e-06</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>8.459224e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>6.042260e-09</td>\n",
       "      <td>6.042260e-09</td>\n",
       "      <td>6.042260e-09</td>\n",
       "      <td>6.042260e-09</td>\n",
       "      <td>6.042260e-09</td>\n",
       "      <td>6.042260e-09</td>\n",
       "      <td>6.042260e-09</td>\n",
       "      <td>6.042260e-09</td>\n",
       "      <td>6.042260e-09</td>\n",
       "      <td>6.042260e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.722740e-04</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>1.338166e-04</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>7.871890e-05</td>\n",
       "      <td>4.723449e-05</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>2.362118e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>7.871103e-09</td>\n",
       "      <td>7.871103e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.023768e-04</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>1.582136e-04</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>1.862158e-05</td>\n",
       "      <td>1.862158e-05</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>9.306135e-09</td>\n",
       "      <td>9.306135e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.907239e-05</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>1.727457e-05</td>\n",
       "      <td>8.641602e-06</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "      <td>8.632969e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.833066e-05</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.729877e-06</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>9.729877e-06</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.720157e-09</td>\n",
       "      <td>9.720157e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.772348e-04</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>4.659864e-04</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>1.238741e-04</td>\n",
       "      <td>1.770136e-05</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>5.898487e-09</td>\n",
       "      <td>5.898487e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>2.572542e-05</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>3.858170e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "      <td>1.285628e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.881972e-05</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>5.352815e-05</td>\n",
       "      <td>2.294500e-05</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "      <td>7.645786e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.173399e-04</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>3.353168e-05</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>2.515085e-05</td>\n",
       "      <td>8.389205e-06</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>8.380825e-09</td>\n",
       "      <td>8.380825e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.034546e-06</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>1.606107e-05</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>8.034546e-06</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>8.026520e-09</td>\n",
       "      <td>8.026520e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.337208e-06</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>1.266808e-05</td>\n",
       "      <td>5.065335e-05</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "      <td>6.330877e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.394759e-04</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>5.066200e-05</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>1.151350e-04</td>\n",
       "      <td>3.224113e-05</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>4.605218e-09</td>\n",
       "      <td>4.605218e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.503713e-05</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>1.669420e-05</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>4.172298e-05</td>\n",
       "      <td>1.669420e-05</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>8.342928e-09</td>\n",
       "      <td>8.342928e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.720143e-05</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>7.557535e-05</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>5.818854e-06</td>\n",
       "      <td>5.232318e-05</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>5.813041e-09</td>\n",
       "      <td>5.813041e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.816911e-04</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>1.291116e-04</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>1.349800e-04</td>\n",
       "      <td>5.282184e-05</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>5.868441e-09</td>\n",
       "      <td>5.868441e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>7.341390e-05</td>\n",
       "      <td>3.212116e-05</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>2.569372e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "      <td>4.588082e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.865371e-05</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>3.115735e-05</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>1.038925e-05</td>\n",
       "      <td>4.154141e-05</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>5.192027e-09</td>\n",
       "      <td>5.192027e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.683691e-05</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.315359e-05</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>2.947026e-05</td>\n",
       "      <td>1.105034e-04</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>3.687006e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.683323e-09</td>\n",
       "      <td>3.683323e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>1.132413e-04</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>4.928243e-06</td>\n",
       "      <td>8.370135e-05</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>4.923319e-09</td>\n",
       "      <td>4.923319e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.364584e-09</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>6.628862e-05</td>\n",
       "      <td>1.473653e-05</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>3.683028e-05</td>\n",
       "      <td>7.371948e-06</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>3.683028e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.371948e-06</td>\n",
       "      <td>7.371948e-06</td>\n",
       "      <td>1.473653e-05</td>\n",
       "      <td>7.371948e-06</td>\n",
       "      <td>7.371948e-06</td>\n",
       "      <td>7.371948e-06</td>\n",
       "      <td>7.371948e-06</td>\n",
       "      <td>7.371948e-06</td>\n",
       "      <td>7.371948e-06</td>\n",
       "      <td>7.371948e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 53975 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "wordIdx          1         2             3             4         5      \\\n",
       "classIdx                                                                 \n",
       "1         7.855542e-05  0.000381  1.661627e-03  5.438638e-05  0.000495   \n",
       "2         4.722740e-04  0.000464  7.871103e-09  1.338166e-04  0.000110   \n",
       "3         1.023768e-04  0.000642  9.306135e-09  1.582136e-04  0.000195   \n",
       "4         6.907239e-05  0.000268  8.632969e-09  8.632969e-09  0.000086   \n",
       "5         5.833066e-05  0.000321  9.720157e-09  9.729877e-06  0.000010   \n",
       "6         2.772348e-04  0.001309  5.898487e-09  4.659864e-04  0.000088   \n",
       "7         1.285628e-08  0.000360  1.285628e-08  2.572542e-05  0.000026   \n",
       "8         6.881972e-05  0.000413  7.645786e-09  7.645786e-09  0.000099   \n",
       "9         1.173399e-04  0.000562  8.380825e-09  3.353168e-05  0.000034   \n",
       "10        8.034546e-06  0.000265  8.026520e-09  1.606107e-05  0.000008   \n",
       "11        6.337208e-06  0.000424  6.330877e-09  6.330877e-09  0.000006   \n",
       "12        2.394759e-04  0.000414  4.605218e-09  5.066200e-05  0.000272   \n",
       "13        2.503713e-05  0.000275  8.342928e-09  1.669420e-05  0.000042   \n",
       "14        8.720143e-05  0.000227  5.813041e-09  7.557535e-05  0.000116   \n",
       "15        2.816911e-04  0.000481  5.868441e-09  1.291116e-04  0.000070   \n",
       "16        4.588082e-09  0.000564  7.341390e-05  3.212116e-05  0.000064   \n",
       "17        9.865371e-05  0.000171  5.192027e-09  3.115735e-05  0.000057   \n",
       "18        3.683691e-05  0.000567  3.683323e-09  3.315359e-05  0.000007   \n",
       "19        4.923319e-09  0.000192  4.923319e-09  1.132413e-04  0.000084   \n",
       "20        7.364584e-09  0.000331  6.628862e-05  1.473653e-05  0.000169   \n",
       "\n",
       "wordIdx      6             7             8         9             10     \\\n",
       "classIdx                                                                 \n",
       "1         0.000248  3.625960e-05  6.048302e-06  0.000205  8.459224e-04   \n",
       "2         0.000457  7.871890e-05  4.723449e-05  0.001354  2.362118e-05   \n",
       "3         0.000316  1.862158e-05  1.862158e-05  0.001340  9.306135e-09   \n",
       "4         0.000414  1.727457e-05  8.641602e-06  0.000414  8.632969e-09   \n",
       "5         0.000457  9.729877e-06  9.720157e-09  0.000457  9.720157e-09   \n",
       "6         0.000307  1.238741e-04  1.770136e-05  0.001398  5.898487e-09   \n",
       "7         0.000411  1.285628e-08  1.285628e-08  0.000360  3.858170e-05   \n",
       "8         0.000658  5.352815e-05  2.294500e-05  0.000138  7.645786e-09   \n",
       "9         0.000696  2.515085e-05  8.389205e-06  0.000034  8.380825e-09   \n",
       "10        0.002424  8.034546e-06  8.026520e-09  0.000016  8.026520e-09   \n",
       "11        0.001323  1.266808e-05  5.065335e-05  0.000025  6.330877e-09   \n",
       "12        0.000373  1.151350e-04  3.224113e-05  0.000511  4.605218e-09   \n",
       "13        0.000250  4.172298e-05  1.669420e-05  0.000242  8.342928e-09   \n",
       "14        0.000401  5.818854e-06  5.232318e-05  0.000145  5.813041e-09   \n",
       "15        0.000599  1.349800e-04  5.282184e-05  0.000141  5.868441e-09   \n",
       "16        0.000463  4.588082e-09  4.588082e-09  0.000078  2.569372e-04   \n",
       "17        0.000550  1.038925e-05  4.154141e-05  0.000099  5.192027e-09   \n",
       "18        0.000575  2.947026e-05  1.105034e-04  0.000037  3.687006e-06   \n",
       "19        0.000743  4.928243e-06  8.370135e-05  0.000049  4.923319e-09   \n",
       "20        0.000324  3.683028e-05  7.371948e-06  0.000140  3.683028e-05   \n",
       "\n",
       "wordIdx       ...              53966         53967         53968  \\\n",
       "classIdx      ...                                                  \n",
       "1             ...       6.042260e-09  6.042260e-09  6.042260e-09   \n",
       "2             ...       7.871103e-09  7.871103e-09  7.871103e-09   \n",
       "3             ...       9.306135e-09  9.306135e-09  9.306135e-09   \n",
       "4             ...       8.632969e-09  8.632969e-09  8.632969e-09   \n",
       "5             ...       9.720157e-09  9.720157e-09  9.720157e-09   \n",
       "6             ...       5.898487e-09  5.898487e-09  5.898487e-09   \n",
       "7             ...       1.285628e-08  1.285628e-08  1.285628e-08   \n",
       "8             ...       7.645786e-09  7.645786e-09  7.645786e-09   \n",
       "9             ...       8.380825e-09  8.380825e-09  8.380825e-09   \n",
       "10            ...       8.026520e-09  8.026520e-09  8.026520e-09   \n",
       "11            ...       6.330877e-09  6.330877e-09  6.330877e-09   \n",
       "12            ...       4.605218e-09  4.605218e-09  4.605218e-09   \n",
       "13            ...       8.342928e-09  8.342928e-09  8.342928e-09   \n",
       "14            ...       5.813041e-09  5.813041e-09  5.813041e-09   \n",
       "15            ...       5.868441e-09  5.868441e-09  5.868441e-09   \n",
       "16            ...       4.588082e-09  4.588082e-09  4.588082e-09   \n",
       "17            ...       5.192027e-09  5.192027e-09  5.192027e-09   \n",
       "18            ...       3.683323e-09  3.683323e-09  3.683323e-09   \n",
       "19            ...       4.923319e-09  4.923319e-09  4.923319e-09   \n",
       "20            ...       7.371948e-06  7.371948e-06  1.473653e-05   \n",
       "\n",
       "wordIdx          53969         53970         53971         53972  \\\n",
       "classIdx                                                           \n",
       "1         6.042260e-09  6.042260e-09  6.042260e-09  6.042260e-09   \n",
       "2         7.871103e-09  7.871103e-09  7.871103e-09  7.871103e-09   \n",
       "3         9.306135e-09  9.306135e-09  9.306135e-09  9.306135e-09   \n",
       "4         8.632969e-09  8.632969e-09  8.632969e-09  8.632969e-09   \n",
       "5         9.720157e-09  9.720157e-09  9.720157e-09  9.720157e-09   \n",
       "6         5.898487e-09  5.898487e-09  5.898487e-09  5.898487e-09   \n",
       "7         1.285628e-08  1.285628e-08  1.285628e-08  1.285628e-08   \n",
       "8         7.645786e-09  7.645786e-09  7.645786e-09  7.645786e-09   \n",
       "9         8.380825e-09  8.380825e-09  8.380825e-09  8.380825e-09   \n",
       "10        8.026520e-09  8.026520e-09  8.026520e-09  8.026520e-09   \n",
       "11        6.330877e-09  6.330877e-09  6.330877e-09  6.330877e-09   \n",
       "12        4.605218e-09  4.605218e-09  4.605218e-09  4.605218e-09   \n",
       "13        8.342928e-09  8.342928e-09  8.342928e-09  8.342928e-09   \n",
       "14        5.813041e-09  5.813041e-09  5.813041e-09  5.813041e-09   \n",
       "15        5.868441e-09  5.868441e-09  5.868441e-09  5.868441e-09   \n",
       "16        4.588082e-09  4.588082e-09  4.588082e-09  4.588082e-09   \n",
       "17        5.192027e-09  5.192027e-09  5.192027e-09  5.192027e-09   \n",
       "18        3.683323e-09  3.683323e-09  3.683323e-09  3.683323e-09   \n",
       "19        4.923319e-09  4.923319e-09  4.923319e-09  4.923319e-09   \n",
       "20        7.371948e-06  7.371948e-06  7.371948e-06  7.371948e-06   \n",
       "\n",
       "wordIdx          53973         53974         53975  \n",
       "classIdx                                            \n",
       "1         6.042260e-09  6.042260e-09  6.042260e-09  \n",
       "2         7.871103e-09  7.871103e-09  7.871103e-09  \n",
       "3         9.306135e-09  9.306135e-09  9.306135e-09  \n",
       "4         8.632969e-09  8.632969e-09  8.632969e-09  \n",
       "5         9.720157e-09  9.720157e-09  9.720157e-09  \n",
       "6         5.898487e-09  5.898487e-09  5.898487e-09  \n",
       "7         1.285628e-08  1.285628e-08  1.285628e-08  \n",
       "8         7.645786e-09  7.645786e-09  7.645786e-09  \n",
       "9         8.380825e-09  8.380825e-09  8.380825e-09  \n",
       "10        8.026520e-09  8.026520e-09  8.026520e-09  \n",
       "11        6.330877e-09  6.330877e-09  6.330877e-09  \n",
       "12        4.605218e-09  4.605218e-09  4.605218e-09  \n",
       "13        8.342928e-09  8.342928e-09  8.342928e-09  \n",
       "14        5.813041e-09  5.813041e-09  5.813041e-09  \n",
       "15        5.868441e-09  5.868441e-09  5.868441e-09  \n",
       "16        4.588082e-09  4.588082e-09  4.588082e-09  \n",
       "17        5.192027e-09  5.192027e-09  5.192027e-09  \n",
       "18        3.683323e-09  3.683323e-09  3.683323e-09  \n",
       "19        4.923319e-09  4.923319e-09  4.923319e-09  \n",
       "20        7.371948e-06  7.371948e-06  7.371948e-06  \n",
       "\n",
       "[20 rows x 53975 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alpha value for smoothing\n",
    "a = 0.001\n",
    "\n",
    "#Calculate probability of each word based on class\n",
    "pb_ij = df.groupby(['classIdx','wordIdx'])\n",
    "pb_j = df.groupby(['classIdx'])\n",
    "Pr =  (pb_ij['count'].sum() + a) / (pb_j['count'].sum() + 16689)    \n",
    "\n",
    "#Unstack series\n",
    "Pr = Pr.unstack()\n",
    "\n",
    "#Replace NaN or columns with 0 as word count with a/(count+|V|+1)\n",
    "for c in range(1,21):\n",
    "    Pr.loc[c,:] = Pr.loc[c,:].fillna(a/(pb_j['count'].sum()[c] + 16689))\n",
    "\n",
    "#Convert to dictionary for greater speed\n",
    "Pr_dict = Pr.to_dict()\n",
    "\n",
    "Pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stop words are words that show up a lot in every document (e.g. prepositions and pronouns). Lets get rid of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n"
     ]
    }
   ],
   "source": [
    "#Common stop words from online\n",
    "stop_words = [\n",
    "\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "\"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",    \n",
    "\"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n",
    "\"this\", \"those\", \"though\", \"through\", \"throughout\",\n",
    "\"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n",
    "\"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "\"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "\"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "\"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "\"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "\"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "]\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61188, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>archive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>resources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>alt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       word\n",
       "0      1    archive\n",
       "1      2       name\n",
       "2      3    atheism\n",
       "3      4  resources\n",
       "4      5        alt"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let’s create the vocabulary dataframe\n",
    "vocab = open(os.path.join(root,'vocabulary.txt')) \n",
    "vocab_df = pd.read_csv(vocab, names = ['word']) \n",
    "vocab_df = vocab_df.reset_index() \n",
    "vocab_df['index'] = vocab_df['index'].apply(lambda x: x+1) \n",
    "print(vocab_df.shape)\n",
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the counts of each word in the vocabulary and setting stop words to 0: \n",
    "\n",
    "#Index of all words\n",
    "tot_list = set(vocab_df['index'])\n",
    "\n",
    "#Index of good words\n",
    "vocab_df = vocab_df[~vocab_df['word'].isin(stop_words)]\n",
    "good_list = vocab_df['index'].tolist()\n",
    "good_list = set(good_list)\n",
    "\n",
    "#Index of stop words\n",
    "bad_list = tot_list - good_list\n",
    "\n",
    "#Set all stop words to 0\n",
    "for bad in bad_list:\n",
    "    for j in range(1,21):\n",
    "        Pr_dict[j][bad] = a/(pb_j['count'].sum()[j] + 16689)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1025, 2, 2562, 1028, 1033]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bad_list)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.042259563386324e-09\n",
      "6.042259563386324e-09\n",
      "6.042259563386324e-09\n",
      "6.042259563386324e-09\n",
      "6.042259563386324e-09\n",
      "6.042259563386324e-09\n",
      "6.042259563386324e-09\n",
      "6.042259563386324e-09\n",
      "6.042259563386324e-09\n",
      "6.042259563386324e-09\n"
     ]
    }
   ],
   "source": [
    "for bad in list(bad_list)[0:10]:\n",
    "    print(Pr_dict[1][bad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docIdx</th>\n",
       "      <th>wordIdx</th>\n",
       "      <th>count</th>\n",
       "      <th>classIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   docIdx  wordIdx  count  classIdx\n",
       "0       1        1      4         1\n",
       "1       1        2      2         1\n",
       "2       1        3     10         1\n",
       "3       1        4      4         1\n",
       "4       1        5      2         1"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/MNB/20news-bydate/matlab'"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate IDF \n",
    "tot = len(df['docIdx'].unique()) \n",
    "pb_ij = df.groupby(['wordIdx']) \n",
    "IDF = np.log(tot/pb_ij['docIdx'].count()) \n",
    "IDF_dict = IDF.to_dict()\n",
    "\n",
    "def MNB(df, smooth = False, IDF = False):\n",
    "    '''\n",
    "    Multinomial Naive Bayes classifier\n",
    "    :param df [Pandas Dataframe]: Dataframe of data\n",
    "    :param smooth [bool]: Apply Smoothing if True\n",
    "    :param IDF [bool]: Apply Inverse Document Frequency if True\n",
    "    :return predict [list]: Predicted class ID\n",
    "    '''\n",
    "    #Using dictionaries for greater speed\n",
    "    df_dict = df.to_dict()\n",
    "    new_dict = {}\n",
    "    prediction = []\n",
    "    \n",
    "    #new_dict = {docIdx : {wordIdx: count},....}\n",
    "    for idx in range(len(df_dict['docIdx'])):\n",
    "        docIdx = df_dict['docIdx'][idx]\n",
    "        wordIdx = df_dict['wordIdx'][idx]\n",
    "        count = df_dict['count'][idx]\n",
    "        try: \n",
    "            new_dict[docIdx][wordIdx] = count \n",
    "        except:\n",
    "            new_dict[df_dict['docIdx'][idx]] = {}\n",
    "            new_dict[docIdx][wordIdx] = count\n",
    "\n",
    "    #Calculating the scores for each doc\n",
    "    for docIdx in range(1, len(new_dict)+1):\n",
    "        score_dict = {}\n",
    "        #Creating a probability row for each class\n",
    "        for classIdx in range(1,21):\n",
    "            score_dict[classIdx] = 1\n",
    "            #For each word:\n",
    "            for wordIdx in new_dict[docIdx]:\n",
    "                #Check for frequency smoothing\n",
    "                #log(1+f)*log(Pr(i|j))\n",
    "                if smooth: \n",
    "                    try:\n",
    "                        probability=Pr_dict[wordIdx][classIdx]         \n",
    "                        power = np.log(1+ new_dict[docIdx][wordIdx])     \n",
    "                        #Check for IDF\n",
    "                        if IDF:\n",
    "                            score_dict[classIdx]+=(\n",
    "                               power*np.log(\n",
    "                               probability*IDF_dict[wordIdx]))\n",
    "                        else:\n",
    "                            score_dict[classIdx]+=power*np.log(\n",
    "                                                   probability)\n",
    "                    except:\n",
    "                        #Missing V will have log(1+0)*log(a/16689)=0 \n",
    "                        score_dict[classIdx] += 0                        \n",
    "                #f*log(Pr(i|j))\n",
    "                else: \n",
    "                    try:\n",
    "                        probability = Pr_dict[wordIdx][classIdx]        \n",
    "                        power = new_dict[docIdx][wordIdx]               \n",
    "                        score_dict[classIdx]+=power*np.log(\n",
    "                                           probability) \n",
    "                        #Check for IDF\n",
    "                        if IDF:\n",
    "                            score_dict[classIdx]+= power*np.log(\n",
    "                                   probability*IDF_dict[wordIdx]) \n",
    "                    except:\n",
    "                        #Missing V will have 0*log(a/16689) = 0\n",
    "                        score_dict[classIdx] += 0      \n",
    "            #Multiply final with pi         \n",
    "            score_dict[classIdx] +=  np.log(pi[classIdx])                          \n",
    "\n",
    "        #Get class with max probabilty for the given docIdx \n",
    "        max_score = max(score_dict, key=score_dict.get)\n",
    "        prediction.append(max_score)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Error:\t\t 3.4608217233117404 %\n",
      "Smooth Error:\t\t 1.8546454876209069 %\n",
      "IDF Error:\t\t 3.478569527021031 %\n",
      "Both Error:\t\t 1.8546454876209069 %\n"
     ]
    }
   ],
   "source": [
    "# Comparing the effects of smoothing and IDF:\n",
    "regular_predict = MNB(df, smooth=False, IDF=False)\n",
    "smooth_predict  = MNB(df, smooth=True, IDF=False)\n",
    "tfidf_predict   = MNB(df, smooth=False, IDF=True)\n",
    "all_predict     = MNB(df, smooth=True, IDF=True)\n",
    "#Get list of labels\n",
    "train_label = pd.read_csv('datasets/MNB/20news-bydate/matlab/train.label',\n",
    "                          names=['t'])\n",
    "train_label= train_label['t'].tolist()\n",
    "total = len(train_label) \n",
    "models = [regular_predict, smooth_predict, \n",
    "          tfidf_predict, all_predict] \n",
    "strings = ['Regular', 'Smooth', 'IDF', 'Both'] \n",
    " \n",
    "for m,s in zip(models,strings):\n",
    "    val = 0\n",
    "    for i,j in zip(m, train_label):\n",
    "        if i != j:\n",
    "            val +=1\n",
    "        else:\n",
    "            pass   \n",
    "    print(s,\"Error:\\t\\t\",val/total * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\t 22.571618920719516 %\n"
     ]
    }
   ],
   "source": [
    "#Get test data\n",
    "test_data = open('datasets/MNB/20news-bydate/matlab/test.data')\n",
    "df = pd.read_csv(test_data, delimiter=' ', names=['docIdx', 'wordIdx', 'count'])\n",
    "\n",
    "#Get list of labels\n",
    "test_label = pd.read_csv('datasets/MNB/20news-bydate/matlab/test.label', names=['t'])\n",
    "test_label= test_label['t'].tolist()\n",
    "\n",
    "#MNB Calculation\n",
    "predict = MNB(df, smooth = True, IDF = False)\n",
    "\n",
    "total = len(test_label)\n",
    "val = 0\n",
    "for i,j in zip(predict, test_label):\n",
    "    if i == j:\n",
    "        val +=1\n",
    "    else:\n",
    "        pass\n",
    "print(\"Error:\\t\",(1-(val/total)) * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
