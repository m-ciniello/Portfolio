{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "In the previous missions, we learned how the **linear regression model estimates the relationship between the feature columns and the target column and how we can use that for making predictions**. In this mission and the next, we'll discuss the 2 most common ways for finding the optimal parameter values for a linear regression model. <font color=red>Each combination of unique parameter values forms a unique linear regression model, and the process of finding these optimal values is known as model fitting. **Both approaches to model fitting we'll explore aim to minimize the following function:**</font>\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} ({\\hat{y_i} - y_i})^2$\n",
    "\n",
    "This function is the mean squared error between the predicted labels made using a given model and the true labels. <font color=green>**The problem of choosing a set of values that minimize or maximize another function is known as an optimization problem.</font>**\n",
    "\n",
    "To build intuition for the optimization process, let's start with a single parameter linear regression model:\n",
    "\n",
    "$\\hat{y} = a_1x_1$\n",
    "\n",
    "Note that this is different from a simple linear regression model, which actually has two parameters: x0 and x1.\n",
    "\n",
    "$\\hat{y} = a_1x_1 + a_0$\n",
    "\n",
    "Let's use the Gr Liv Area column for the single parameter:\n",
    "\n",
    "$\\hat{SalePrice} = a_1 * Gr Liv Area$\n",
    "\n",
    "\n",
    "test\n",
    "![](download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Single Variable Gradient Descent\n",
    "\n",
    "In the last screen's widget, **we observed how the optimization function follows a curve with a minimum value.** <font color='red'>This should remind you of our exploration of relative minimum values from calculus. **If you recall, we computed the critical points by calculating the curve's derivative, setting it equal to 0, and finding the x value at this point.**</font> *Unfortunately, this approach won't work when we have multiple parameter values because minimizing one parameter value may increase another parameter's value.* In addition, while we can plot the MSE curve when we only have a single parameter we're trying to find and visually select the value that minimizes the MSE, **this approach won't work when we have multiple parameter value because we can't visualize past 3 dimensions.**\n",
    "\n",
    "In this mission, we'll explore an iterative technique for solving this problem, known as gradient descent. **The gradient descent algorithm works by iteratively trying different parameter values until the model with the lowest mean squared error is found.** <font color = red>Gradient descent is a commonly used optimization technique for other models as well, like neural networks, which we'll explore later in this track.</font>\n",
    "\n",
    "Here's an overview of the gradient descent algorithm for a single parameter linear regression model:\n",
    "\n",
    "- select initial values for the parameter: $a_1$\n",
    "- repeat until convergence (usually implemented with a max number of iterations):\n",
    "  - calculate the error (MSE) of model that uses current parameter value: $MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)} ) ^2$\n",
    "  - calculate the derivative of the error (MSE) at the current parameter value: $\\frac{d}{da_1} MSE(a_1)$\n",
    "  - update the parameter value by subtracting the derivative times a constant ($\\alpha$, called the learning rate): $a_1 := a_1 - \\alpha \\frac{d}{da_1} MSE(a_1)$\n",
    "  \n",
    "In the last step of the algorithm, you'll notice we used we used := to indicate that the value on the right is assigned to the variable on the left. While in Python, we've used to the equals operator (=) for assignment, we've used it in math (=) to signify equality. For example, a = 1 in Python assigns the value 1 to the variable a. In math, a=1 asserts that a is equal to 1. In mathematical papers, sometimes ‚Üê is also used to signify assignment:\n",
    "\n",
    "$a_1 \\leftarrow a_1 - \\alpha \\frac{d}{da_1} MSE(a_1)$\n",
    "\n",
    "**Selecting an appropriate initial parameter and learning rate will reduce the number of iterations required to converge, and is part of hyperparameter optimization.** We won't dive into those techniques in this course and will instead focus on how the algorithm works. In the next screen, we'll unpack how to calculate the derivative of the error function at each iteration of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Derivative of the Cost Function\n",
    "\n",
    "In mathematical optimization, a function that we optimize through minimization is known as a **cost function or sometime as the loss function**. Because we're trying to fit a single parameter model, we can replace with $\\hat{y}^{(i)}$ with $a_1x_1^{(i)}$ in the cost function:\n",
    "\n",
    "$MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} (a_1x_1^{(i)} - y^{(i)} ) ^2$\n",
    "\n",
    "In this screen, we'll apply calculus properties to simplify this derivative to something we can compute. We encourage you to follow along using pencil and paper, and see if you can apply the properties we mention at each step to obtain the same result we did. Note that while you'll probably never have to implement gradient descent yourself (as most packages have high performance implementations), understanding the math will help make it easier for you to debug when you run into issues.\n",
    "\n",
    "$\\frac{d}{da_1} MSE(a_1) = \\frac{d}{da_1} \\frac{1}{n} \\sum_{i=1}^{n} (a_1x_1^{(i)} - y^{(i)} ) ^2$\n",
    "\n",
    "\n",
    "By applying the linearity of differentiation property from calculus, we can bring the derivative term inside the summation:\n",
    "\n",
    "$\\frac{d}{da_1} MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{d}{da_1} (a_1x_1^{(i)} - y^{(i)} ) ^2$\n",
    "\n",
    "By applying the power rule we get:\n",
    "\n",
    "$\\frac{d}{da_1} MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} 2(a_1x_1^{(i)} - y^{(i)})  \\frac{d}{da_1} (a_1x_1^{(i)} - y^{(i)} )$ \n",
    "\n",
    "Because we're differentiating $a_1x_1^{(i)} - y^{(i)}$ with respect to a1, we treat y(i) and x1(i) as constants. $\\frac{d}{da_1} (a_1x_1^{(i)} - y^{(i)})$ then simplifies to just $x_1^{(i)}$:\n",
    "\n",
    "$\\frac{d}{da_1} MSE(a_1) = \\frac{2}{n} \\sum_{i=1}^{n} x_1^{(i)}(a_1x_1^{(i)} - y^{(i)})$\n",
    "\n",
    "For every iteration of gradient descent:\n",
    "\n",
    "- this derivative is computed using the current a1 value\n",
    "- the derivative is multiplied by the learning rate $\\alpha \\frac{d}{da_1} MSE(a_1)$\n",
    "- the result is subtracted from the current parameter value and assigned as the new parameter value:$ a_1 := a_1 - \\alpha \\frac{d}{da_1} MSE(a_1)$\n",
    "\n",
    "Here's what this would look like in code if we ran gradient descent for 10 iterations:\n",
    "\n",
    "    a1_list = [1000]\n",
    "    alpha = 10\n",
    "\n",
    "    for x in range(0, 10):\n",
    "        a1 = a1_list[x]\n",
    "        deriv = derivative(a1, alpha, xi_list, yi_list)\n",
    "        a1_new = a1 - alpha*deriv\n",
    "        a1_list.append(a1_new)\n",
    "        \n",
    "To test your understanding, implement the derivative() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 82)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('AmesHousing2.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[150,\n",
       " 106.24258269493151,\n",
       " 126.61281661731272,\n",
       " 117.12993450021699,\n",
       " 121.54446668425497,\n",
       " 119.48938531096931,\n",
       " 120.44607998998796,\n",
       " 120.00071333893449,\n",
       " 120.20804328256295,\n",
       " 120.11152571569237,\n",
       " 120.15645719327628,\n",
       " 120.13554040327286,\n",
       " 120.1452777216869,\n",
       " 120.14074474268385,\n",
       " 120.14285496418101,\n",
       " 120.14187260031741,\n",
       " 120.14232991665213,\n",
       " 120.142117023815,\n",
       " 120.14221613105579,\n",
       " 120.14216999401657,\n",
       " 120.14219147202738]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def derivative(a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += xi_list[i]*(a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):\n",
    "    a1_list = [a1_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        deriv = derivative(a1, xi_list, yi_list)\n",
    "        a1_new = a1 - alpha*deriv\n",
    "        a1_list.append(a1_new)\n",
    "    return(a1_list)\n",
    "\n",
    "param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)\n",
    "param_iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.7574173051\n",
      "-20.3702339224\n",
      "9.4828821171\n",
      "-4.41453218404\n",
      "2.05508137329\n",
      "-0.956694679019\n",
      "0.445366651053\n",
      "-0.207329943628\n"
     ]
    }
   ],
   "source": [
    "print(derivative(param_iterations[0],train['Gr Liv Area'], train['SalePrice'])*.0000003)\n",
    "print(derivative(param_iterations[1],train['Gr Liv Area'], train['SalePrice'])*.0000003)\n",
    "print(derivative(param_iterations[2],train['Gr Liv Area'], train['SalePrice'])*.0000003)\n",
    "print(derivative(param_iterations[3],train['Gr Liv Area'], train['SalePrice'])*.0000003)\n",
    "print(derivative(param_iterations[4],train['Gr Liv Area'], train['SalePrice'])*.0000003)\n",
    "print(derivative(param_iterations[5],train['Gr Liv Area'], train['SalePrice'])*.0000003)\n",
    "print(derivative(param_iterations[6],train['Gr Liv Area'], train['SalePrice'])*.0000003)\n",
    "print(derivative(param_iterations[7],train['Gr Liv Area'], train['SalePrice'])*.0000003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Ohhhhh okay. I see. It keeps grinding down to the extreme point (where the derivative value is 0), and that gets you the minimum point!!! Dope!!!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Understanding Multi-Parameter Gradient Descent\n",
    "\n",
    "Now that we've understood how single parameter gradient descent works, let's build some intuition for multi parameter gradient descent. Let's start by visualizing the MSE as a function of the parameter values for the following simple linear regression model:\n",
    "\n",
    "$SalePrice = a_1 * Gr Liv Area + a_0$\n",
    "\n",
    "In this screen's widget, we've generated a 3D scatter plot with:\n",
    "\n",
    "- a0 on the x-axis\n",
    "- a1 on the y-axis\n",
    "- MSE on the z-axis\n",
    "\n",
    "![](coeffs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gradient of the Cost Function\n",
    "\n",
    "In the widget from the first screen, you were able to use the parameter sliders to try to reduce the residual sum of squares (which, by proxy, also reduces the mean squared error). The gradient is a multi variable generalization of the derivative. In the last few screens, we were concerned with minimizing the following cost function:\n",
    "\n",
    "$MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} (a_1x_1^{(i)} - y^{(i)} ) ^2$\n",
    "\n",
    "When we have 2 parameter values (a0 and a1), the cost function is now a function of 2 variables, not 1:\n",
    "\n",
    "$MSE(a_0, a_1) = \\frac{1}{n} \\sum_{i=1}^{n} (a_0 + a_1x_1^{(i)} - y^{(i)} ) ^2$\n",
    "\n",
    "Instead of one update rule, we now need two update rules. We need one for a0:\n",
    "\n",
    "$a_0 := a_0 - \\alpha \\frac{d}{da_0} MSE(a_0, a_1)$\n",
    "\n",
    "and one for a1:\n",
    "\n",
    "$a_1 := a_1 - \\alpha \\frac{d}{da_1} MSE(a_0, a_1)$\n",
    "\n",
    "Earlier in this mission, we determined that $\\frac{d}{da_1} MSE(a_1)$ worked out to $\\frac{2}{n} \\sum_{i=1}^{n} x_1^{(i)}(a_1x_1^{(i)} - y^{(i)})$. For the multiparameter case, we need to include the additional parameter :\n",
    "\n",
    "$\\frac{d}{da_1} MSE(a_0, a_1) = \\frac{2}{n} \\sum_{i=1}^{n} x_1^{(i)}(a_0 + a_1x_1^{(i)} - y^{(i)})$\n",
    "\n",
    "For $\\frac{d}{da_0} MSE(a_0, a_1)$, we won't walk through the proof for this derivative, but it's similar to the one we did for a1 and we encourage you to derive this yourself on pencil and paper:\n",
    "\n",
    "$\\frac{d}{da_0} MSE(a_0, a_1) = \\frac{2}{n} \\sum_{i=1}^{n} (a_0 + a_1x_1^{(i)} - y^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 999.21319912369859, 998.4271020923004, 997.64170827616999, 996.85701704623523, 996.07302777398684, 995.28973983147762, 994.50715259132221, 993.72526542669652, 992.94407771133717, 992.16358881954091, 991.38379812616438, 990.60470500662336, 989.82630883689228, 989.04860899350399, 988.27160485354898, 987.49529579467503, 986.71968119508654, 985.94476043354416, 985.17053288936438, 984.39699794241881]\n",
      "[150, 106.24258269493151, 126.61281661731272, 117.12993450021699, 121.54446668425497, 119.48938531096931, 120.44607998998796, 120.00071333893449, 120.20804328256295, 120.11152571569237, 120.15645719327628, 120.13554040327286, 120.1452777216869, 120.14074474268385, 120.14285496418101, 120.14187260031741, 120.14232991665213, 120.142117023815, 120.14221613105579, 120.14216999401657, 120.14219147202738]\n"
     ]
    }
   ],
   "source": [
    "def a1_derivative(a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += xi_list[i]*(a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def a0_derivative(a0, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += a0*xi_list[i] - yi_list[i]\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):\n",
    "    a1_list = [a1_initial]\n",
    "    a0_list = [a0_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        a0 = a0_list[i]\n",
    "        \n",
    "        a1_deriv = a1_derivative(a1, xi_list, yi_list)\n",
    "        a0_deriv = a0_derivative(a0, xi_list, yi_list)\n",
    "        \n",
    "        a1_new = a1 - alpha*a1_deriv\n",
    "        a0_new = a0 - alpha*a0_deriv\n",
    "        \n",
    "        a1_list.append(a1_new)\n",
    "        a0_list.append(a0_new)\n",
    "    return(a0_list, a1_list)\n",
    "\n",
    "a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)\n",
    "print(a0_params)\n",
    "print(a1_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gradient Descent for Higher Dimensions\n",
    "\n",
    "What if we want to use many parameters in our model? Gradient descent actually scales to as many variables as you want. Each parameter value will need its own update rule, and it closely matches the update rule for a1:\n",
    "\n",
    "$ a_0 := a_0 - \\alpha \\frac{d}{da_0} MSE \\\\\n",
    " a_1 := a_1 - \\alpha \\frac{d}{da_1} MSE \\\\ \n",
    " a_2 := a_2 - \\alpha \\frac{d}{da_2} MSE \\\\ \n",
    " a_3 := a_3 - \\alpha \\frac{d}{da_3} MSE \\\\ \n",
    " a_n := a_n - \\alpha \\frac{d}{da_n} MSE \\\\$\n",
    "\n",
    "Besides the derivative for the MSE with respect to the intercept value (a0), the derivative for other parameters are identical:\n",
    "\n",
    "$\\frac{d}{da_1} MSE = \\frac{2}{n} \\sum_{i=1}^{n} x_1^{(i)}(\\hat{y}^{(i)} - y^{(i)}) \\\\  \n",
    " \\frac{d}{da_2} MSE = \\frac{2}{n} \\sum_{i=1}^{n} x_2^{(i)}(\\hat{y}^{(i)} - y^{(i)}) \\\\\n",
    " \\frac{d}{da_n} MSE = \\frac{2}{n} \\sum_{i=1}^{n} x_n^{(i)}(\\hat{y}^{(i)} - y^{(i)})  \\\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Next Steps\n",
    "\n",
    "In this mission, we explored how to find a linear regression model using the gradient descent algorithm. The main challenges with gradient descent include:\n",
    "\n",
    "- choosing good initial parameter values\n",
    "- choosing a good learning rate (falls under the domain of hyperparameter optimization)\n",
    "\n",
    "In the next mission, we'll explore a technique called OLS estimation which doesn't require any parameter or hyperparameter value selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
