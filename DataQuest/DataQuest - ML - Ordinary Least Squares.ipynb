{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In the last mission, we explored an iterative technique for model fitting named gradient descent. The gradient descent algorithm requires multiple iterations to converge on the optimal parameter values and the number of iterations is highly dependent on the initial parameter values and the learning rate we select.\n",
    "\n",
    "In this mission, we'll explore a technique called ordinary least squares estimation or OLS estimation for short. Unlike gradient descent, OLS estimation provides a clear formula to directly calculate the optimal parameter values that minimizes the cost function. To understand OLS estimation, we need to first frame our linear regression problem in the matrix form. We've mostly worked with the following form of the linear regression model:\n",
    "\n",
    "$\\hat{y} = a_0 + a_1x_1 + a_2x_2 + ... + a_nx_n$\n",
    "\n",
    "While this form represents the relationship between the features (x1 to xn) and the target column (y) well when there are just a few parameter values, it doesn't scale well to when we have hundreds of parameters. If you recall from the Linear Algebra for Machine Learning course, we explored how matrix notation lets us better represent and reason about a linear system with many variables. With that in mind, here's what the matrix form of our linear regression model looks like:\n",
    "\n",
    "$Xa  =  \\hat{y}$\n",
    "\n",
    "Where X is a matrix representing the columns from the training set our model uses, a is a vector representing the parameter values, and $\\hat{y}$ is the vector of predictions. Here's a diagram with some sample values for each:\n",
    "\n",
    "Python Calculator 2\n",
    "\n",
    "Now that we've gained an understanding for the matrix representation of the linear regression model, let's take a peek at the OLS estimation formula that results in the optimal vector A:\n",
    "\n",
    "$a = (X^TX)^{-1} X^Ty$\n",
    "\n",
    "Let's start by computing OLS estimation to find the best parameters for a model using the following features:\n",
    "\n",
    "    features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area','Gr Liv Area', 'Overall Qual']\n",
    "       \n",
    "In the following screens, we'll dive into the mathematical derivation of the OLS estimation technique. It's important to note that you'll most likely never implement this technique in a data science role and will instead use an existing, efficient implementation (scikit-learn uses OLS under the hood when you call fit() on a LinearRegression instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    53.75693376  18232.3137575   -6434.65300989     22.53151963\n",
      "     86.81522574     28.08976713  11397.64135314]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "test = data[1460:]\n",
    "\n",
    "features = ['Wood Deck SF', 'Fireplaces', 'Full Bath', '1st Flr SF', 'Garage Area',\n",
    "       'Gr Liv Area', 'Overall Qual']\n",
    "X = train[features]\n",
    "y = train['SalePrice']\n",
    "\n",
    "first_term = np.linalg.inv(\n",
    "        np.dot(\n",
    "            np.transpose(X), \n",
    "            X\n",
    "        )\n",
    "    )\n",
    "second_term = np.dot(\n",
    "        np.transpose(X),\n",
    "        y\n",
    "    )\n",
    "a = np.dot(first_term, second_term)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cost Function\n",
    "\n",
    "Unlike gradient descent, OLS estimation provides what is known as a **closed form solution** to the problem of finding the optimal parameter values. **A closed form solution is one where a solution can be computed arithmetically with a predictable amount of mathematical operations. <font color = blue> Gradient descent, on the other hand, is an algorithmic approach that can require a different number of iteration (and therefore a different number of mathematical operations) based on the initial parameter values, the learning rate, etc. While the approach is different, both techniques share the high level objective of minimizing the cost function.** </font>\n",
    "\n",
    "Before we can dive into how the cost function is represented in the matrix form, let's understand how the error is represented. Because the error is the difference between the predictions made using the model y^ and the actual labels y, it's represented as a vector. The greek letter for E (epsilon Ïµ) is often used to represent the error vector:\n",
    "\n",
    "$\\epsilon =  \\hat{y} - y$\n",
    "\n",
    "We can build on this to define y:\n",
    "\n",
    "$y = Xa - \\epsilon$\n",
    "\n",
    "Even though this closely resembles the matrix equation of $Ax = b$, we have 2 unknowns (the vector a and the vector $\\hat{y}$. We're looking for a model, represented using the parameter vector a, that will minimize the mean squared error between the labels, y, and the predictions, y^. Said another way, the cost function is this mean squared error.\n",
    "\n",
    "Here's what the cost function looks like in matrix form:\n",
    "\n",
    "$J(a) = \\dfrac{1}{n} (Xa - y)^T(Xa - y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Derivative of the Cost Function\n",
    "The derivative of the cost function is decently involved, and out of scope for this mission. Understanding the derivation requires some familiarity with matrix calculus(https://en.wikipedia.org/wiki/Matrix_calculus), which is a specific notation for applying calculus concepts to matrices. If you're interested in the derivation, we recommend that you read Eli Bendersky's wonderful walkthrough of the derivation on his blog (http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/).\n",
    "\n",
    "Here's the derivative of the cost function:\n",
    "\n",
    "$\\frac{dJ(a)}{da} = 2X^TXa - 2X^Ty$\n",
    "\n",
    "To find the vector a that minmizes the cost function J(a), we need to set the derivative equal to 0 and solve for a:\n",
    "\n",
    "$2X^TXa - 2X^Ty = 0$\n",
    "\n",
    "Let's move the second term to the right hand side and divide both sides by 2:\n",
    "\n",
    "$X^TXa = X^Ty$\n",
    "\n",
    "Our goal is to isolate a, the parameter vector. The last step we need to perform is \"divide out\" $X^TX$ from the left hand side.\n",
    "\n",
    "If you recall, we can \"divide\" matrix terms by computing the inverse. Let's dig up the example we explored in the linear algebra course. We can cancel $A$ from the following equation $Ax=b$ by multiplying both sides by the inverse $A^{-1}Ax = A^{-1}b$. This leaves us with $x = A^{-1}b$.\n",
    "\n",
    "To cancel $X^TX$ from the left side, we need to compute the inverse of it and multiply it by both sides. We're now left with the OLS estimation formula:\n",
    "\n",
    "$a = (X^TX)^{-1}X^Ty$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Descent vs. Ordinary Least Squares\n",
    "\n",
    "Now that we've explored a lot of the math that underlies OLS estimation, let's understand its limitations. The biggest limitation is that OLS estimation is computationally expensive when the data is large. This is because computing a matrix inverse has a computational complexity of apprximately O(n^3). You can read more about computational complexity of the matrix inverse and other common matrix operations on Wikipedia.\n",
    "\n",
    "**OLS is commonly used when the number of elements in the dataset (and therefore the matrix that's inverted) is less than a few million elements. On larger datasets, gradient descent is used because it's much more flexible. For many practical problems, we can set a threshold accuracy value (or a set number of iterations) and use a \"good enough\" solution. This is especially useful when iterating and trying different features in our model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Next Steps\n",
    "\n",
    "In this mission, we explored a closed form solution to fitting a linear regression model called OLS estimation. We explored some of the intuition behind the math for this technique and ended by exploring it's computational complexity. In the next mission, we'll explore how to clean some of the remaining features in the training set to use in our model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
