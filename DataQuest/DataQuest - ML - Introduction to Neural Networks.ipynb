{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Networks and Iris Flowers\n",
    "\n",
    "Many machine learning prediction problems are rooted in complex data and its non-linear relationships between features. Neural networks are a class of models that can learn these non-linear interactions between variables.\n",
    "\n",
    "We will introduce neural networks by predicting the species of iris flowers from data with the following features:\n",
    "\n",
    "- sepal_length - Continuous variable measured in centimeters.\n",
    "- sepal_width - Continuous variable measured in centimeters.\n",
    "- petal_length - Continuous variable measured in centimeters.\n",
    "- petal_width - Continuous variable measured in centimeters.\n",
    "- species - Categorical. 2 species of iris flowers, Iris-virginica or Iris-versicolor.\n",
    "\n",
    "The DataFrame class includes a hist() method which creates a histogram for every numeric column in that DataFrame. The histograms are generated using Matplotlib and displayed using plt.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sepal_length  sepal_width  petal_length  petal_width          species\n",
      "85           7.7          3.0           6.1          2.3   Iris-virginica\n",
      "17           5.8          2.7           4.1          1.0  Iris-versicolor\n",
      "52           7.1          3.0           5.9          2.1   Iris-virginica\n",
      "77           6.1          3.0           4.9          1.8   Iris-virginica\n",
      "74           6.7          3.3           5.7          2.1   Iris-virginica\n",
      "['Iris-virginica' 'Iris-versicolor']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9869be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read in dataset\n",
    "iris = pandas.read_csv(\"iris.csv\")\n",
    "\n",
    "# shuffle rows\n",
    "shuffled_rows = np.random.permutation(iris.index)\n",
    "iris = iris.loc[shuffled_rows,:]\n",
    "\n",
    "print(iris.head())\n",
    "\n",
    "# There are 2 species\n",
    "print(iris.species.unique())\n",
    "iris.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neurons\n",
    "\n",
    "So far we have talked about methods which do not allow for a large amount of non-linearity. For example, in the two dimensional case shown below, we want to find a function that can cleanly seperate the X's from the O's.\n",
    "\n",
    "![](l92oKZd.png)\n",
    "\n",
    "Neither a linear model nor logistic model is capable of building such a function, so we must explore other options like neural networks. Neural networks are very loosely inspired by the structure of neurons in the human brain. These models are built by using a series of activation units, known as neurons, to make predictions of some outcome. Neurons take in some input, apply a transformation function, and return an output. Below we see a representation of a neuron.\n",
    "\n",
    "![](RdPPxre.png)\n",
    "\n",
    "This neuron is taking in 5 units represented as x, a bias unit, and 4 features. This bias unit (1) is similar in concept to the intercept in linear regression and it will shift the activity of the neuron to one direction or the other. These units are then fed into an activation function h. We will use the popular sigmoid (logistic) activation function because it returns values between 0 and 1 and can be treated as probabilities.\n",
    "\n",
    "$\\text{Sigmoid Function: }g(z) = \\dfrac{1}{1 + e^{-z}}$\n",
    "\n",
    "\n",
    "This sigmoid function then leads to the corresponding activation function:\n",
    "\n",
    "$\\text{Sigmoid Activation Function: }h_{\\Theta}(x) = \\dfrac{1}{1+e^{-\\Theta^T x}} = \\dfrac{1}{1+e^{-(\\theta_01 + \\theta_1x_1 + \\theta_2 x_2)}}$\n",
    "\n",
    "If you look closely, you might notice that the logistic regression function we learned in previous lessons can be represented here as a neuron.\n",
    "\n",
    "**MC Sidenote: This video is very helpful... essentailly it breaks the activation function (which can change) into a pre activation (which is essentially a linear equation and an activation (which in this example is the sigmoid funciton). So if your activation is 0, then your neuron output will just be a linear model. **\n",
    "https://www.youtube.com/watch?v=SGZ6BttHMPw\n",
    "\n",
    "pre-activation = a(x)\n",
    "activation (output) = h(x) = g(a(x))\n",
    "\n",
    "![](Picture1.png)\n",
    "\n",
    "\n",
    "\n",
    "<font color=red size=8> Below we are trying to predict whether or not a flower is a 'Iris-veriscolor' or not. This is a binary classifier.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17]]\n",
      "Prediction/final output node:  [ 0.55740271]\n"
     ]
    }
   ],
   "source": [
    "z = np.asarray([[9, 5, 4]])\n",
    "y = np.asarray([[-1, 2, 4]])\n",
    "\n",
    "# np.dot is used for matrix multiplication\n",
    "# z is 1x3 and y is 1x3,  z * y.T is then 1x1 (columns must match rows for matrix multiplicaiton to work!)\n",
    "print(np.dot(z,y.T))\n",
    "\n",
    "# Variables to test sigmoid_activation\n",
    "iris[\"ones\"] = np.ones(iris.shape[0])\n",
    "X = iris[['ones', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values #turn X into a matrix\n",
    "y = (iris.species == 'Iris-versicolor').values.astype(int) \n",
    "\n",
    "# The first observation\n",
    "x0 = X[0]\n",
    "\n",
    "# Initialize thetas randomly (mean=0, std = 0.01, rows=5, cols=1)\n",
    "# MC EDIT; theta values are essentially the coefficients... I think\n",
    "theta_init = np.random.normal(0,0.01,size=(5,1)) # set random weights to start\n",
    "def sigmoid_activation(x, theta):\n",
    "    x = np.asarray(x)\n",
    "    theta = np.asarray(theta) #create vector of random weights\n",
    "    return 1 / (1 + np.exp(-np.dot(theta.T, x))) # note that np.dot(theta.T,x0) is equal to np.dot(x0,theta)\n",
    "                \n",
    "a1 = sigmoid_activation(x0, theta_init)\n",
    "print('Prediction/final output node: ',a1)\n",
    "\n",
    "# this is essentiall taking the the multiplication of the input nodes and the weights, and then applying a activation function\n",
    "# in this case, the activation function is the sigmoid function\n",
    "# because we are only use one row of data in this first calc, we only get ONE number, 0.4730833.... This is out ouput node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta shape:  (5, 1)\n",
      "x0 shape:  (5,)\n",
      "[-0.00048914]\n",
      "[-0.00048914]\n",
      "[-0.00048914]\n"
     ]
    }
   ],
   "source": [
    "# Note that these are the same!\n",
    "\n",
    "theta = np.array([[-0.0209818 ],\n",
    "                       [ 0.00366465],\n",
    "                       [-0.00123329],\n",
    "                       [-0.00730515],\n",
    "                       [ 0.01762441]])\n",
    "\n",
    "print('theta shape: ',theta.shape)\n",
    "print('x0 shape: ',x0.shape)\n",
    "print(np.dot(theta.T,x0))\n",
    "print(np.dot(x0,theta))\n",
    "print(np.dot(x0.T,theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "(100, 5)\n",
      "We need to transpose the X matrix so it can me mutiplied in the sigmoid function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.55740271,  0.54388022,  0.55407451,  0.5469637 ,  0.55178596,\n",
       "         0.53739573,  0.5492237 ,  0.55198488,  0.55513417,  0.54194608,\n",
       "         0.53769949,  0.55175755,  0.54712715,  0.54981058,  0.54643092,\n",
       "         0.54687526,  0.54647577,  0.54785269,  0.54856388,  0.55923935,\n",
       "         0.54988707,  0.53815012,  0.54331673,  0.54148917,  0.54584216,\n",
       "         0.54953956,  0.55030976,  0.54022803,  0.54226941,  0.54484682,\n",
       "         0.54572855,  0.54623624,  0.54628055,  0.54564577,  0.55601848,\n",
       "         0.54504013,  0.55081912,  0.54665484,  0.55258887,  0.54669923,\n",
       "         0.54849869,  0.55022421,  0.54585225,  0.5429631 ,  0.54328137,\n",
       "         0.54947011,  0.543496  ,  0.54857446,  0.5494121 ,  0.54895881,\n",
       "         0.54975973,  0.54904662,  0.55903303,  0.54130922,  0.54778779,\n",
       "         0.55130072,  0.55127736,  0.55815667,  0.5419006 ,  0.54787957,\n",
       "         0.54271306,  0.54846778,  0.55119305,  0.54168089,  0.54438668,\n",
       "         0.54572855,  0.54727434,  0.54780292,  0.54670519,  0.54420444,\n",
       "         0.54627953,  0.54951819,  0.55505443,  0.55166768,  0.54643833,\n",
       "         0.55262466,  0.53784256,  0.55158273,  0.54288687,  0.54292662,\n",
       "         0.55608472,  0.53986392,  0.54968661,  0.54366617,  0.54552539,\n",
       "         0.54356369,  0.54450808,  0.55882871,  0.55459381,  0.55964122,\n",
       "         0.54775249,  0.54370814,  0.55033193,  0.5483091 ,  0.54179749,\n",
       "         0.54427332,  0.55037959,  0.55028881,  0.55113936,  0.54917006]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, we can now do the activation function with THE ENTIRE DATA set, and get a prediction value (h0(x)) for each row:\n",
    "print(theta_init.T.shape)\n",
    "print(X.shape)\n",
    "\n",
    "print('We need to transpose the X matrix so it can me mutiplied in the sigmoid function')\n",
    "\n",
    "sigmoid_activation(X.T,theta_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cost Function\n",
    "\n",
    "We can train a single neuron as a two layer network using gradient descent. As we learned in the previous mission, we need to minimize a cost function which measures the error in our model. The cost function measures the difference between the desired output and actual output, defined as:\n",
    "\n",
    "$J(\\Theta) = -\\dfrac{1}{m} \\sum_{i=1}^{m} (y_i * log(h_{\\Theta}(x_i)) + (1-y_i) log(1-h_{\\Theta}(x_i)))$\n",
    "\n",
    "Since our targets, yi, are binary, either $y_i$ or $(1-y_i)$ will equal zero. One of the terms in the summation will disappear because of this result and. the activation function is then used to compute the error. For example, if we observe a true target, $y_i=1$, then we want $h_{\\Theta}(x_i)$ to also be close to 1. So as $h_{\\Theta}(x_i)$ approaches 1, the $log(h_{\\Theta}(x_i))$ becomes very close to 0. Since the log of a value between 0 and 1 is negative, we must take the negative of the entire summation to compute the cost. The parameters are randomly initialized using a normal random variable with a small variance, less than 0.1.\n",
    "\n",
    "<font color=red> **Ok, so in plain english: This is a cost function above, essentially a mean squared error function (or whatever the equivalent is for logistic classifiers) that we are trying to minimize. We are trying to optimize the model be finding the group of model weights (coefficients in the linear regression sense) that produce the lowest possible value for the cost function. This would look like the bottom of the parabala in a three input equation... Anyway, we take a bunch of deriviatives out fo this equation for each weight, find the slope of each, and adjust the weights each time (using backward propogation) by subtracting the slope*learning rate from the current weight. This is gradient decsent, which we have covered multiple times now! You are a pro!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67873018492738701"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First observation's features and target\n",
    "x0 = X[0] # this is the features (select first row)\n",
    "y0 = y[0] # this is the single column of 1s and 0s (the thing we are trying to predict)(select first row)\n",
    "\n",
    "# Initialize parameters, we have 5 units (features) and just 1 layer (dependent variable)\n",
    "theta_init = np.random.normal(0,0.01,size=(5,1))\n",
    "\n",
    "def singlecost(X, y, theta):\n",
    "    # Compute activation\n",
    "    h = sigmoid_activation(X.T, theta)\n",
    "    # Take the negative average of target*log(activation) + (1-target) * log(1-activation)\n",
    "    cost = -np.mean(y * np.log(h) + (1-y) * np.log(1-h))\n",
    "    return cost\n",
    "\n",
    "first_cost = singlecost(x0, y0, theta_init)\n",
    "first_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So above we have produced the cost for ONE SINGLE row of data (because we initialized x0 = X[0] and y0 = y[0]) and one single outcome, using the carzy cost function we have seen above... not totally sure whats going on there, but you get the concept, and you can reverse engineer the math at a later date if need be. Anyway, now lets see what the costs are for EVERY row. The cost function essentially just takes the mean of all the costs from every indvidual data point. So...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_init:  [[  5.64927228e-03]\n",
      " [ -1.20163511e-02]\n",
      " [  3.70487384e-03]\n",
      " [ -6.84381899e-05]\n",
      " [  2.04933031e-02]] \n",
      "\n",
      "Actual vals:  [0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0\n",
      " 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0\n",
      " 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0] \n",
      "\n",
      "Predictors (first 5):  [[ 1.   7.7  3.   6.1  2.3]\n",
      " [ 1.   5.8  2.7  4.1  1. ]\n",
      " [ 1.   7.1  3.   5.9  2.1]\n",
      " [ 1.   6.1  3.   4.9  1.8]\n",
      " [ 1.   6.7  3.3  5.7  2.1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('theta_init: ',theta_init,'\\n')\n",
    "\n",
    "print('Actual vals: ',y,'\\n')\n",
    "\n",
    "print('Predictors (first 5): ',X[0:5],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.67873018 -0.71020508 -0.68027114 -0.68320547 -0.68320294 -0.70525546\n",
      "  -0.68685316 -0.68165791 -0.68482454 -0.70976169 -0.70523669 -0.67869953\n",
      "  -0.68224773 -0.68407419 -0.68438904 -0.70522192 -0.70732632 -0.70845607\n",
      "  -0.68841319 -0.67916069 -0.68158544 -0.70662123 -0.70811624 -0.70788744\n",
      "  -0.70159868 -0.68284497 -0.7119844  -0.70239398 -0.70831265 -0.68669944\n",
      "  -0.68544995 -0.68380386 -0.70915375 -0.70586338 -0.67610577 -0.70872124\n",
      "  -0.68485482 -0.70751766 -0.6844476  -0.71089817 -0.71037025 -0.70787873\n",
      "  -0.70388974 -0.70586974 -0.70628718 -0.6888283  -0.70830773 -0.6784023\n",
      "  -0.68321508 -0.71160852 -0.68509005 -0.71079301 -0.67797329 -0.70892415\n",
      "  -0.70773597 -0.68588648 -0.71245027 -0.67728692 -0.70562959 -0.68211053\n",
      "  -0.70203252 -0.67860982 -0.68369583 -0.70547385 -0.7123677  -0.68544995\n",
      "  -0.71037519 -0.68146721 -0.70978661 -0.70630105 -0.70839641 -0.68180973\n",
      "  -0.67701172 -0.68206539 -0.67929431 -0.68385899 -0.70605129 -0.68726904\n",
      "  -0.70530367 -0.70649365 -0.67586457 -0.68843931 -0.70976138 -0.70695358\n",
      "  -0.69073159 -0.70610301 -0.7050573  -0.67532149 -0.67464091 -0.67597089\n",
      "  -0.70590758 -0.7032455  -0.68080882 -0.70910157 -0.70600474 -0.68785258\n",
      "  -0.68469969 -0.6896508  -0.7109905  -0.68187263]] \n",
      "\n",
      "Mean cost (cost function output) calculatated manually:  0.695026838444 \n",
      "\n",
      "Mean cost (cost function output) calculatated with function:  0.695026838444\n"
     ]
    }
   ],
   "source": [
    "# so these are all the costs\n",
    "h=sigmoid_activation(X.T,theta_init)\n",
    "individual_costs = (y * np.log(h)) + ((1-y) * np.log(1-h))\n",
    "print(individual_costs,'\\n')\n",
    "\n",
    "print('Mean cost (cost function output) calculatated manually: ', -np.mean(individual_costs),'\\n')\n",
    "\n",
    "print('Mean cost (cost function output) calculatated with function: ', singlecost(X,y,theta_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wooohooo! We got the same number!!! This is super excellent, we are on the right track! Now its time to work through backward propogation and gradient descent!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute the Gradients\n",
    "\n",
    "In the previous mission we learned that we need to compute the partial deriviatives of the cost function to get the gradients. Calculating derivatives are more complicated in neural networks than in linear regression. Here we must compute the overall error and then distribute that error to each parameter. Compute the derivative using the chain rule.\n",
    "\n",
    "$\\dfrac{\\partial J}{\\partial \\theta_j} = \\dfrac{\\partial J}{\\partial h(\\Theta)} \\dfrac{\\partial h(\\Theta)}{\\partial \\theta_j}$\n",
    "\n",
    "This rule may look complicated, but we can break it down. The first part is computing the error between the target variable and prediction. The second part then computes the sensitivity relative to each parameter. In the end, the gradients are computed as: $\\delta = (y_i - h_\\Theta(x_i)) * h_\\Theta(x_i) * (1-h_\\Theta(x_i)) * x_i$.\n",
    "\n",
    "Now we will step through the math. \n",
    "- A. $(y_i - h_\\Theta(x_i))$ is a scalar and the error between our target and prediction. \n",
    "- B. $h_\\Theta(x_i) * (1-h_\\Theta(x_i))$ is also a scalar and the sensitivity of the activation function. \n",
    "- C. xi is the features for our observation i. \n",
    "- **OUTPUT: $\\delta$ (delta) is then a vector of length 5, 4 features plus a bias unit, corresponding to the gradients.**\n",
    "\n",
    "To implement this, we compute δ for each observation, then average to get the average gradient. The average gradient is then used to update the corresponding parameters.\n",
    "\n",
    "<font color=red> **Interesting!!! These 4 peices of the equation correspond the same 4 peices we listed in the DataCamp lesson... well, kind of. We will need to work out the math at some point to figure out the main discrepencies. The 3 things we multiply to get the slope of the loss funciton w.r.t. each weight are:**\n",
    "1. the slope of the loss function w.r.t. the node the weight FEEDS INTO **(Coresponds to A... maybe?)**\n",
    "2. the slope of the activation function w.r.t. the node of the weight FEEDS INTO **(Coresponds to B... maybe?)**\n",
    "3. the value of the node that feeds into the weight  **(Coresponds to C... maybe?)**\n",
    "\n",
    "**Anyway, we get the point here. We are calculating the gradients (slopes) for each weight. Then we can perform gradient descent to adjust the weights until we get to slopes of 0 or as close as possible!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_init:  [[-0.0209818 ]\n",
      " [ 0.00366465]\n",
      " [-0.00123329]\n",
      " [-0.00730515]\n",
      " [ 0.01762441]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00049126],\n",
       "       [-0.03774042],\n",
       "       [-0.01138082],\n",
       "       [-0.0784368 ],\n",
       "       [-0.0430239 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "#theta_init = np.random.normal(0,0.01,size=(5,1))\n",
    "\n",
    "#create one theta_init for repeating code!\n",
    "theta_init = np.array([[-0.0209818 ],\n",
    "                       [ 0.00366465],\n",
    "                       [-0.00123329],\n",
    "                       [-0.00730515],\n",
    "                       [ 0.01762441]])\n",
    "print('theta_init: ',theta_init,'\\n')\n",
    "\n",
    "# Store the updates into this array\n",
    "grads = np.zeros(theta_init.shape)\n",
    "\n",
    "# Number of observations \n",
    "n = X.shape[0]\n",
    "for j, obs in enumerate(X):\n",
    "    # Compute activation\n",
    "    h = sigmoid_activation(obs, theta_init)\n",
    "    # Get delta\n",
    "    delta = (y[j]-h) * (h * (1-h)) * obs\n",
    "    # accumulate\n",
    "    grads += delta[:,np.newaxis]/X.shape[0]\n",
    "    \n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_init:  [[-0.0209818 ]\n",
      " [ 0.00366465]\n",
      " [-0.00123329]\n",
      " [-0.00730515]\n",
      " [ 0.01762441]] \n",
      "\n",
      "delta of single observation:  [-0.12496942 -0.96226455 -0.37490826 -0.76231347 -0.28742967] \n",
      "\n",
      "Sum of all slopes w.r.t. weights for each row (observation):  \n",
      " [ 0.0491257  -3.77404175 -1.13808218 -7.84368025 -4.3023902 ] \n",
      "\n",
      "Average of all slopes w.r.t. weights for each row (observation):  \n",
      " [ 0.00049126 -0.03774042 -0.01138082 -0.0784368  -0.0430239 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cool. Lets try this manually to make sure we understand it. \n",
    "# First, lets calculate the gradients of ONE OBSERVATION\n",
    "print('theta_init: ',theta_init,'\\n')\n",
    "\n",
    "h=sigmoid_activation(X[0],theta_init) #create output using just first observation\n",
    "delta = (y[0]-h) * (h * (1-h)) * X[0] # create gradients of just one observation\n",
    "\n",
    "print('delta of single observation: ',delta,'\\n')\n",
    "\n",
    "slope_wrt_weights_sum = np.array([0.0,0.0,0.0,0.0,0.0])\n",
    "for j, obs in enumerate(X):\n",
    "    #compute the output node value!\n",
    "    output=sigmoid_activation(obs,theta_init)\n",
    "    #compute the 5 slopes of weights!\n",
    "    slope_wrt_weights_sum += (y[j]-output) * (output*(1-output)) * obs\n",
    "    \n",
    "print('Sum of all slopes w.r.t. weights for each row (observation): ','\\n',slope_wrt_weights_sum,'\\n')\n",
    "print('Average of all slopes w.r.t. weights for each row (observation): ','\\n',slope_wrt_weights_sum/100,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neat!!! As we can see, the average of all slopes with w.r.t. weights for each row is equal to that of our funciton. So we are doing this right! We now have the gradients for each weight (which is basically the avearage slope for each weight using the error at each observation).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Two Layer Network\n",
    "\n",
    "Now that you can compute the gradients, use gradient descent to learn the parameters and predict the species of iris flower given the 4 features. Gradient descent minimizes the cost function by adjusting the parameters accordingly. Adjust the parameters by substracting the product of the gradients and the learning rate from the previous parameters. Repeat until the cost function converges or a maximum number of iterations is reached.\n",
    "\n",
    "The high level algorithm is,\n",
    "\n",
    "    while (number_of_iterations < max_iterations and (prev_cost - cost) > convergence_thres ) {\n",
    "        update paramaters\n",
    "        get new cost\n",
    "        repeat\n",
    "    }\n",
    "We have implemented all these pieces in a single function learn() that can learn this two layer network. After setting a few initial variables, we begin to iterate until convergence. During each iteration we compute our gradients, update accordingly, and compute the new cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_init:  [[-0.0209818 ]\n",
      " [ 0.00366465]\n",
      " [-0.00123329]\n",
      " [-0.00730515]\n",
      " [ 0.01762441]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8VuX9//HXJ4sNARJWWEE2KCABFyLWhVaKqxW11Wot\natXaYVs6tL/aZVvbb7WOaq1aWxWtSl11K6K4CJvIkJ0wwwp7JPn8/jgHvUkD3AnJvfJ+Ph73I/d9\nznWd87nu3Lk/Odd1znXM3REREamptHgHICIiyUkJREREakUJREREakUJREREakUJREREakUJRERE\nakUJRKSOWOBhM9tsZh9HWecRM/tVfcfWEJhZkZmNinccDYkSSANhZpeaWaGZbTezNWb2spmNiHdc\nKWYEcAbQ2d2HV11pZl83s/fqMwAzO8vMppjZNjMrNbN3zOxLR7jNyWZ29SHWdzczDz9b+x+zj2Sf\nUcT0P4nX3Qe4++T63K8cSAmkATCz7wF/Bn4DtAe6AvcAR/TFUpfMLCPeMdSBbsByd98Rj52b2UXA\nv4FHgc4Ev+tbgTExCiHb3ZuHj0Ex2qfEk7vrkcIPoBWwHfjyIco0Ikgwq8PHn4FG4bpRQAnwfWA9\nsAa4Mlx3HLAWSI/Y1vnAnPB5GjABWAJsBJ4C2oTrugMOfANYCUwJl18OrAjL3wIsB06vwfauCLe3\nAfhpRFzpwE/CutuA6UCXcF1f4HVgE7AQ+Moh3qtOwPNh2cXAN8Pl3wB2AxXh+/2LKvX6VVm/JVz+\nCEEyfymM6yPgqIh6UcUGWNjuHxwi9jTgZ+H7u54g0bQK1zUG/hW+r1uAaQQJ6NdhzLvDuO+uZrv7\n3/uMatb9P+BfBysLTAZ+CUwN2/8akBNRfgTwfhhTMfB1YDywD9gbxvRCWDbys1Krz7QeNfx+iXcA\netTzLxhGA+XV/XFHlLkN+BBoB+SGf7C/DNeNCuvfBmQC5wA7gdbh+iXAGRHb+jcwIXx+U7jdzuEf\n9P3AE+G6/V8kjwLNgCZA//ALYQSQBdwRflGcXoPt/S3c1iBgD9AvXP8DYC7Qh+DLdhDQNtx3MXAl\nkAEMIUg+/Q/yXk0B7iX4wh0MlAJfCNd9HXjvEO/z/6wnSCAbgeHh/h8DJobroo6NINE4kH+I/V9F\nkPR6AM2BZ4F/huuuAV4AmhIk26FAy3DdZODqQ2x3/3tf2wSyBOgd/t4mA7eH67oRJJVLCD57bYHB\nEe/br6rsa3nEZ6XWn2k9avD9Eu8A9KjnXzBcBqw9TJklwDkRr88i6IrZ/8e2K/LLgeC/tuPD578C\nHgqftwB2AN3C1/OB0yLqdSRICBkRXyQ9ItbfSpgQwtdNCf7LPL0G2+scsf5jYFz4fCEwtpq2Xwy8\nW2XZ/cDPqynbheC/8RYRy34LPBI+/zq1SyAPRrw+B1hQi9hOCtvf+BD7fxP4VsTrPhHv31Xhl+wx\n1dSbTHQJZEvE4+Zw3f/j8AnkZxHrvwW8Ej7/MTDpIPt8hEMnkFp/pvWI/pEK/c5yaBuBHDPLcPfy\ng5TpRNCtsd+KcNln26hSdyfBf7AAjwPvm9l1wAXADHffv61uwCQzq4yoW0HQNbJfcZU4Pnvt7jvN\nbGPE+mi2t/YgcXYh+FKpqhtwnJltiViWAfyzmrKdgE3uvi1i2QqgoJqyNXGwmGsS2/73qSOw7CD7\nqe73nEHw/v2T4D2aaGbZBN1ZP3X3fTVoR84hPmOHUtPfWTSO5DMtUdIgeur7gKAr57xDlFlN8GW1\nX9dw2WG5+ycEf5xnA5cSJJT9ioGz3T074tHY3VdFbiLi+RqC7ikAzKwJQbdFTbZ3MMXAUQdZ/k6V\nbTZ39+uqKbsaaGNmLSKWdQWi2T8c2NZo1CS2hWH5Cw+xvep+z+XAOnff5+6/cPf+wInAuQTjUbWJ\nO9IOgiPJ/TrUoO7Bfmdw+Jhq/ZmW6CmBpDh3LyPoGrrHzM4zs6ZmlmlmZ5vZ78NiTwA/M7NcM8sJ\ny/+rBrt5nGB8YiTBGMh+fwV+bWbdAMLtjz3Edp4GxpjZiWaWRdD9YUewvUgPAr80s17h9RrHmFlb\n4EWgt5l9LXxfMs1smJn1q7oBdy8m6Ob5rZk1NrNjCAbPo32v1gGdw7ZFoyaxOfA94BYzu9LMWppZ\nmpmNMLMHwmJPAN81s3wza05wVt6T7l5uZqea2dFmlg5sJeja2n+kt45g3KQ2ZgEjzayrmbUi6JaK\n1mPA6Wb2FTPLMLO2ZjY4ypiO9DMtUVACaQDc/Y8EXy4/Ixj0LQZuAP4TFvkVUAjMIRhonhEui9YT\nwCnAW+6+IWL5nQRnLL1mZtsIBjWPO0ScRcCNwESCo5HtBH3Te2qzvSr+RHDW1msEX5B/B5qE3VFn\nAuMI/kNdC/yOYJC+OpcQ9OOvBiYRjEe8EWUMbwFFwFoz23C4wjWNzd2fJhg3uSosv47g9/hcWOQh\ngq6qKQTdXLsJ3m8IjgyeJnhv5gPv8HlX2Z3AReEFkndF2db9Mb0OPEnw2ZpOkBSjrbuSYEzo+wRn\noc0iOPkBgt9ffzPbYmb/qab6kX6mJQoWDiCJJJzwv+QtQC93P1i/vojEiY5AJKGY2Ziwm60ZwWm8\ncwnOrhGRBKMEIolmLJ9f/NWL4DRcHSaLJCB1YYmISK3oCERERGolpS8kzMnJ8e7du8c7DBGRpDF9\n+vQN7p4bTdmUTiDdu3ensLAw3mGIiCQNM1tx+FIBdWGJiEitxDSBmNloM1toZovNbEI1639gZrPC\nxzwzqzCzNtHUFRGR2IpZAgmnSLiHYM6k/sAlZtY/soy7/8HdB7v7YIIpD95x903R1BURkdiK5RHI\ncGCxuy91970E01Ucah6jSwimyKhNXRERqWexTCB5HDh1d0m47H+YWVOCGyE9U4u648N7fxeWlpYe\ncdAiIlK9RB1EHwNMdfdNNa3o7g+4e4G7F+TmRnUmmoiI1EIsE8gqghvE7NeZg99HYRyfd1/VtK6I\niMRALBPINKBXeC+CLIIk8XzVQuE9A07h8ymoo65bF3bvq+CBKUuYuviws22LiDRoMbuQMLxpzQ3A\nq0A6wX20i8zs2nD9X8Oi5wOvufuOw9Wtjzgz09N4YMpShue34aSeOfWxCxGRlBDTK9Hd/b/Af6ss\n+2uV148Aj0RTtz6kpxnnHtOJxz5aweotu+iU3aS+dykikpQSdRA9rr45sgeGcffbi+MdiohIwlIC\nqUZedhMuGd6Fp6YVs3LjzniHIyKSkJRADuL6U3uSnmb8+c1F8Q5FRCQhKYEcRLuWjbnixO78Z+Yq\nFq/fFu9wREQSjhLIIVwzsgdNMtP542s6ChERqUoJ5BDaNm/E+JFH8fK8tcxYuTne4YiIJBQlkMO4\n+uR8cppncfvLC9D940VEPqcEchjNGmVw02m9+HjZJt5euD7e4YiIJAwlkCiMG96V7m2b8ruXF1JR\nqaMQERFQAolKZnoaN5/Vh4XrtjFppuZwFBEBJZConTOwI8d0bsWfXlvI7n0V8Q5HRCTulECilJZm\nTDi7L6vLdvPoB8vjHY6ISNwpgdTAiUflcErvXO55ewllO/fFOxwRkbhSAqmhH43uy9bd+7h3siZa\nFJGGTQmkhvp3askFQzrz8NTlmmhRRBo0JZBa+OHoPqSnGb99eX68QxERiRslkFpo37Ix140Kpjj5\ncOnGeIcjIhIXMU0gZjbazBaa2WIzm3CQMqPMbJaZFZnZOxHLl5vZ3HBdYeyirt43T+5Bp1aN+eWL\nn+jiQhFpkGKWQMwsHbgHOBvoD1xiZv2rlMkG7gW+5O4DgC9X2cyp7j7Y3QtiEfOhNMlK50dn96Vo\n9VaemVES73BERGIulkcgw4HF7r7U3fcCE4GxVcpcCjzr7isB3D2hJ5/60qBODOmazR9eXciOPeXx\nDkdEJKZimUDygOKI1yXhski9gdZmNtnMppvZ5RHrHHgjXD7+YDsxs/FmVmhmhaWlpXUW/EH2xS3n\n9qd02x7um7ykXvclIpJoEm0QPQMYCnwROAu4xcx6h+tGuPtggi6w681sZHUbcPcH3L3A3Qtyc3Pr\nPeBju7Zm7OBOPPDuUko267ReEWk4YplAVgFdIl53DpdFKgFedfcd7r4BmAIMAnD3VeHP9cAkgi6x\nhPDD0X0x4LcvL4h3KCIiMRPLBDIN6GVm+WaWBYwDnq9S5jlghJllmFlT4Dhgvpk1M7MWAGbWDDgT\nmBfD2A8pL7sJ1406ipfmrGHq4g3xDkdEJCZilkDcvRy4AXgVmA885e5FZnatmV0blpkPvALMAT4G\nHnT3eUB74D0zmx0uf8ndX4lV7NG49pSj6NqmKT9/voi95ZXxDkdEpN5ZKt+mtaCgwAsLY3fJyFsL\n1nHVI4X8+Oy+XHPKUTHbr4hIXTGz6dFeKpFog+hJ7Qt923N6v3bc+eanrCnbFe9wRETqlRJIHfv5\nmAFUVDq/fknzZIlIalMCqWNd2jTlulFH8eKcNbyvAXURSWFKIPVg/4D6rRpQF5EUpgRSDxpnpvP/\nvtSfxeu38/DUZfEOR0SkXiiB1JNgQL09f37jU4o36Qp1EUk9SiD16LaxA0gzuOW5eaTy6dIi0jAp\ngdSjTtlNuPmsPkxeWMqLc9bEOxwRkTqlBFLPLj+hO4M6t+IXLxSxZefeeIcjIlJnlEDqWXqa8ZsL\njmbzzn389r+abFFEUocSSAwM6NSKq0/O58nCYj5Yonuoi0hqUAKJke+c1psubZrw00lz2b2vIt7h\niIgcMSWQGGmSlc6vzzuapRt2cM/bi+MdjojIEVMCiaGRvXO5YEge901eQtHqsniHIyJyRJRAYuzW\nMf1p3SyLm/89R9OciEhSUwKJseymWfzm/KOZv2arurJEJKkpgcTBGf3bc/6QPO55e7G6skQkacU0\ngZjZaDNbaGaLzWzCQcqMMrNZZlZkZu/UpG4y+XnYlfX9p2arK0tEklLMEoiZpQP3AGcD/YFLzKx/\nlTLZwL3Al9x9APDlaOsmm+ymWfz2/KNZsHYbd6srS0SSUCyPQIYDi919qbvvBSYCY6uUuRR41t1X\nArj7+hrUTTqn92/PBUPyuPftxcxbpa4sEUkusUwgeUBxxOuScFmk3kBrM5tsZtPN7PIa1AXAzMab\nWaGZFZaWltZR6PXn52MG0KZZFjf/ezZ7ynWBoYgkj0QbRM8AhgJfBM4CbjGz3jXZgLs/4O4F7l6Q\nm5tbHzHWqVZNM/ndhcewYO027nh1YbzDERGJWiwTyCqgS8TrzuGySCXAq+6+w903AFOAQVHWTVqn\n9m3H147vxt/eXcZU3UddRJJELBPINKCXmeWbWRYwDni+SpnngBFmlmFmTYHjgPlR1k1qPzmnH0fl\nNuP7T83WtO8ikhRilkDcvRy4AXiVICk85e5FZnatmV0blpkPvALMAT4GHnT3eQerG6vYY6FJVjp3\njhvCxh17+MmkubqDoYgkPEvlL6qCggIvLCyMdxg1ct/kJfzulQXc8eVBXDS0c7zDEZEGxsymu3tB\nNGUTbRC9wRs/sgfD89vw8+fmsXLjzniHIyJyUEogCSY9zfi/iweTlmZ858mZlFfoKnURSUxKIAko\nL7sJvz7/aGas3MKfXl8U73BERKqlBJKgvjSoE+OGdeHeyUt4Z1HiXxApIg2PEkgC+/mYAfRp34Lv\nPTmLdVt3xzscEZEDKIEksCZZ6dxz2RB27q3gpokzqahM3TPmRCT5KIEkuJ7tWvDL8wby4dJN3Pnm\np/EOR0TkM0ogSeCioZ258NjO/OWtT3lfU52ISIJQAkkSt40dQI+cZnx74izWazxERBKAEkiSaNYo\ng3svG8qOPeVc//gM9un6EBGJMyWQJNKnQwtuv/Bopi3fzK9fmh/vcESkgcuIdwBSM2MH5zG7uIyH\npi5jcJdszhtS7X21RETqnY5AktCPz+nL8Pw2THh2Dp+s3hrvcESkgVICSUKZ6WncfekQWjXJ5Np/\nTads5754hyQiDZASSJJq16Ix9142lDVlu/jOkzOp1EWGIhJjSiBJbGi31tx6bn/eXljKHa/pfuoi\nElsaRE9yXz2+G5+s2ca9k5fQq31zzh+im1CJSGzE9AjEzEab2UIzW2xmE6pZP8rMysxsVvi4NWLd\ncjObGy5PrtsM1iMz47axAzi+Rxt+9MxcZqzcHO+QRKSBiFkCMbN04B7gbKA/cImZ9a+m6LvuPjh8\n3FZl3anh8qhut9hQZKancd9lQ+nQsjHjH53O6i274h2SiDQAsTwCGQ4sdvel7r4XmAiMjeH+U1rr\nZln8/YoC9uyr4Op/FLJzb3m8QxKRFBfLBJIHFEe8LgmXVXWimc0xs5fNbEDEcgfeMLPpZjb+YDsx\ns/FmVmhmhaWlDetGTL3at+CuS4ewYO1WvvfkbJ2ZJSL1KtHOwpoBdHX3Y4C/AP+JWDfC3QcTdIFd\nb2Yjq9uAuz/g7gXuXpCbm1v/ESeYU/u04yfn9OOVorU6M0tE6lUsE8gqoEvE687hss+4+1Z33x4+\n/y+QaWY54etV4c/1wCSCLjGpxjdG5HPJ8OB2uI9/tDLe4YhIioplApkG9DKzfDPLAsYBz0cWMLMO\nZmbh8+FhfBvNrJmZtQiXNwPOBObFMPakYmb8cuxARvXJ5Wf/mctbC9bFOyQRSUExSyDuXg7cALwK\nzAeecvciM7vWzK4Ni10EzDOz2cBdwDh3d6A98F64/GPgJXd/JVaxJ6OM9DTuufRY+ndqyfWPzWRO\nyZZ4hyQiKcaC7+fUVFBQ4IWFDfuSkfXbdnP+Pe+zp7yCSd86iS5tmsY7JBFJYGY2PdpLJRJtEF3q\nWLsWjfnHVcPYV+Fc8fDHbN6xN94hiUiKUAJpAHq2a8HfLi+gZNMuvvloIbv2VsQ7JBFJAUogDcTw\n/Db838WDmb5yM996bLpuiSsiR0wJpAH54jEd+dV5A3l7YSnff0oXGorIkdFsvA3MZcd1o2zXPn7/\nykJaNcnktrEDCM+cFhGpESWQBui6U46ibOc+7p+ylOymmXz/zD7xDklEkpASSANkZkw4uy9bdu7j\nL28tplWTTK4+uUe8wxKRJKME0kCZGb+54Gi27dnHr16aT9OsDC49rmu8wxKRJKIE0oClpxn/d/Fg\ndu2dzk8mzSUjzfjKsC6Hrygigs7CavAaZaRz31eHcnKvHH707ByemV4S75BEJEkogQiNM9P52+UF\nnHhUW37w9Gyem7Xq8JVEpMGrcQIJZ8ZNr49gJH4aZ6bz4OXDGJ7fhu8+OYsX56yOd0gikuAOm0DM\nLM3MLjWzl8xsPbAAWGNmn5jZH8ysZ/2HKbHQJCudv18xjKHdWnPTxFm8PHdNvEMSkQQWzRHI28BR\nwI+BDu7exd3bASOAD4HfmdlX6zFGiaFmjTJ4+MrhDO6SzQ1PzOSF2ToSEZHqRXMW1unuvq/qQnff\nBDwDPGNmmXUemcRN80YZ/OOq4Vz1yDRumjiT3fsq+HKBzs4SkQMd9ghkf/Iws8ZmNjB8NK6ujKSO\n5o0y+MeVwzmpZw4/eHoO//xgebxDEpEEE80YSIaZ/R4oAf4BPAoUm9ntZqbrSFJYk6zg7KzT+7Xj\nlueKePDdpfEOSUQSSDRjIH8AWgP5wIvufizBmEgOcEdNdmZmo81soZktNrMJ1awfZWZlZjYrfNwa\nbV2pH40zg+tEvnh0R3710nz+8uan8Q5JRBJENEcQ5wK93d3NbAzwc3ffambXAAuB70Szo/DU33uA\nMwiOZqaZ2fPu/kmVou+6+7m1rCv1IDM9jTvHDaZRZhp/fH0R2/eUM+HsvprFV6SBiyaBuH9+43SL\nWFhhZjW5K9FwYLG7LwUws4nAWCCaJHAkdaUOZKSnccdFg2iWlcH9U5ayYftebr/waDLTdS2qSEMV\nzV//fDO7PHz+2Tmd4am782uwrzygOOJ1SbisqhPNbI6ZvWxmA2pYFzMbb2aFZlZYWlpag/DkcNLS\njNvGDuB7Z/TmmRkljH+0kJ17y+MdlojESTQJ5HrgRjObDCwwsz+a2TvAjcB1dRzPDKCrux8D/AX4\nT0034O4PuHuBuxfk5ubWcXhiZnz7tF785vyjeWdRKZc9+BGbd+yNd1giEgfRnMZb4u7DgF8Cy8PH\nL9z9OHevyVVmq4DIiwk6h8si97XV3beHz/8LZJpZTjR1JbYuPa4r9152LEWrt/Ll+z9g9ZZd8Q5J\nRGIsmtN4DcDd33T3v4SPt6orcxjTgF5mlm9mWcA44Pkq2+mwf1tmNjyMb2M0dSX2Rg/syKNXDWdd\n2W4uvO99Fq7dFu+QRCSGoprKxMxuNLMD7jZkZllm9gUz+wdwxeE24u7lwA3AqwRjJ0+5e5GZXWtm\n14bFLgLmmdls4C5gnAeqrRttI6X+HN+jLU9ecwIVlc5F973Pu59q3EmkobDPT7A6SIHgqvOrgMsI\nrgXZAjQhSD6vAfe6+8x6jrNWCgoKvLCwMN5hNAirt+ziqkem8en67fzqvIFcMlx3NxRJRmY23d0L\noip7uARSZcOZBBcQ7nL3LbWML2aUQGJr+55ybnh8BpMXlnLNyB78aHRf0tJ0rYhIMqlJAqnRSfzu\nvs/d1yRD8pDYa94ogwcvL+Brx3fj/ilLuf7xGezaWxHvsESknkQziL7NzLYe5FFqZh+a2WmxCFYS\nX0Z6GreNHcAt5/bnlaK1jPvbh6zfujveYYlIPYjmNN4W7t6yugfQAbgGuLPeI5WkYWZ8Y0Q+9391\nKIvWbmPM3e8xq1gHrSKp5ojmoXD3CnefTXDRn8gBzhzQgUnXn0hWRhpfuf8Dnp5eEu+QRKQO1clE\nRu5+f11sR1JP3w4tef76ERR0a83N/57NL14ooryiJlOoiUii0kx4Uu9aN8vi0auGc9VJ+Tw8dTmX\nP/Sxpj8RSQFKIBITGelp3DqmP3d8eRCFKzYz5u73KFpdFu+wROQIKIFITF00tDNPXXMC5RXO+fe+\nz5PTVlKTa5FEJHEogUjMDe6SzUvfHsFx+W340TNzufnfc3S9iEgSUgKRuGjbvBGPXDmcm07rxbMz\nSzj/3qksLd0e77BEpAaUQCRu0tOM757Rm39cOZx1W3fzpbun8tKcNfEOS0SipAQicTeydy4vfftk\nerdvzvWPz+DW5+axe5+6tEQSnRKIJIRO2U2YOP4Erh6Rz6MfrGDs3VN1fxGRBKcEIgkjKyONn53b\nn0euHMbGHXv40t3v8c8PlussLZEEpQQiCWdUn3a8fNNIju/RllueK+Kbj05nky48FEk4SiCSkHJb\nNOLhrw/jlnP7M2VRKaP/PIWpizfEOywRiRDTBGJmo81soZktNrMJhyg3zMzKzeyiiGXLzWyumc0y\nM90lqgFISwtm9X32WyfSvHEGlz34Eb94oUjXjIgkiJglEDNLB+4Bzgb6A5eYWf+DlPsdwe1yqzrV\n3QdHe7csSQ0D81rx4o0juOKEbjw8dTlfvOtdZqzcHO+wRBq8WB6BDAcWu/tSd98LTATGVlPuRuAZ\nYH0MY5ME1zQrg1+MHchjVx/H7n0VXHTf+/zh1QXsKdfRiEi8xDKB5AHFEa9LwmWfMbM84Hzgvmrq\nO/CGmU03s/EH24mZjTezQjMrLC0trYOwJZGc1DOHV747kouGduaet5cw9u6pfLJ6a7zDEmmQEm0Q\n/c/Aj9y9uhtGjHD3wQRdYNeb2cjqNuDuD7h7gbsX5Obm1mesEictG2fy+4sG8fcrCti4Yy9j73mP\nu978lL3lus+ISCzFMoGsArpEvO4cLotUAEw0s+XARcC9ZnYegLuvCn+uByYRdIlJA3Zav/a89p2R\nnD2wI396fRFj/vIeMzU2IhIzsUwg04BeZpZvZlnAOOD5yALunu/u3d29O/A08C13/4+ZNTOzFgBm\n1gw4E5gXw9glQbVulsVdlwzh71cUsHX3Pi64731+8UIRO/aUxzs0kZSXEasduXu5md0AvAqkAw+5\ne5GZXRuu/+shqrcHJpkZBDE/7u6v1HfMkjxO69ee4flt+MOrC3nk/eW8VrSOX58/kFF92sU7NJGU\nZak8TURBQYEXFuqSkYamcPkmJjw7l8Xrt3Pe4E7ccm5/2jZvFO+wRJKCmU2P9lKJRBtEFzliBd3b\n8NK3R3DTab14ae4avvDHd3jsoxVUVKbuP0si8aAEIimpUUY63z2jN//99sn069iCn06ax/n3TmV2\n8ZZ4hyaSMpRAJKX1at+CJ755PHeOG8yast2cd+9UfjJpLlt2anJGkSOlBCIpz8wYOziPt75/Clee\nmM+T04r5wh/f4alpxVSqW0uk1pRApMFo0TiTW8f058UbR3BUbjN++MwcLvzr+5pXS6SWlECkwenX\nsSVPXXMCf/zyIEo27+KCe9/npokzWb1lV7xDE0kqSiDSIJkZFw7tzOSbR3HDqT15Zd5aTr1jMn96\nbaEuQhSJkhKINGjNGmVw81l9eOvmUZw1oAN3vbWYU++YzNPTSzQ+InIYSiAiQF52E+66ZAjPXHci\nnbKbcPO/ZzP2nqm8v0R3QRQ5GCUQkQhDu7Xm2etO5M5xg9m4fQ+X/u0jLn/oY+atKot3aCIJRwlE\npIq0tPC035tH8dNz+jGnZAvn/uU9bnxiJis27oh3eCIJQ3NhiRzG1t37uP+dJfz9vWWUVziXDO/K\njaf1pF2LxvEOTaTO1WQuLCUQkSit37qbO9/8lInTislKT+Pqk/O5ekQPWjXNjHdoInVGCSSkBCL1\nYdmGHfzxtYW8OGcNLRpncNVJ+Vw1Ip9WTZRIJPkpgYSUQKQ+fbJ6K3e+uYhXi9bRonEG3xgRJJKW\njZVIJHkpgYSUQCQWilaXcecbn/LaJ+to2TiDb4zowZUjuiuRSFJSAgkpgUgszVtVxp1vfsrrn6yj\nVZNMvjEinytO7K6uLUkqCXtDKTMbbWYLzWyxmU04RLlhZlZuZhfVtK5IvAzMa8XfLi/gxRtHMKx7\nG/70+iJOuv0tfvvyfNZv2x3v8ETqXMyOQMwsHVgEnAGUANOAS9z9k2rKvQ7sJrhv+tPR1q1KRyAS\nT0Wry7hv8hL+O3cNGelpfKWgM9eMPIoubZrGOzSRg6rJEUhGfQcTYTiw2N2XApjZRGAsUDUJ3Ag8\nAwyrRV1XKqyGAAARZ0lEQVSRhDGgUyvuvvRYlm/Ywf1TlvDUtBKe+LiYMcd05LpRPenToUW8QxQ5\nIrHswsoDiiNel4TLPmNmecD5wH01rRuxjfFmVmhmhaWlpUcctMiR6p7TjN9ecAzv/uhUrjqpO699\nso6z/jyFbzwyjY+XbSKVxyEltSXaVCZ/Bn7k7pW13YC7P+DuBe5ekJubW4ehiRyZ9i0b89Mv9uf9\nCV/gu6f3ZsbKzXzl/g8Ye89Unpu1in0Vtf7Yi8RFLLuwVgFdIl53DpdFKgAmmhlADnCOmZVHWVck\nKWQ3zeKm03sxfmQPnplRwkNTl3HTxFnc/vICrjixO5cM66qr2yUpxHIQPYNgIPw0gi//acCl7l50\nkPKPAC+Gg+g1qrufBtElGVRWOpMXrefBd5fx/pKNNM1K58tDO3PlSfl0z2kW7/CkgUnIQXR3Lzez\nG4BXgXSCM6yKzOzacP1fa1o3FnGL1Le0NOMLfdvzhb7tKVpdxkPvLefxj1fy6IcrOL1fe644oTsn\n9WxLeGQukjB0IaFIAlq/dTePfrCCxz9eyaYde+mR24yvHd+NC4d21hXuUq90JXpICUSS3e59Ffx3\n7hoe/WAFs4q30CQznfOG5HH5Cd3o17FlvMOTFKQEElICkVQyt6SMf364nOdmrWZPeSXDurfmayd0\nZ/SADmRlJNoJlZKslEBCSiCSirbs3Mu/C0v454crWLlpJznNG3HR0M5cPKwL+Rp0lyOkBBJSApFU\nVlnpvPNpKY99uJK3F66notI5vkcbxg3ryuiBHWicmR7vECUJKYGElECkoVi3dTdPTy/hyWnFrNy0\nk1ZNMjl/SB4XD+uisRKpESWQkBKINDSVlc6HSzcycVoxr8xby96KSgZ1bsXFw7oyZlBHWugMLjkM\nJZCQEog0ZJt37GXSzFVMnLaSReu20ygjjbMGdOCCY/MY0TOHjHQNvMv/UgIJKYGIgLszq3gLz85Y\nxfOzV1O2ax+5LRpx3uBOXDi0M307qItLPqcEElICETnQnvIK3l5QyrMzSnhrwXrKK53+HVtywbF5\njB2cR26LRvEOUeJMCSSkBCJycJt27OXFOat5ZnoJs0vKSE8zRvbK4bwheZzerz3NGsVyrlVJFEog\nISUQkegsXr+dSTNLmDRjFavLdtM4M43T+rZnzKCOjOrTTqcENyBKICElEJGaqax0ZqzczAuzV/PS\n3DVs2L6X5o0yOHNAe8YM6sSInjlkavA9pSmBhJRARGqvvKKSD5du4oXZq3l53hq27i4nu2kmZw/s\nyJhBHTkuvy3paZohONUogYSUQETqxt7ySt79tJTnZ6/m9U/WsXNvBTnNG3HWgPaMHtiB43u01ZFJ\nilACCSmBiNS9XXsreHPBOl6eu5a3F65n594Ksptmcnq/9owe0IERvXI0ZpLElEBCSiAi9Wv3vgqm\nLCrllXlreX3+OrbtLqdZVjqn9m3H2QM7MqpPrs7mSjIJeUdCEUk9jTPTOXNAB84c0IG95ZV8sHQj\nr8xbw2tF63hxzhoaZaQxsncuZw3owKl9cmnbXNeZpJKYHoGY2WjgToLb0j7o7rdXWT8W+CVQCZQD\n33H398J1y4FtQAVQHk2G1BGISHxUVDrTlm/ilXlreWXeWtZu3U2awbFdW3Nav/ac3q8dPds11216\nE1BCdmGZWTqwCDgDKAGmAZe4+ycRZZoDO9zdzewY4Cl37xuuWw4UuPuGaPepBCISf+7OvFVbeX3+\nOt6cv46i1VsB6NqmKaf1a8cZ/dozLL+NBuETRKJ2YQ0HFrv7UgAzmwiMBT5LIO6+PaJ8MyB1B2hE\nGggz4+jOrTi6cyu+d0Zv1pTt4s3563lj/joe+2glD09dTovGGZzSO5fT+7VnVJ9csptmxTtsiUIs\nE0geUBzxugQ4rmohMzsf+C3QDvhixCoH3jCzCuB+d3+gup2Y2XhgPEDXrl3rJnIRqTMdWzXhq8d3\n46vHd2PHnnLeW7yBN+ev460F63lxzhrS04whXbIZ1SeXU3q3Y0CnlqTpepOEFMsurIuA0e5+dfj6\na8Bx7n7DQcqPBG5199PD13nuvsrM2gGvAze6+5RD7VNdWCLJo7LSmVWyhbfmr+edRaXMXVUGQE7z\nLEb2yuWUPrmM6Jmjgfh6lqhdWKuALhGvO4fLquXuU8ysh5nluPsGd18VLl9vZpMIusQOmUBEJHmk\npRnHdm3NsV1bc/NZfSjdtod3Py3lnUWlvL1wPc/OXIUZHJPXilN653JKn3YM7pKtq+HjKJZHIBkE\ng+inESSOacCl7l4UUaYnsCQcRD8WeIEg0TQF0tx9m5k1IzgCuc3dXznUPnUEIpIaKiqdeavKmLyw\nlHcWrWdW8RYqHVo1yWRErxxO7pnDST1z6NKmabxDTXoJeQTi7uVmdgPwKsFpvA+5e5GZXRuu/ytw\nIXC5me0DdgEXh8mkPTApPOUvA3j8cMlDRFJHepoxqEs2g7pkc9Ppvdiycy/vLd7AOwuDI5SX5qwB\ngjO7TuqZw0k923LiUTm0aabB+PqkK9FFJKm5O4vXb+e9xRuYungjHy7dyPY95QD079iSEb1yOPGo\ntgzPb0PTLF07fTgJeR1IPCiBiDQ85RWVzFlVxtRPNzB1yQZmrNjC3opKMtONIV1bMyI8Qjk6L5us\nDF17UpUSSEgJRER27a1g2vJNTF0cJJSi1Vtxh8aZaQzt1prj8ttyXH4bBnXJ1iSQJOgYiIhIPDTJ\nSmdk71xG9s4FYPOOvXy0bCMfLt3ER8s28X9vLMIdsjLSGNwlm+Pz23Bcj7Yc27U1TbKUUA5FRyAi\n0qCV7dzHx8s38dHSjXy0bBNFq8uodMhMN47pnM1xYUIp6Na6QcwsrC6skBKIiNTU1t37mL58Mx8u\n28hHSzcxd1UZFZVOeprRv2NLhnZrTUH31hR0a0OHVo3jHW6dUwIJKYGIyJHasaec6Ss289GyjRQu\n38zski3s3lcJQF52Ewq6t2Zot+DRt0PLpL+wUWMgIiJ1pFmjjAPGUPZVVPLJ6q0UrtjM9BWb+GDJ\nRp6btRqA5o0yGNI1OzhK6daGwV2zaZ7C3V46AhEROQLuTsnmXRSu2MT0FZspXL6Zheu24Q5pBv06\ntmRwl2wGd8lmSNdseuQ0T+jJIdWFFVICEZF42Lp7HzNXbmH68k1MX7mZOcVlbAsvbmzROINBnYOE\nMihMLLktEmeCSHVhiYjEUcvGmcGEj2G3V2Wls6R0OzOLtzCreAuzi7dw3ztLqKgM/oHPy27C4K7Z\nDAkTyoBOrZLiFGIlEBGRepaWZvRq34Je7VvwlYJgUvJdeyuYt7qMWSuDpDJr5ZbP5vRKTzP6dmjB\n4C7ZHJ0X3Iyrd/sWCXfXRnVhiYgkiPXbdjO7uIxZxZuZVbzlgK6vrIw0+nVoEdzdMa8VA/PqJ6lo\nDCSkBCIiyayy0lmxaSdzV5Uxb1UZc0q2ULRq64FJpWNLjs5ryTF52QzMa0Wv9s2PKKkogYSUQEQk\n1exPKnNKtjBvVVmYXLZ+NgNxVkYagzq34snxJ9TqbC8NoouIpKi0NCM/pxn5Oc0YOzgPCJLK8o07\nmLuqjLklZWzfUx6TU4WVQEREklxamtEjtzk9cpt/llRist+Y7UlERFJKTBOImY02s4VmttjMJlSz\nfqyZzTGzWWZWaGYjoq0rIiKxFbMEYmbpwD3A2UB/4BIz61+l2JvAIHcfDFwFPFiDuiIiEkOxPAIZ\nDix296XuvheYCIyNLODu2/3z08KaAR5tXRERia1YJpA8oDjidUm47ABmdr6ZLQBeIjgKibpuWH98\n2P1VWFpaWieBi4jI/0q4QXR3n+TufYHzgF/Wov4D7l7g7gW5ubl1H6CIiACxTSCrgC4RrzuHy6rl\n7lOAHmaWU9O6IiJS/2KZQKYBvcws38yygHHA85EFzKynmVn4/FigEbAxmroiIhJbMbuQ0N3LzewG\n4FUgHXjI3YvM7Npw/V+BC4HLzWwfsAu4OBxUr7bu4fY5ffr0DWa2opYh5wAbalk3UaVimyA125WK\nbYLUbFeqtalbtAVTei6sI2FmhdHOB5MsUrFNkJrtSsU2QWq2KxXbFK2EG0QXEZHkoAQiIiK1ogRy\ncA/EO4B6kIptgtRsVyq2CVKzXanYpqhoDERERGpFRyAiIlIrSiAiIlIrSiBVJPO08WbWxczeNrNP\nzKzIzG4Kl7cxs9fN7NPwZ+uIOj8O27rQzM6KX/SHZmbpZjbTzF4MX6dCm7LN7GkzW2Bm883shGRv\nl5l9N/zszTOzJ8yscTK2ycweMrP1ZjYvYlmN22FmQ81sbrjurv0XSqcMd9cjfBBcpLgE6AFkAbOB\n/vGOqwbxdwSODZ+3ABYRTH//e2BCuHwC8Lvwef+wjY2A/LDt6fFux0Ha9j3gceDF8HUqtOkfwNXh\n8ywgO5nbRTDB6TKgSfj6KeDrydgmYCRwLDAvYlmN2wF8DBwPGPAycHa821aXDx2BHCipp4139zXu\nPiN8vg2YT/BHPZbgy4rw53nh87HARHff4+7LgMUE70FCMbPOwBcJ7w8TSvY2tSL4kvo7gLvvdfct\nJHm7CGa3aGJmGUBTYDVJ2CYP5uLbVGVxjdphZh2Blu7+oQfZ5NGIOilBCeRAUU8bn+jMrDswBPgI\naO/ua8JVa4H24fNkae+fgR8ClRHLkr1N+UAp8HDYNfegmTUjidvl7quAO4CVwBqgzN1fI4nbVEVN\n25EXPq+6PGUogaQgM2sOPAN8x923Rq4L/xNKmnO3zexcYL27Tz9YmWRrUyiDoIvkPncfAuwg6Bb5\nTLK1KxwTGEuQHDsBzczsq5Flkq1NB5Mq7ThSSiAHSvpp480skyB5PObuz4aL14WH04Q/14fLk6G9\nJwFfMrPlBF2KXzCzf5HcbYLgv9ESd/8ofP00QUJJ5nadDixz91J33wc8C5xIcrcpUk3bsSp8XnV5\nylACOVBSTxsfnuHxd2C+u/8pYtXzwBXh8yuA5yKWjzOzRmaWD/QiGPRLGO7+Y3fv7O7dCX4fb7n7\nV0niNgG4+1qg2Mz6hItOAz4hudu1EjjezJqGn8XTCMbhkrlNkWrUjrC7a6uZHR++H5dH1EkN8R7F\nT7QHcA7B2UtLgJ/GO54axj6C4LB6DjArfJwDtAXeBD4F3gDaRNT5adjWhST4GSLAKD4/Cyvp2wQM\nBgrD39d/gNbJ3i7gF8ACYB7wT4Izk5KuTcATBOM4+wiOFr9Rm3YABeF7sQS4m3D2j1R5aCoTERGp\nFXVhiYhIrSiBiIhIrSiBiIhIrSiBiIhIrSiBiIhIrSiBiETBzLaHP7ub2aV1vO2fVHn9fl1uX6S+\nKIGI1Ex3oEYJJJxY8FAOSCDufmINYxKJCyUQkZq5HTjZzGaF975IN7M/mNk0M5tjZtcAmNkoM3vX\nzJ4nuMIcM/uPmU0P75cxPlx2O8HstbPM7LFw2f6jHQu3PS+8p8TFEdueHHEvkcdS7j4TkhQO95+R\niBxoAnCzu58LECaCMncfZmaNgKlm9lpY9lhgoAdTfANc5e6bzKwJMM3MnnH3CWZ2g7sPrmZfFxBc\nrT4IyAnrTAnXDQEGEEyXPpVgzrD36r65IgenIxCRI3MmcLmZzSKYOr8twVxIEMyHtCyi7LfNbDbw\nIcHke704tBHAE+5e4e7rgHeAYRHbLnH3SoIpa7rXSWtEakBHICJHxoAb3f3VAxaajSKYoj3y9enA\nCe6+08wmA42PYL97Ip5XoL9liQMdgYjUzDaC2wXv9ypwXTiNPmbWO7wxVFWtgM1h8uhLcJvT/fbt\nr1/Fu8DF4ThLLsEdDBN5tlppYPRfi0jNzAEqwq6oR4A7CbqPZoQD2aVUf9vSV4BrzWw+wYytH0as\newCYY2Yz3P2yiOWTgBMI7rftwA/dfW2YgETiTrPxiohIragLS0REakUJREREakUJREREakUJRERE\nakUJREREakUJREREakUJREREauX/A5rzCf43ycCuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4eb2b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.62225886],\n",
       "       [ 1.20995977],\n",
       "       [ 0.95376563],\n",
       "       [-1.71023516],\n",
       "       [-1.56657761]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#theta_init = np.random.normal(0,0.01,size=(5,1))\n",
    "#create one theta_init for repeating code!\n",
    "theta_init = np.array([[-0.0209818 ],\n",
    "                       [ 0.00366465],\n",
    "                       [-0.00123329],\n",
    "                       [-0.00730515],\n",
    "                       [ 0.01762441]])\n",
    "print('theta_init: ',theta_init,'\\n')\n",
    "\n",
    "# set a learning rate\n",
    "learning_rate = 0.1\n",
    "# maximum number of iterations for gradient descent\n",
    "maxepochs = 20000       \n",
    "# costs convergence threshold, ie. (prevcost - cost) > convergence_thres\n",
    "convergence_thres = 0.0001  \n",
    "\n",
    "def learn(inputs, actuals, weights, learning_rate, maxepochs, convergence_thres):\n",
    "    costs = []\n",
    "    cost = singlecost(inputs, actuals, weights)  # compute initial cost\n",
    "    costprev = cost + convergence_thres + 0.01  # set an inital costprev to past while loop\n",
    "    counter = 0  # add a counter\n",
    "    # Loop through until convergence\n",
    "    for counter in range(maxepochs):\n",
    "        slopes_of_weights_avg = np.zeros(weights.shape)\n",
    "        for j, row in enumerate(inputs):\n",
    "            output = sigmoid_activation(row, weights)   # Compute activation\n",
    "            single_row_slopes = (actuals[j]-output) * output * (1-output) * row   # Get delta\n",
    "            slopes_of_weights_avg += single_row_slopes[:,np.newaxis]/X.shape[0]  # accumulate\n",
    "        \n",
    "        # update parameters \n",
    "        weights += (slopes_of_weights_avg * learning_rate)\n",
    "        counter += 1  # count\n",
    "        costprev = cost  # store prev cost\n",
    "        cost = singlecost(inputs, actuals, weights) # compute new cost\n",
    "        costs.append(cost)\n",
    "        if np.abs(costprev-cost) < convergence_thres:\n",
    "            break\n",
    "        \n",
    "    plt.plot(costs)\n",
    "    plt.title(\"Convergence of the Cost Function\")\n",
    "    plt.ylabel(\"J($\\Theta$)\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "    return weights\n",
    "        \n",
    "updated_weights = learn(X, y, theta_init, learning_rate, maxepochs, convergence_thres)\n",
    "updated_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neat! That actually worked! So essentially what we are doing above is subtracting from or adding to the weights lil by lil until we get to a slope of zero, and a minimized cost function. As the slopes get closer and closer to zero, the change in weights (and therefore the change in cost) will be minimial, because you are change the weights by slope * learning rate (ie. if the slope is close to zero, then u will be chaning the weights by a veerrrryyyy tiny amount... and so the cost function will not change very much). This is why we break the function at a certain threshold (ie if the cost function is not decreasing very much any more, we can break the function as we have gotten pretty close to as low as can go!!!).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Neural Network\n",
    "\n",
    "Neural networks are usually built using mulitple layers of neurons. Adding more layers into the network allows you to learn more complex functions. Here's a picture representing a 3 layer neural network.\n",
    "\n",
    "![](pictures\\J1Y6YEf.png)\n",
    "\n",
    "We have a 3 layer neural network with four input variables x1,x2,x3, and x4 and a bias unit. Each variable and bias unit is then sent to four hidden units, a1(2),a2(2),a3(2), and a4(2). The hidden units have different sets of parameters θ.\n",
    "\n",
    "$a_1^{(2)} = g(\\theta_{1,0}^{(1)} + \\theta_{1,1}^{(1)} x_1 + \\theta_{1,2}^{(1)} x_2 + \\theta_{1,3}^{(1)} x_3 + \\theta_{1,4}^{(1)} x_4)$\n",
    "\n",
    "$a_2^{(2)} = g(\\theta_{2,0}^{(1)} + \\theta_{2,1}^{(1)} x_1 + \\theta_{2,2}^{(1)} x_2 + \\theta_{2,3}^{(1)} x_3 + \\theta_{2,4}^{(1)} x_4)$\n",
    "\n",
    "$a_3^{(2)} = g(\\theta_{3,0}^{(1)} + \\theta_{3,1}^{(1)} x_1 + \\theta_{3,2}^{(1)} x_2 + \\theta_{3,3}^{(1)} x_3 + \\theta_{3,4}^{(1)} x_4)$\n",
    "\n",
    "$a_4^{(2)} = g(\\theta_{4,0}^{(1)} + \\theta_{4,1}^{(1)} x_1 + \\theta_{4,2}^{(1)} x_2 + \\theta_{4,3}^{(1)} x_3 + \\theta_{4,4}^{(1)} x_4)$\n",
    "\n",
    "\n",
    "$\\theta_{i,k}^{(j)}$ represents the parameter of input unit k which transform the units in layer j to activation unit $a_i^{(j+1)}$.\n",
    "\n",
    "This layer is known as a hidden layer because the user does not directly interact with it by passing or retrieving data. The third and final layer is the \n",
    "output, or prediction, of our model. Similar to how each variable was sent to each neuron in the hidden layer, the activation units in each neuron are then sent to each neuron on the next layer. Since there is only a single layer, we can write it as:\n",
    "\n",
    "$h_{\\Theta}(X) = g(\\theta_{1,0}^{(2)} + \\theta_{1,1}^{(2)} a_1^{(2)} + \\theta_{1,2}^{(2)} a_2^{(2)} + \\theta_{1,3}^{(2)} a_3^{(2)} + \\theta_{1,4}^{(2)} a_4^{(2)})$ \n",
    "\n",
    "While the mathematical notation may seem confusing at first, at a high level, we are organizing multiple logistic regression models to create a more complex function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta0_init = np.random.normal(0,0.01,size=(5,4))\n",
    "theta1_init = np.random.normal(0,0.01,size=(5,1))\n",
    "def feedforward(X, theta0, theta1):\n",
    "    # feedforward to the first layer\n",
    "    a1 = sigmoid_activation(X.T, theta0).T\n",
    "    # add a column of ones for bias term\n",
    "    a1 = np.column_stack([np.ones(a1.shape[0]), a1])\n",
    "    # activation units are then inputted to the output layer\n",
    "    out = sigmoid_activation(a1.T, theta1)\n",
    "    return out\n",
    "\n",
    "h = feedforward(X, theta0_init, theta1_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_weights.T shape:  (4, 5) \n",
      "\n",
      "X.T shape:  (5, 100) \n",
      "\n",
      " ...This dot product should produce a 4*100 matrix \n",
      "\n",
      "hidden layer values shape:  (4, 100) \n",
      "\n",
      "Each row represents the 4 hidden layer output values for each row of input data (4 for each input row!!!)\n"
     ]
    }
   ],
   "source": [
    "first_weights = np.array([[  2.86999282e-03,   2.52850202e-04,  -7.54220358e-03, 7.37058461e-03],\n",
    "          [  6.92288115e-03,  -1.71379272e-02,   1.62054417e-02, -1.24161252e-02],\n",
    "          [  2.61033604e-03,   4.17650574e-03,  -8.59951945e-03, -3.89896041e-03],\n",
    "          [  6.04576405e-03,   6.92358290e-03,   6.43781547e-03, -6.51997646e-05],\n",
    "          [ -1.08470508e-02,   7.39129180e-03,   9.83259085e-03, -7.80096354e-03]])\n",
    "\n",
    "second_weights = np.array([[-0.0209818 ],\n",
    "                       [ 0.00366465],\n",
    "                       [-0.00123329],\n",
    "                       [-0.00730515],\n",
    "                       [ 0.01762441]])\n",
    "\n",
    "print('first_weights.T shape: ',first_weights.T.shape,'\\n')\n",
    "print('X.T shape: ',X.T.shape,'\\n'*2,\"...This dot product should produce a 4*100 matrix\",'\\n')\n",
    "hidden_layer_nodes = sigmoid_activation(X.T,first_weights)\n",
    "print('hidden layer values shape: ',hidden_layer_nodes.shape,'\\n')\n",
    "\n",
    "print('Each row represents the 4 hidden layer output values for each row of input data (4 for each input row!!!)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed hidden layer with 1s:  (100, 5) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.51897541,  0.48501802,  0.53825677,  0.47046682],\n",
       "       [ 1.        ,  0.51599734,  0.4869798 ,  0.5248441 ,  0.4792024 ],\n",
       "       [ 1.        ,  0.51817815,  0.4868715 ,  0.53502987,  0.47271526],\n",
       "       [ 1.        ,  0.51575232,  0.48886956,  0.52865766,  0.47641106],\n",
       "       [ 1.        ,  0.51738036,  0.48855134,  0.53245426,  0.47366497],\n",
       "       [ 1.        ,  0.51303727,  0.48913664,  0.52056414,  0.48229689],\n",
       "       [ 1.        ,  0.51565085,  0.48940906,  0.53130638,  0.47430838],\n",
       "       [ 1.        ,  0.51714284,  0.4869673 ,  0.53320841,  0.47324519],\n",
       "       [ 1.        ,  0.51796064,  0.48815404,  0.53544814,  0.47104197],\n",
       "       [ 1.        ,  0.51485323,  0.48626581,  0.523691  ,  0.47961935]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now we need to stack another bias layer on top, of the hidden layer nodes, and then calculate the next layer\n",
    "\n",
    "hidden_layer_nodes = np.column_stack([np.ones(hidden_layer_nodes.T.shape[0]),hidden_layer_nodes.T])\n",
    "print('Transposed hidden layer with 1s: ',hidden_layer_nodes.shape,'\\n')\n",
    "\n",
    "hidden_layer_nodes[0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_weights transposed shape:  (1, 5) \n",
      "\n",
      "Hidden layer oupouts transpose shape:  (5, 100) \n",
      "\n",
      "Output nodes shape:  (1, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.49617046,  0.49623011,  0.49618496,  0.49621004,  0.4961926 ,\n",
       "         0.49624818,  0.49619568,  0.49618964,  0.49617623,  0.49623323,\n",
       "         0.49624358,  0.49619729,  0.49620846,  0.49619829,  0.49621323,\n",
       "         0.49621467,  0.49621713,  0.4962113 ,  0.49619907,  0.4961707 ,\n",
       "         0.49620331,  0.49624598,  0.49622732,  0.49623461,  0.49621386,\n",
       "         0.49619893,  0.49620404,  0.49623498,  0.49623258,  0.4962139 ,\n",
       "         0.49621346,  0.49621207,  0.49621758,  0.49621755,  0.49618234,\n",
       "         0.49622005,  0.49619442,  0.49621692,  0.49618715,  0.49621561,\n",
       "         0.4962085 ,  0.49620057,  0.4962159 ,  0.49622869,  0.49622708,\n",
       "         0.49619522,  0.49622711,  0.49621501,  0.49619913,  0.49620969,\n",
       "         0.49619668,  0.49620668,  0.49616879,  0.49623644,  0.49621028,\n",
       "         0.49618801,  0.49620059,  0.49617441,  0.49623095,  0.49620464,\n",
       "         0.49622842,  0.49621084,  0.49618707,  0.49622996,  0.49622675,\n",
       "         0.49621346,  0.49621396,  0.49620645,  0.49622015,  0.49622622,\n",
       "         0.496214  ,  0.49620492,  0.49618774,  0.49619125,  0.49621655,\n",
       "         0.49618577,  0.49624638,  0.49618616,  0.49622888,  0.49623169,\n",
       "         0.49618532,  0.49623431,  0.49620442,  0.49622846,  0.49620541,\n",
       "         0.49622686,  0.49622   ,  0.49617402,  0.49619141,  0.49617094,\n",
       "         0.49621004,  0.49622479,  0.49620151,  0.49620829,  0.49623097,\n",
       "         0.4962159 ,  0.49619048,  0.49619276,  0.49620037,  0.49620217]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time for the third layer - the prediction/output node!\n",
    "print('second_weights transposed shape: ',second_weights.T.shape,'\\n')\n",
    "print('Hidden layer oupouts transpose shape: ',hidden_layer_nodes.T.shape,'\\n')\n",
    "output_nodes = sigmoid_activation(hidden_layer_nodes.T,second_weights)\n",
    "print('Output nodes shape: ',output_nodes.shape)\n",
    "output_nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Multiple Neural Network Cost Function\n",
    "The cost function to multiple layer neural networks is identical to the cost function we used in the last screen, but $h_{\\Theta}(x_{i})$ is more complicated.\n",
    "\n",
    "$J(\\Theta) = -\\dfrac{1}{m} \\sum_{i=1}^m (y_{i} * log(h_{\\Theta}(x_{i}))  +  (1-y_{i}) log(1-h_{\\Theta}(x_i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69315004691061033"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_weights = np.array([[  2.86999282e-03,   2.52850202e-04,  -7.54220358e-03, 7.37058461e-03],\n",
    "          [  6.92288115e-03,  -1.71379272e-02,   1.62054417e-02, -1.24161252e-02],\n",
    "          [  2.61033604e-03,   4.17650574e-03,  -8.59951945e-03, -3.89896041e-03],\n",
    "          [  6.04576405e-03,   6.92358290e-03,   6.43781547e-03, -6.51997646e-05],\n",
    "          [ -1.08470508e-02,   7.39129180e-03,   9.83259085e-03, -7.80096354e-03]])\n",
    "\n",
    "second_weights = np.array([[-0.0209818 ],\n",
    "                       [ 0.00366465],\n",
    "                       [-0.00123329],\n",
    "                       [-0.00730515],\n",
    "                       [ 0.01762441]])\n",
    "\n",
    "# X and y are in memory and should be used as inputs to multiplecost()\n",
    "def multiplecost(inputs, actuals, weights_1, weights_2):\n",
    "    # feed through network\n",
    "    outputs = feedforward(X, weights_1, weights_2) \n",
    "    # compute error\n",
    "    costs = actuals * np.log(outputs) + (1-actuals) * np.log(1-outputs)\n",
    "    # negative of average error\n",
    "    return -np.mean(costs)\n",
    "\n",
    "c = multiplecost(X, y, first_weights, second_weights)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, this is interesting. With these weights we get a cost of 0.6931. In other words, taking all the obersevations, calculating the error, then taking the mean of that error, we get the value of the cost function. Our next step is to minimize this error using gradient descent!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Backpropogation\n",
    "\n",
    "Now that we have mulitple layers of parameters to learn, we must implement a method called backpropagation. We've already implemented forward propagation by feeding the data through each layer and returning an output. Backpropagation focuses on updating parameters starting at the last layer and circling back through each layer, updating accordingly. As there are multiple layers we are forced to compute $\\dfrac{\\partial}{\\partial \\Theta_{i,j}^{(l)}} J(\\Theta)$ where $l$ is the layer. For a three layer network, use the following approach:\n",
    "\n",
    "- $\\delta_j^l \\text{ is the 'error' for unit j in layer l}$\n",
    "- $\\delta^3 = h_\\Theta(X) - y$\n",
    "- $(2))Tδ3.∗g′(z(2))$\n",
    "\n",
    "**$\\text{There is no } \\delta^1 \\text{ since the first layer are the features and have no error.}$**\n",
    "\n",
    "We have written code that trains a three layer neural network in the code cell. You will notice that there are many parameters and moving parts to this algorithm. To make the code more modular, we have refactored our previous code as a class, allowing us to organize related attributes and methods.\n",
    "\n",
    "We have reused feedforward() and multiplecost() but in more condensed forms. During initialization, we set attributes like the learning rate, maximum number of iterations to convergence, and number of units in the hidden layer. In learn() you'll find the backpropagation algorithm, which computes the gradients and updates the parameters. We then test the class by using the features and the species of the flower.\n",
    "\n",
    "<font color=red> Remember, 1e4 equalas 10000, because you move the decimal to the right 4 places!!! And 1e-5 is 0.00001 because you move the decimal to the left by 5 places!!! That's scientific notation in case you forgot... which you probably did because you haven't done anymath since highschool..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEWCAYAAACT7WsrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCT1/wH3GtwkHgpI6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6661fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a class for this model, it's good practice and condenses the code\n",
    "class NNet3:\n",
    "    # __init__ initializes the instance of the class. SO every time you call this class, you need these inputs. Right?\n",
    "    def __init__(self, learning_rate=0.5, maxepochs=1e4, convergence_thres=1e-5, hidden_layer=4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.maxepochs = int(maxepochs)\n",
    "        self.convergence_thres = convergence_thres\n",
    "        self.hidden_layer = int(hidden_layer)\n",
    "    \n",
    "    # this is another method in the class (functions inside classes are called methods).\n",
    "    # you can call this method on a instance of this class to get the result\n",
    "    # ALSO, a single underscore before a function is meant to denote that the function is meant to be private...\n",
    "    # ... which I guess means its not really meant to be called. \n",
    "    def _multiplecost(self, X, y):\n",
    "        # feed through network\n",
    "        l1, l2 = self._feedforward(X) \n",
    "        # compute error\n",
    "        inner = y * np.log(l2) + (1-y) * np.log(1-l2)\n",
    "        # negative of average error\n",
    "        return -np.mean(inner)\n",
    "    \n",
    "    def _feedforward(self, X):\n",
    "        # feedforward to the first layer\n",
    "        l1 = sigmoid_activation(X.T, self.theta0).T\n",
    "        # add a column of ones for bias term\n",
    "        l1 = np.column_stack([np.ones(l1.shape[0]), l1])\n",
    "        # activation units are then inputted to the output layer\n",
    "        l2 = sigmoid_activation(l1.T, self.theta1)\n",
    "        return l1, l2\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        _, y = self._feedforward(X)\n",
    "        return y\n",
    "    \n",
    "    def learn(self, X, y):\n",
    "        nobs, ncols = X.shape\n",
    "        self.theta0 = np.random.normal(0,0.01,size=(ncols,self.hidden_layer))\n",
    "        self.theta1 = np.random.normal(0,0.01,size=(self.hidden_layer+1,1))\n",
    "        \n",
    "        self.costs = []\n",
    "        cost = self._multiplecost(X, y)\n",
    "        self.costs.append(cost)\n",
    "        costprev = cost + self.convergence_thres+1  # set an inital costprev to past while loop\n",
    "        counter = 0  # intialize a counter\n",
    "\n",
    "        # Loop through until convergence\n",
    "        for counter in range(self.maxepochs):\n",
    "            # feedforward through network\n",
    "            l1, l2 = self._feedforward(X)\n",
    "\n",
    "            # Start Backpropagation\n",
    "            # Compute gradients\n",
    "            l2_delta = (y-l2) * l2 * (1-l2)\n",
    "            l1_delta = l2_delta.T.dot(self.theta1.T) * l1 * (1-l1)\n",
    "\n",
    "            # Update parameters by averaging gradients and multiplying by the learning rate\n",
    "            self.theta1 += l1.T.dot(l2_delta.T) / nobs * self.learning_rate\n",
    "            self.theta0 += X.T.dot(l1_delta)[:,1:] / nobs * self.learning_rate\n",
    "            \n",
    "            # Store costs and check for convergence\n",
    "            counter += 1  # Count\n",
    "            costprev = cost  # Store prev cost\n",
    "            cost = self._multiplecost(X, y)  # get next cost\n",
    "            self.costs.append(cost)\n",
    "            if np.abs(costprev-cost) < self.convergence_thres and counter > 500:\n",
    "                break\n",
    "\n",
    "# Set a learning rate\n",
    "learning_rate = 0.5\n",
    "# Maximum number of iterations for gradient descent\n",
    "maxepochs = 10000       \n",
    "# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres\n",
    "convergence_thres = 0.00001  \n",
    "# Number of hidden units\n",
    "hidden_units = 4\n",
    "\n",
    "# Initialize model \n",
    "model = NNet3(learning_rate=learning_rate, maxepochs=maxepochs,\n",
    "              convergence_thres=convergence_thres, hidden_layer=hidden_units)\n",
    "# Train model\n",
    "model.learn(X, y)\n",
    "\n",
    "# Plot costs\n",
    "plt.plot(model.costs)\n",
    "plt.title(\"Convergence of the Cost Function\")\n",
    "plt.ylabel(\"J($\\Theta$)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> NOTE: In the code above, you have to run model.learn before you can run model.predict, because model.learn sets the theta values... But I guess that makes sense. Because you want to train the model first before you make any predictions... plus if you really wanted to 'predict' with some random theta (or weight) values, you would just use the separate feed forward function. </font>\n",
    "\n",
    "**Neat! Lets play around with this a little to get a better understanding of whats really happening under the hood here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEWCAYAAACT7WsrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSSEhIiJ9UkiIiEifFBIiItKn/w82v8spR0EwUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6a29da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NNet3_vM:\n",
    "    # __init__ initializes the instance of the class. SO every time you call this class, you need these inputs. Right?\n",
    "    def __init__(self, learning_rate=0.5, maxepochs=1e4, convergence_thres=1e-5, hidden_layer_nodes=4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.maxepochs = int(maxepochs)\n",
    "        self.convergence_thres = convergence_thres\n",
    "        self.hidden_layer_nodes = int(hidden_layer_nodes)\n",
    "    \n",
    "    # this is another method in the class (functions inside classes are called methods).\n",
    "    # you can call this method on a instance of this class to get the result\n",
    "    # ALSO, a single underscore before a function is meant to denote that the function is meant to be private...\n",
    "    # ... which I guess means its not really meant to be called. \n",
    "    def _multiplecost(self, X, y):\n",
    "        # feed through network\n",
    "        layer1, layer2 = self._feedforward(X) \n",
    "        # compute error\n",
    "        costs_sum = y * np.log(layer2) + (1-y) * np.log(1-layer2)\n",
    "        # negative of average error\n",
    "        return -np.mean(costs_sum)\n",
    "    \n",
    "    def _feedforward(self, X):\n",
    "        # feedforward to the first layer\n",
    "        layer1 = sigmoid_activation(X.T, self.weights1).T\n",
    "        # add a column of ones for bias term\n",
    "        layer1 = np.column_stack([np.ones(layer1.shape[0]), layer1])\n",
    "        # activation units are then inputted to the output layer\n",
    "        layer2 = sigmoid_activation(layer1.T, self.weights2)\n",
    "        return layer1, layer2\n",
    "    \n",
    "    # use the '_', as shown in this function, when assigning variables you don't want to return!\n",
    "    def predict(self, X):\n",
    "        _, y = self._feedforward(X)\n",
    "        return y\n",
    "    \n",
    "    def learn(self, X, y):\n",
    "        nrows, ncols = X.shape\n",
    "        # create FIVE weight1s for each of the HIDDEN LAYERS NODES(there are 4 in the initial case above)\n",
    "        self.weights1 = np.random.normal(0,0.01,size=(ncols,self.hidden_layer_nodes))\n",
    "        \n",
    "        # create 1 weight2s for each of the hidden layer... There are n+1 hidden layer nodes...\n",
    "        # n is number of hidden layers nodes we specified, add col of 1s to the data so there are 5 weights needed\n",
    "        self.weights2 = np.random.normal(0,0.01,size=(self.hidden_layer_nodes+1,1))\n",
    "        \n",
    "        self.costs = []\n",
    "        cost = self._multiplecost(X, y)\n",
    "        self.costs.append(cost)\n",
    "        costprev = cost + self.convergence_thres+1  # set an inital costprev to past while loop\n",
    "        counter = 0  # intialize a counter\n",
    "\n",
    "        # Loop through until convergence\n",
    "        for counter in range(self.maxepochs):\n",
    "            # feedforward through network\n",
    "            layer1, layer2 = self._feedforward(X)\n",
    "\n",
    "            # Start Backpropagation\n",
    "            # Compute gradients\n",
    "            weight2_slopes = (y-layer2) * layer2 * (1-layer2)\n",
    "            weight1_slopes = weight2_slopes.T.dot(self.weights2.T) * layer1 * (1-layer1)\n",
    "\n",
    "            # Update parameters by averaging gradients and multiplying by the learning rate\n",
    "            \n",
    "            # for weights2 you have to multiply by layer1 (because, as we learned from datacamp, one of the four...\n",
    "            # ...components you need to multiply to get the slopes is the value of the node that FEEDS INTO the weight...\n",
    "            # ...which in this case is layer1 (the hidden layer)).\n",
    "            # for some reason, we have not done this in the weight2_slopes calc, and do it here instead. Not sure why tbh..\n",
    "            self.weights2 += layer1.T.dot(weight2_slopes.T) / nrows * self.learning_rate\n",
    "            \n",
    "            # just like the adjustment above, we multiple the weight slopes above by the node that FEEDS INTO the...\n",
    "            # ...weight. In this case, for weights1, the node that feeds into the weight is the input (X).\n",
    "            # Next we simply take the first column off the matrix (not sure why, figure this out later), divide by the...\n",
    "            # ...nrows to get the average slope, multiple by the learning rate, and then ADD (not subtract) to the...\n",
    "            # current weights. Not sure why we add... I think its because its a logistic regression and it switches the...\n",
    "            # signs around in the calcs... because we take the negative of the mean of slopes in the calc if you recall. \n",
    "            self.weights1 += X.T.dot(weight1_slopes)[:,1:] / nrows * self.learning_rate\n",
    "            \n",
    "            # Store costs and check for convergence\n",
    "            counter += 1  # Count\n",
    "            costprev = cost  # Store prev cost\n",
    "            cost = self._multiplecost(X, y)  # get next cost\n",
    "            self.costs.append(cost)\n",
    "            if np.abs(costprev-cost) < self.convergence_thres and counter > 500:\n",
    "                break\n",
    "\n",
    "# Set a learning rate\n",
    "learning_rate = 0.5\n",
    "# Maximum number of iterations for gradient descent\n",
    "maxepochs = 10000       \n",
    "# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres\n",
    "convergence_thres = 0.00001  \n",
    "# Number of hidden units\n",
    "hidden_units = 4\n",
    "\n",
    "# Initialize model \n",
    "model = NNet3_vM(learning_rate=learning_rate, maxepochs=maxepochs,\n",
    "              convergence_thres=convergence_thres, hidden_layer_nodes=hidden_units)\n",
    "# Train model\n",
    "model.learn(X, y)\n",
    "\n",
    "# Plot costs\n",
    "plt.plot(model.costs)\n",
    "plt.title(\"Convergence of the Cost Function\")\n",
    "plt.ylabel(\"J($\\Theta$)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Splitting Data\n",
    "\n",
    "Now that we have learned about neural networks, learned about backpropagation, and have code which will train a 3-layer neural network, we will split the data into training and test datasets and run the model.\n",
    "\n",
    "- Choose the first 70 rows in both X and y and assign them respectively to X_train and y_train.\n",
    "- The last 30 rows should be assigned to variables X_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First 70 rows to X_train and y_train\n",
    "# Last 30 rows to X_test and y_test\n",
    "X_train = X[:70]\n",
    "y_train = y[:70]\n",
    "\n",
    "X_test = X[-30:]\n",
    "y_test = y[-30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Predicting Iris Flowers\n",
    "\n",
    "To benchmark how well a three layer neural network performs when predicting the species of iris flowers, you will have to compute the AUC, area under the curve, score of the receiver operating characteristic. The function NNet3 not only trains the model but also returns the predictions. The method predict() will return a 2D matrix of probabilities. Since there is only one target variable in this neural network, select the first row of this matrix, which corresponds to the type of flower.\n",
    "\n",
    "- Train the neural network using X_test and y_test and model, which has been initialized with a set of parameters.\n",
    "- Once training is complete, use the predict() function to return the probabilities of the flower matching the species Iris-versicolor.\n",
    "- Compute the AUC score, using roc_auc_score() and assign it to auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56976864  0.06258701  0.18635891  0.06753133  0.11770418  0.04062221\n",
      "  0.99594089  0.02570479  0.9901387   0.91293318  0.05832532  0.06096316\n",
      "  0.98562578  0.99354224  0.02061874  0.98477151  0.97745225  0.02662189\n",
      "  0.45665084  0.37092065  0.96077731  0.8284595   0.12998042  0.95434457\n",
      "  0.90617368  0.04572712  0.05683757  0.01740779  0.98144347  0.06710265]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Set a learning rate\n",
    "learning_rate = 0.5\n",
    "# Maximum number of iterations for gradient descent\n",
    "maxepochs = 10000       \n",
    "# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres\n",
    "convergence_thres = 0.00001  \n",
    "# Number of hidden units\n",
    "hidden_units = 4\n",
    "\n",
    "# Initialize model \n",
    "model = NNet3_vM(learning_rate=learning_rate, maxepochs=maxepochs,\n",
    "              convergence_thres=convergence_thres, hidden_layer_nodes=hidden_units)\n",
    "model.learn(X_train, y_train)\n",
    "\n",
    "yhat = model.predict(X_test)[0]\n",
    "\n",
    "auc = roc_auc_score(y_test, yhat)\n",
    "\n",
    "print(yhat)\n",
    "auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
