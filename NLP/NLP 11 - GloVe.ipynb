{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 11. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recommender systems and matrix factorization\n",
    "\n",
    "##### This is a brief detour, but when we move onto Glove it will be very clear why we did this first!\n",
    "So whats the deal with recommender systems?\n",
    "- Suppose you are Amazon and want to predict how much customer will like a product.\n",
    "- You could frame it as a typical ML problem (ie you have set of user attributes and product attributes and combine into prediction model with USER RATINGS as labeled data)\n",
    "- **But what if we want to learn PURELY from user ratings? Yes, and the answer is very related to PCA:**\n",
    "    - **If one word row-vector looks like another word row-vector (eg. the word appears in the same documents a similar number of times), then these words are likely similar!!!**\n",
    "    - So if two distinct words appear in a similar number of times in the same documents (or similar documents), then they are likely to have similar meaning!\n",
    "    \n",
    "We can apply some similar to movie ratings:  \n",
    "![](pictures/NLP_11_movieratings.png)\n",
    "\n",
    "- We see that one user likes a lot of the same movie as another user.\n",
    "- So what if we didn't have some of these ratings? How do we will them in? **Matrix Factorization!**\n",
    "\n",
    "### Matrix Factorization\n",
    "- We pretend that the big original movie matrix exists in full (with NO empty spaces).\n",
    "- We can model this big full matrix by saying its a product of two smaller matrices:\n",
    "    - W (NxK): User matrix, each row represents a user\n",
    "    - U.T (KxM): movie matrix, each row represents a movie \n",
    "        - **K: The rank**, the smaller dimension\n",
    "- When you multiply the matrices together, you get back the FULL SIZE matrix:\n",
    "    - A(NxM) ~= W(NxK).dot(U.T(KxM))\n",
    "- Total parameters are N\\*K + K\\*M: ** But you want to choose a K thats just right, as too large and you will overfit, too little and you will have a crappy model.**\n",
    "    \n",
    "\n",
    "    \n",
    "![](pictures/NLP_11_matrixfact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prediction\n",
    "Suppose we want to predict what a user 'i' has rated movie 'j':\n",
    "- A(i,j) = Full movie matrix in row i (the user) and column j (the movie)\n",
    "- A(i,j) = W(i).T.dot(U(j))\n",
    "\n",
    "Careful here! if U is defined as K x M matrix, then:\n",
    "- A ~ WU, and A(i,j) = W(i,:).T.dot(U(:,j) \n",
    "\n",
    "### Training\n",
    "- Regression problem ---> squared error\n",
    "- Cost function: J = ΣiΣj ( W(i)<sup>t</sup>U(j) - A(i,j) )<sup>2</sup> \n",
    "    - We can also treat A like a probability matrix and use KL-divergence\n",
    "    - But we will stick to squarred errorr!\n",
    "- i=1... N, j=1...M? NO!!! **Because we can only check the model for values that actually exist((**\n",
    "- So we hope that A is full enough such that meaningful patterns can be found. \n",
    "- So we only train the model on existing data points (omega):\n",
    "    - i,j ∈ Ω\n",
    "    - Ω = set of all existing (i,j pairs)\n",
    "- So the next step is to take the derivative and set it to 0\n",
    "![](pictures/NLP_11_matfact.png)\n",
    "- Just like we do with deep learning, we take the deriviative **with respect to each matrix one at a a time**. However, what we have here is a closed form solution!!! WOOOT!\n",
    "- Be careful about the sums, we are only summing for the values for which A(i,j) is defined. \n",
    "    - Ωi means the set of all movies j that user i has rated\n",
    "    - Ωj means the set of all users i who have rated movie j\n",
    "- The tricky part here is invert the part where we have the dot product multiplied by a vector... And solving these two equations gets you U in terms of W, and W in terms of U, but **it doesnt solve for both!!!**.\n",
    "- OMG WHAT DO WE DO!!!!\n",
    "\n",
    "### Alternating Least Squares\n",
    "- Because we have two unknowns, we loop through a number of epochs, and alternative between updating U and W\n",
    "        for epoch in range(epochs):\n",
    "            Update W\n",
    "            Update U\n",
    "- NOTE the proof is outside the scope of this lesson, but there is indeed a mathematical proof that guaruntees the cost will decrease after every update. Eventually you will fall into a **local BUT NOT global minimum**.             \n",
    "- This is NOT gradient descent (we solved for W and U). We call this \"alternating least squares\" - we find the best W given the current U, then find the best U given the current W.\n",
    "- This is mroe like the expectation algorithm than it is like GS, in the sense that we update one paramter to get the objective, while holding the other param constant.\n",
    "- This is similar to K-means clustering, Guassian mixtures, Hidden markov Models\n",
    "- NOTE: You CAN use gradient descent (which we will do later)... but it usually take more steps to settle into a minimum that ALS. \n",
    "\n",
    "### Adding Bias Terms\n",
    "Now that we have the basics down, we want to add to the model to improve it. A good first option is bias terms, as per usual:\n",
    "\n",
    "Bias terms:\n",
    "- A(i,j) = W(i)<sup>T</sup>U(j) + b(i) + c(j) + μ\n",
    "    - b(i): user bias (eg. if user i rates all movies high)\n",
    "    - c(j): movies bias (eg. if everyone like Avatar)\n",
    "\n",
    "- In matrix factorization, we usually have 2 or 3 (instead of just 1)\n",
    "- There is a single scalar bias for each user, and a single scalar bias for each movie, so they will be of size n and m respectively\n",
    "- You can think of a bias as an actual bias! If there is a dude who likes to troll, for instance, he will have a negative bias!\n",
    "- Mu (μ) is the third bias here, which the global mean of the A matrix, which effectively centres the matrix around 0.\n",
    "- Below are the updates wite the bias terms:\n",
    "![](pictures/NLP_11_matfactadd.png)\n",
    "\n",
    "\n",
    "### Regularization\n",
    "- We have one left thing to add, regularization!\n",
    "- We add this so that our cost doesnt explode to inifinity and it prevents us from OVERFITTING\n",
    "- As usual, **its simply adding the frobenius norms (or euclidean norm) to the cost funciton with a hyperparameter λ to adjust its influence):**\n",
    "![](pictures/NLP_11_mactfactreg.png)\n",
    "- Solve for the derivatives by setting them to zero and find the parameter updates in closed form:\n",
    "![](pictures/NLP_11_mactfactreg2.png)\n",
    "\n",
    "### Summary\n",
    "- We can create recommender systems by creating huge matrices with users along rows and movies along columns\n",
    "- Different from usual ML problems because we are NOT using features, just existing ratings... We use existing rating to predict new ratings\n",
    "- Model the big matrix as the product of 2 mall matrices\n",
    "- Training: gradient descent or alternating least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GLoVe\n",
    "### Overview of Glove\n",
    "- Glove vectotrs for word representation is an alternative method to word2vec\n",
    "- Its just some initial work up front, then its exactly like matrix factorization\n",
    "- Similar to w2v, we are trying to incorporate context\n",
    "- Will not build a term-document matrix like we did for w2v, but instead we will build a term-term matrix. \n",
    "- X(i,j) = higher if word(i) appears in context of word(j) oftern (and vice versa)\n",
    "- With glove, its the details of how we build matrix X that are important, the rest is just simple matrix mactorization\n",
    "\n",
    "### Building matrix X with \"Context Distance\"\n",
    "- The main idea is that words get scored based on their distance to a target word. So in the sentence \"I love dogs and cats\", if word(i) = \"I\", then \"love\" gets a +1, dogs gets a +0.5, etc.\n",
    "- Example: \"I love dogs and cats\": \n",
    "    - X(I, love) += 1\n",
    "    - X(I, dogs) += 1/2\n",
    "    - X(I, and) += 1/3\n",
    "    - Et... up to a certian context size\n",
    "- Remember that words frequencies have long tail distribution, so that means that we are going to have many zeros (this will be a very sparse matrix)\n",
    "- Where we do have non-zero values, those values will be VERY large (even in the millions), depending on how many documents u have in corpus... \n",
    "- A common technique for scaling things down when they grow exponentially large is to take the log of those values, so thats what we do! \n",
    "    - **We can't take the log of 0, so we will add a 1 to every entry**\n",
    "- log X(i,j) will be the target matrix!!!\n",
    "\n",
    "### Cost function\n",
    "We could construct a cost funciton from what we have so far, but we wont just yet... **Instead we will weight every (i,j)th entry by its X value.**\n",
    "Reasearches have found this works well:*if its X value is sufficiently large, we will give it a weight of 1, if its X value is small we will give it a weight of less than 1.* The function used in the original paper is below:\n",
    "- f(x) = (X/Xmax)<sup>α</sup> if X < Xmax, else 1\n",
    "- α = 0.75\n",
    "- Xmax = 100\n",
    "- J = ΣiΣj f(Xij) (W(i)<sup>t</sup>U(j) - logXij)<sup>2</sup> \n",
    "- i=1...N, j=1...M? YES! **Because we dont have any missing values in this matrix. **\n",
    "\n",
    "![](pictures/NLP_11_gloveJ.png)\n",
    "\n",
    "### Solving the equation\n",
    "- logXij ~= Wi<sup>T</sup>Uj + (b)i + (c)j + μ\n",
    "- The original paper uses just 2 bias terms and gradient descent\n",
    "- We will use 3 bias terms and Alternating Least squares (as well as GS)\n",
    "- NOTE: Training time doesnt depend on the amount of data, you are always fitting to a VxV matrix (only the creation of X will take longer)\n",
    "    - These derivatives are actually easier to solve for because you don't have to consider the set of values you have to consider under the summation (there are no missing values, unlike in our recomennder system example)\n",
    "- Here are the gradients:\n",
    "![](pictures/NLP_11_glovegrad.png)\n",
    "![](pictures/NLP_11_glovegrad2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lets write some Code!\n",
    "- First we will write Glove using Gradient descent\n",
    "- Then we will add in Alternating Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import custom get_setnces: make sure custom script is in same folder as j notebook\n",
    "from nltk.corpus import brown\n",
    "from MC_NLP_util import get_sentences_with_word2idx_limit_vocab\n",
    "from MC_NLP_util import find_analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Glove Class\n",
    "- define 'fit' method\n",
    "    - build in options for Gradient Descent or Alternating Least Squares\n",
    "    - for GS option, build-in option to use TensorFlow or numpy\n",
    "- define 'save' method so save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create Glove class\n",
    "class Glove:\n",
    "    def __init__(self, D, V, context_size):\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.context_size = context_size\n",
    "    \n",
    "    def fit(self, sentences, cc_matrix_filepath=\"MC_cc_matrix.npy\", learning_rate=1e-4,\n",
    "            reg=0.1, ccmax=100, alpha=0.75, epochs=10, \n",
    "            gd=False, use_tensorflow=False, momentum=0.9):\n",
    "        \n",
    "        # INITIALIZE VARIABLES\n",
    "        t0 = datetime.now()\n",
    "        V = self.V\n",
    "        D = self.D\n",
    "        N_sents = len(sentences)\n",
    "        \n",
    "        # PRELOD CO-OCCURRENCE MATRIX IF EXISTS\n",
    "        if os.path.exists(cc_matrix_filepath):\n",
    "            print(\"Loading pretrained co-occurrence matrix: \", cc_matrix_filepath)\n",
    "            cc_matrix = np.load(cc_matrix_filepath)\n",
    "        # CREATE NEW CO-OCCURRENCE MATRIX\n",
    "        else:            \n",
    "            cc_matrix = np.zeros((V,V))\n",
    "            print(\"Creating co-occurrence matrix\\nNumber of sentences to process: \", N_sents)  \n",
    "            # LOOP THROUGH EACH SENTENCE\n",
    "            iteration = 0\n",
    "            for sentence in sentences:\n",
    "                iteration += 1\n",
    "                if iteration % 10000 == 0:\n",
    "                    print(\"Processed {} of {} sentences\".format(iteration,N_sents))\n",
    "                # LOOP THROUGH EACH WORD IN SENTENCE\n",
    "                len_sent = len(sentence)\n",
    "                for word_idx in range(len_sent):\n",
    "                    # NOTE: word_idx is NOT the matrix word index\n",
    "                    # ...it is only the within-sentence index\n",
    "                    # ie. it points to which element of the sequence (sentence) we're looking at\n",
    "                    # wi is target word\n",
    "                    wi = sentence[word_idx]\n",
    "                    # EXTRACT CONTEXT\n",
    "                    start = max(0, word_idx - self.context_size)\n",
    "                    end = min(len_sent, word_idx + self.context_size)\n",
    "                    # NOTE: We can choose either 1 side of context or both\n",
    "                    # Here we will use both side of the context\n",
    "                    # MAKE SURE START AND END TOKENS ARE PART OF SOME CONTEXT\n",
    "                    # otherwise their f(x) will be 0 (denominator in bias update)\n",
    "                    if word_idx - self.context_size < 0:\n",
    "                        # Start tokens get assigned points because they are in the context\n",
    "                        # word_idx+1 is min (context size is truncated)\n",
    "                        points = 1.0 / (word_idx + 1)\n",
    "                        cc_matrix[wi,0] += points\n",
    "                        cc_matrix[0,wi] += points\n",
    "                    if word_idx + self.context_size > len_sent:\n",
    "                        points = 1.0 / (len_sent - word_idx)\n",
    "                        # End tokens get assigned points because they are in context \n",
    "                        # len_sent-word_idx is context length (context size is truncated) \n",
    "                        cc_matrix[wi,1] += points\n",
    "                        cc_matrix[1,wi] += points \n",
    "                    # UPDATE LEFT SIDE OF TARGET WORD\n",
    "                    for j in range(start, word_idx): # exclusive, as we want to exclude target word_idx\n",
    "                        wj = sentence[j]\n",
    "                        # Points increase as j gets closer to word index\n",
    "                        # Ie. words right next to target are scored hire\n",
    "                        points = 1.0/(word_idx-j)\n",
    "                        cc_matrix[wi,wj] += points\n",
    "                        cc_matrix[wj,wi] += points\n",
    "                    # UPDATE RIGHT SIDE OF TARGET WORD\n",
    "                    for j in range (word_idx+1, end): \n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0/(j-word_idx)\n",
    "                        #cc_matrix[wi,wj] += points\n",
    "                        #cc_matrix[wj,wi] += points\n",
    "            np.save(cc_matrix_filepath, cc_matrix)\n",
    "        \n",
    "        # PRINT OUT MAXIMUM VALUE\n",
    "        print(\"Max in cc_matrix:\", cc_matrix.max())\n",
    "        \n",
    "        # EXTRACT AND APPLY WEIGHTING\n",
    "        fX = np.zeros((V,V))\n",
    "        # If less than ccmax threshold (100), new_val = (orig_val/ccmax)^alpha\n",
    "        fX[cc_matrix < ccmax] = (cc_matrix[cc_matrix < ccmax] / float(ccmax))**alpha\n",
    "        # If greater than ccmax, new_val = 1\n",
    "        fX[cc_matrix >= ccmax] = 1\n",
    "        print(\"Max in f(cc_matrix):\", fX.max())\n",
    "        \n",
    "        # EXTRACT TARGET (log of cc_matrix, add 1 so no zeros)\n",
    "        logCC = np.log(cc_matrix +1)\n",
    "        print(\"Max in log(cc_matrix): \", logCC.max())\n",
    "        print(\"Time to build co-occurrence matrix:\", datetime.now() - t0)\n",
    "        \n",
    "        # INITIALIZE WEIGHTS\n",
    "        W = np.random.rand(V, D) / np.sqrt(V + D)\n",
    "        U = np.random.rand(V, D) / np.sqrt(V + D)\n",
    "        b = np.zeros(V)\n",
    "        c = np.zeros(V)\n",
    "        mu = logCC.mean()\n",
    "        \n",
    "        # CREATE TENSORFLOW GRAPH\n",
    "        if use_tensorflow:\n",
    "            # Create tf variables with np.arrays\n",
    "            tf_W = tf.Variable(W.astype(np.float32))\n",
    "            tf_U = tf.Variable(U.astype(np.float32))\n",
    "            tf_b = tf.Variable(b.reshape(V, 1).astype(np.float32))\n",
    "            tf_c = tf.Variable(b.reshape(1, V).astype(np.float32))\n",
    "            tf_fX = tf.placeholder(tf.float32, shape=(V,V))\n",
    "            tf_logCC = tf.placeholder(tf.float32, shape=(V,V))\n",
    "            \n",
    "            # Calcuate cost\n",
    "            delta = (tf.matmul(tf_W, tf.transpose(tf_U))+ tf_b + tf_c + mu) - tf_logCC\n",
    "            cost = tf.reduce_sum(tf_fX * delta*delta)\n",
    "            \n",
    "            # Add regularization to cost\n",
    "            for param in (tf_W, tf_b, tf_U, tf_c ):\n",
    "                cost += reg*tf.reduce_sum(param*param)\n",
    "            \n",
    "            # Set training op (Use the momentum optimizer)\n",
    "            train_op = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum).minimize(cost)\n",
    "            \n",
    "            # Initialize variables\n",
    "            init = tf.global_variables_initializer()\n",
    "            session = tf.InteractiveSession()\n",
    "            session.run(init)\n",
    "        \n",
    "        # RUN MODEL AND PRINT COST\n",
    "        costs = []\n",
    "        sentences_idxs = range(N_sents)\n",
    "        for epoch in range(epochs):\n",
    "            te0 = datetime.now()\n",
    "            delta = W.dot(U.T) + b.reshape(V, 1) + c.reshape(1, V) + mu - logCC\n",
    "            cost = (fX * delta *  delta).sum()\n",
    "            costs.append(cost)\n",
    "            print(\"Epoch:\",epoch,\"Cost:\",cost)\n",
    "            \n",
    "            # RUN MODEL MODEL WITH GRADIENT DESCENT\n",
    "            if gd:\n",
    "                # Train model with tensorflow\n",
    "                if use_tensorflow:\n",
    "                    session.run(train_op, feed_dict={tf_logCC: logCC, tf_fX: fX})\n",
    "                    # Update weights to be fed into cost function\n",
    "                    W,U,b,c = session.run([tf_W,tf_U,tf_b,tf_c])\n",
    "                # Train model with numpy\n",
    "                else:\n",
    "                    oldW = W.copy()\n",
    "                    # UPDATE W\n",
    "                    for i in range(V):\n",
    "                        # FULL CODE FOR W UPDATE (NON VECTORIZED)\n",
    "                        # for j in range(V)\n",
    "                        #     W[i] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logCC[i,j])*U[j]\n",
    "                        W[i] -= learning_rate*(fX[i,:]*delta[i,:]).dot(U)\n",
    "                    W -= learning_rate*reg*W\n",
    "                    \n",
    "                    # UPDATE b\n",
    "                    for i in range(V):\n",
    "                        # FULL CODE FOR b UPDATE (NON VECTORIZED)\n",
    "                        # for j in range(V)\n",
    "                        #     b[i] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logCC[i,j])\n",
    "                        b[i] -= learning_rate*fX[i,:].dot(delta[i,:])\n",
    "                    b -= learning_rate*reg*b\n",
    "                    \n",
    "                    # UPDATE U\n",
    "                    for j in range(V):\n",
    "                        # FULL CODE FOR U UPDATE (NON VECTORIZED)\n",
    "                        # for i in range(V)\n",
    "                        #     U[j] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logCC[i,j])*W[j]\n",
    "                        U[j] -= learning_rate*(fX[:,j]*delta[:,j]).dot(oldW)\n",
    "                    U -= learning_rate*reg*U\n",
    "                    \n",
    "                    # UPDATE c\n",
    "                    for j in range(V):\n",
    "                        # FULL CODE FOR c UPDATE (NON VECTORIZED)\n",
    "                        # for i in range(V)\n",
    "                        #     c[i] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logCC[i,j])\n",
    "                        c[i] -= learning_rate*fX[:,j].dot(delta[:,j])\n",
    "                    c -= learning_rate*reg*c\n",
    "            else:\n",
    "                # RUN ALTERNATING LEAST SQUARES (ALS)\n",
    "                for i in range(V):\n",
    "                    # matrix = reg*np.eye(D) + np.sum((fX[i,j]*np.outer(U[j], U[j]) for j in range(V)), axis=0)\n",
    "                    matrix = reg*np.eye(D) + (fX[i,:]*U.T).dot(U)\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 1e-5)\n",
    "                    vector = (fX[i,:]*(logCC[i,:] - b[i] - c - mu)).dot(U)\n",
    "                    #SOLVE TO GET UPDATED WEIGHTS\n",
    "                    W[i] = np.linalg.solve(matrix, vector)\n",
    "                # print \"fast way took:\", (datetime.now() - t0)\n",
    "\n",
    "                # slow way\n",
    "                # t0 = datetime.now()\n",
    "                # for i in range(V):\n",
    "                #     matrix2 = reg*np.eye(D)\n",
    "                #     vector2 = 0\n",
    "                #     for j in range(V):\n",
    "                #         matrix2 += fX[i,j]*np.outer(U[j], U[j])\n",
    "                #         vector2 += fX[i,j]*(logX[i,j] - b[i] - c[j])*U[j]\n",
    "                # print \"slow way took:\", (datetime.now() - t0)\n",
    "\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 1e-5)\n",
    "                    # assert(np.abs(vector - vector2).sum() < 1e-5)\n",
    "                    # W[i] = np.linalg.solve(matrix, vector)\n",
    "                # print \"updated W\"\n",
    "\n",
    "                # update b\n",
    "                for i in range(V):\n",
    "                    denominator = fX[i,:].sum()\n",
    "                    # assert(denominator > 0)\n",
    "                    numerator = fX[i,:].dot(logCC[i,:] - W[i].dot(U.T) - c - mu)\n",
    "                    # for j in range(V):\n",
    "                    #     numerator += fX[i,j]*(logX[i,j] - W[i].dot(U[j]) - c[j])\n",
    "                    b[i] = numerator / denominator / (1 + reg)\n",
    "                # print \"updated b\"\n",
    "\n",
    "                # update U\n",
    "                for j in range(V):\n",
    "                    # matrix = reg*np.eye(D) + np.sum((fX[i,j]*np.outer(W[i], W[i]) for i in range(V)), axis=0)\n",
    "                    matrix = reg*np.eye(D) + (fX[:,j]*W.T).dot(W)\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 1e-8)\n",
    "                    vector = (fX[:,j]*(logCC[:,j] - b - c[j] - mu)).dot(W)\n",
    "                    # matrix = reg*np.eye(D)\n",
    "                    # vector = 0\n",
    "                    # for i in range(V):\n",
    "                    #     matrix += fX[i,j]*np.outer(W[i], W[i])\n",
    "                    #     vector += fX[i,j]*(logX[i,j] - b[i] - c[j])*W[i]\n",
    "                    #SOLVE TO GET UPDATED WEIGHTS\n",
    "                    U[j] = np.linalg.solve(matrix, vector)\n",
    "                # print \"updated U\"\n",
    "\n",
    "                # update c\n",
    "                for j in range(V):\n",
    "                    denominator = fX[:,j].sum()\n",
    "                    numerator = fX[:,j].dot(logCC[:,j] - W.dot(U[j]) - b  - mu)\n",
    "                    # for i in range(V):\n",
    "                    #     numerator += fX[i,j]*(logX[i,j] - W[i].dot(U[j]) - b[i])\n",
    "                    c[j] = numerator / denominator / (1 + reg)\n",
    "                # print \"updated c\"\n",
    "            print(\"Epoch {} complete. Total runtime: {}\".format(epoch, datetime.now()-te0))\n",
    "        \n",
    "        # STORE FINAL WEIGHTS\n",
    "        self.W = W\n",
    "        self.U = U\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "        \n",
    "    def save(self, filename):\n",
    "        # word_analogies function expects a (V,D) matrix and a (D,V) matrix\n",
    "        arrays = [self.W, self.U.T]\n",
    "        np.savez(filename, *arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get Data & Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:  9944\n",
      "Vocab:  21214\n",
      "Creating co-occurrence matrix\n",
      "Number of sentences to process:  9944\n",
      "Max in cc_matrix: 8614.0\n",
      "Max in f(cc_matrix): 1.0\n",
      "Max in log(cc_matrix):  9.06126014896\n",
      "Time to build co-occurrence matrix: 0:04:39.190488\n"
     ]
    }
   ],
   "source": [
    "# SET FILENAME VARIABLES FOR REUSING MODEL LATER (without re-training)\n",
    "w2i_file = \"glove_mod_w2i.json\"\n",
    "ccmat_file = \"glove_mod_ccmat.npy\"\n",
    "mod_file = \"glove_mod_ccmat.npy\"\n",
    "\n",
    "# EXTRACT DATA\n",
    "keep_these = set(['king', 'man', 'woman',\n",
    "    'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england',\n",
    "    'french', 'english', 'japan', 'japanese', 'chinese', 'italian',\n",
    "    'december', 'november', 'june',\n",
    "    'january', 'february', 'march', 'april', 'may', 'july', 'august',\n",
    "    'september', 'october'])\n",
    "sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=1000000, n_sent=10000, keep_words=keep_these)\n",
    "V = len(word2idx)\n",
    "\n",
    "# SAVE W2I\n",
    "with open(w2i_file, 'w') as f:\n",
    "    json.dump(word2idx, f) \n",
    "          \n",
    "# RUN MODEL\n",
    "glove_mod = Glove(100, V, 10)\n",
    "glove_mod.fit(sentences, cc_matrix_filepath=ccmat_file, gd=False, use_tensorflow=False, epochs=20)\n",
    "\n",
    "# SAVE MODEL WEIGHTS\n",
    "glove_mod.save(mod_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Test Word Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape: (14457, 100)\n",
      "W2.T shape: (14457, 100)\n",
      "Word embeddings shape: (14457, 100)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word2idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d3b0a59814b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mW_embs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Word embeddings shape:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_embs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Word2idx shape:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word2idx' is not defined"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "npz = np.load(mod_file)\n",
    "W1 = npz['arr_0']\n",
    "W2 = npz['arr_1']\n",
    "with open(w2i_file) as f:\n",
    "    word2idx = json.load(f)\n",
    "print(\"W1 shape:\",W1.shape)\n",
    "print(\"W2.T shape:\",W2.T.shape)\n",
    "\n",
    "# CONCAT WORD VECTORS\n",
    "W_embs = (W1+W2.T)/2\n",
    "print(\"Word embeddings shape:\", W_embs.shape)\n",
    "print(\"Word2idx shape:\", len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 14457 is out of bounds for axis 0 with size 14457",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8af0c8d968ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Find word analogies!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfind_analogies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'king'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'man'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'woman'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW_embs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfind_analogies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'queen'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'woman'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'man'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW_embs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Python\\NLP\\LazyProgrammer\\MC_NLP_util.py\u001b[0m in \u001b[0;36mfind_analogies\u001b[1;34m(w1, w2, w3, We, word2idx)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                 \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 14457 is out of bounds for axis 0 with size 14457"
     ]
    }
   ],
   "source": [
    "# Find word analogies!\n",
    "find_analogies('king','man','woman',W_embs,word2idx)\n",
    "find_analogies('queen','woman','man',W_embs,word2idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
