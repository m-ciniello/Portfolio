{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Part 1. FuzzyFinder in 10 lines of code\n",
    "\n",
    "#### Stolen from 'Brain Spill' - https://blog.amjith.com/fuzzyfinder-in-10-lines-of-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro:\n",
    "FuzzyFinder is a popular feature available in decent editors to open files. The idea is to start typing partial strings from the full path and the list of suggestions will be narrowed down to match the desired file. \n",
    "\n",
    "### Problem Statement:\n",
    "We have a collection of strings (filenames). We're trying to filter down that collection based on user input. The user input can be partial strings from the filename. Let's walk this through with an example. Here is a collection of filenames:\n",
    "\n",
    "    collection = ['django_migrations.py',\n",
    "                'django_admin_log.py',\n",
    "                'main_generator.py',\n",
    "                'migrations.py',\n",
    "                'api_user.doc',\n",
    "                'user_group.doc',\n",
    "                'accounts.txt',\n",
    "                ]\n",
    "                \n",
    "When the user types 'djm' we are supposed to match 'django_migrations.py' and 'django_admin_log.py'. The simplest route to achieve this is to use regular expressions. \n",
    "\n",
    "### Solutions:\n",
    "#### Naive Regex Matching:\n",
    "Convert 'djm' into 'd.\\*j.\\*m' and try to match this regex against every item in the list. Items that match are the possible candidates.\n",
    "\n",
    "NOTE: We use regex.search instead of regex.match, as the match function will only return true if there is a match from the BEGINNING of the string. Here are the main attributes/methods of match/search functions:\n",
    "\n",
    "- group():\tReturn the string matched by the RE\n",
    "- start()\tReturn the starting position of the match\n",
    "- end():\tReturn the ending position of the match\n",
    "- span():\tReturn a tuple containing the (start, end) positions of the match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['django_migrations.py',\n",
       " 'django_admin_log.py',\n",
       " 'main_generator.py',\n",
       " 'migrations.py']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "collection = ['django_migrations.py',\n",
    "            'django_admin_log.py',\n",
    "            'main_generator.py',\n",
    "            'migrations.py',\n",
    "            'api_user.doc',\n",
    "            'user_group.doc',\n",
    "            'accounts.txt',\n",
    "            ]\n",
    "\n",
    "def FuzzyFinder(search_term, collection):\n",
    "    \"\"\"This function essentially returns the strings that have the specified characters in a sequential order\"\"\"\n",
    "    suggestions = []\n",
    "    pattern = '.*'.join(search_term)\n",
    "    compiled_re = re.compile(pattern)\n",
    "    for item in collection:\n",
    "        match = compiled_re.search(item)\n",
    "        if match:\n",
    "               suggestions.append(item)\n",
    "    return suggestions\n",
    "    \n",
    "FuzzyFinder('mig',collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking based on match position:\n",
    "We can rank the results based on the position of the **first occurrence of the matching character**. We make the list of suggestions to be tuples where the first item is the position of the match and second item is the matching filename. Then we use a list comprehension to iterate over the sorted list of tuples and extract just the second item which is the file name we're interested in.\n",
    "\n",
    "NOTE: When this list is sorted used the 'sorted' funciton, python will sort them based on the first item in tuple and use the second item as a tie breaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'TEST'),\n",
       " (3, 'TEST'),\n",
       " (5, '1TEST'),\n",
       " (5, 'aTEST'),\n",
       " (5, 'bTEST'),\n",
       " (7, 'TEST')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting tuples example\n",
    "sorted([(5,'bTEST'),(5,'aTEST'),(5,'1TEST'),(7,'TEST'),(3,'TEST'),(1,'TEST')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['main_generator.py',\n",
       " 'migrations.py',\n",
       " 'django_migrations.py',\n",
       " 'django_admin_log.py']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def FuzzyFinder(search_term, collection):\n",
    "    \"\"\"This function essentially returns the strings in a list that have the specified characters in a sequential order.\n",
    "    It ranks the strings based on the position of the first occurence of the matching character\"\"\"\n",
    "    suggestions = []\n",
    "    pattern = '.*'.join(search_term) # Converts 'djm' to 'd.*j.*m'\n",
    "    compiled_re = re.compile(pattern) # Compiles a regex.\n",
    "    for item in collection:\n",
    "        match = compiled_re.search(item) # Checks if the current item matches the regex.\n",
    "        if match:\n",
    "               suggestions.append((match.start(),item))\n",
    "    return [x for _,x in sorted(suggestions)]\n",
    "    \n",
    "FuzzyFinder('mig',collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking based on compact match:\n",
    "\n",
    "The last example got us close to the end result, but as shown its not perfect. Our search for 'mig' included 'main_generator' as the first suggestion, when we should have got migrations.py... this is because it was ranking them best on the position of the first characted!!! \n",
    "\n",
    "Using regex.match() might solve this, but there is another way, compact match! When a user started typing a partial string they will conitnue to type consecutive letters in a effort to find the exact match. When some types 'mig' they are looking for 'migrations' or 'django_migrations', NOT main_generator.py. **The key here is to find the most COMPACT MATCH for the user input.**\n",
    "\n",
    "This is pretty easily implemented in pyhton:\n",
    "- When we match a string against a reg expressoin, the matched string is stored in the match.group(). \n",
    "- We use the length of the caputre group as our **PRIMARY RANK** and use the starting position as our secondary rank. \n",
    "    - to do this we add len(match.group()) as the firest item in the tuple, match.start() as the second, and the filename as the third\n",
    "    - python will sort the list first by the primary key, then by the secondary, and then by the third key (the filename) (the tie breaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['migrations.py',\n",
       " 'django_migrations.py',\n",
       " 'main_generator.py',\n",
       " 'django_admin_log.py']"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = ['django_migrations.py',\n",
    "            'django_admin_log.py',\n",
    "            'main_generator.py',\n",
    "            'migrations.py',\n",
    "            'api_user.doc',\n",
    "            'user_group.doc',\n",
    "            'accounts.txt',\n",
    "            ]\n",
    "\n",
    "def FuzzyFinder(search_term, collection):\n",
    "    \"\"\"This function essentially returns the strings in a list that have the specified characters in a sequential order.\n",
    "    It ranks the strings based on the compact match lengths and position of the first occurence of the matching character\"\"\"\n",
    "    suggestions = []\n",
    "    pattern = '.*'.join(search_term) # Converts 'djm' to 'd.*j.*m'\n",
    "    compiled_re = re.compile(pattern) # Compiles a regex.\n",
    "    for item in collection:\n",
    "        match = compiled_re.search(item) # Checks if the current item matches the regex.\n",
    "        if match:\n",
    "               suggestions.append((len(match.group()),match.start(), item))\n",
    "    return [x for _,_,x in sorted(suggestions)]\n",
    "    \n",
    "FuzzyFinder('mig',collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Greedy Match\n",
    "\n",
    "Not bad so far!!! One more subtle adjustment we need to make:\n",
    "\n",
    "Consider these two items in the collection: \n",
    "        \n",
    "        ['api_user', 'user_group']. \n",
    "\n",
    "When you enter 'user' the ideal suggestion list would be ['user_group','api_user'], but heres what we actually get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['api_user.doc', 'user_group.doc']"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FuzzyFinder('user', collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list is inversed!!! hmmm... Digging into this more, we notice that 'api_user' actually contains two 'r' characters... so what u.\\*s.\\*e.\\*r actually matches is **user_gr** instead of just 'user', which is why it comes second in our suggestion list. \n",
    "\n",
    "Fortunately, there is an easy fix for this. **We use the NON-GREEDY version of the regex notation (.\\*? instead of .\\*)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_group.doc', 'api_user.doc']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = ['django_migrations.py',\n",
    "            'django_admin_log.py',\n",
    "            'main_generator.py',\n",
    "            'migrations.py',\n",
    "            'api_user.doc',\n",
    "            'user_group.doc',\n",
    "            'accounts.txt',\n",
    "            ]\n",
    "\n",
    "def FuzzyFinder(search_term, collection):\n",
    "    \"\"\"This function essentially returns the strings in a list that have the specified characters in a sequential order.\n",
    "    It ranks the strings based on the compact match lengths and position of the first occurence of the matching character\"\"\"\n",
    "    suggestions = []\n",
    "    pattern = '.*?'.join(search_term) # Converts 'djm' to 'd.*j.*m'\n",
    "    compiled_re = re.compile(pattern) # Compiles a regex.\n",
    "    for item in collection:\n",
    "        match = compiled_re.search(item) # Checks if the current item matches the regex.\n",
    "        if match:\n",
    "               suggestions.append((len(match.group()),match.start(), item))\n",
    "    return [x for _,_,x in sorted(suggestions)]\n",
    "    \n",
    "FuzzyFinder('user',collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidenote on the Greedy vs Lazy Quantifiers\n",
    "http://www.rexegg.com/regex-quantifiers.html\n",
    "\n",
    "#### The Greedy Trap\n",
    "The classic trap with greedy quantifiers is that they may match more than you expect. Suppose you want to match tokens that begin with {START} and end with {END}. You may try this pattern:\n",
    "\n",
    "    {START}.*{END}\n",
    "\n",
    "However, you will find that this pattern matches this entire string from start to finish:\n",
    "\n",
    "    {START} Mary {END} had a {START} little lamb {END} \n",
    "\n",
    "…whereas we wanted to find two matches:\n",
    "    \n",
    "    {START} Mary {END}\n",
    "    {START} little lamb {END}\n",
    "    \n",
    "#### Lazy Quantifier Solution\n",
    "The easiest way is to make the dot-star lazy by adding a ? question mark:\n",
    "    \n",
    "    {START}.*?{END}\n",
    "\n",
    "The lazy .\\*? quantifier guarantees that **the quantified dot only matches as many characters as needed for the rest of the pattern to succeed.** Therefore, the pattern only matches one {START}…{END} item at a time, which is what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{START} Mary {END} had a {START} little lamb {END}\n",
      "['{START} Mary {END} had a {START} little lamb {END}']\n"
     ]
    }
   ],
   "source": [
    "#greedy search\n",
    "string = \"{START} Mary {END} had a {START} little lamb {END} \"\n",
    "pattern = r\"{START}.*{END}\"\n",
    "print(re.search(pattern,string).group())\n",
    "print(re.findall(pattern,string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{START} Mary {END}\n",
      "['{START} Mary {END}', '{START} little lamb {END}']\n"
     ]
    }
   ],
   "source": [
    "#lazy quantifier search\n",
    "string = \"{START} Mary {END} had a {START} little lamb {END} \"\n",
    "pattern = \"{START}.*?{END}\"\n",
    "print(re.search(pattern,string).group())\n",
    "print(re.findall(pattern,string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lazy Quantifiers are Expensive\n",
    "It's important to understand how the lazy .\\*? works in this example because there is a cost to using lazy quantifiers. \n",
    "\n",
    "When it first encounters .\\*? the engine starts out by matching the minimum number of characters allowed by the quantifier—which is zero. The engine then advances in the pattern and tries the next token (which is {) against the M in Mary. This fails, so the engine backtracks and allows the .\\*? to expand its match by one item, so that it matches the M. Once again, the engine advances in the pattern. It now tries the { against the a in Mary. This fails, so the engine backtracks and allows the .\\*? to expand and match the a. The process then repeats itself—the engine advances, fails, backtracks, allows the lazy .\\*? to expand its match by one item, advances, fails and so on. \n",
    "\n",
    "As you can see, for each character matched by the .*\\?, the engine has to backtrack. From a computing standpoint, this process of matching one item, advancing, failing, backtracking, expanding is \"expensive\". \n",
    "\n",
    "On a modern processor, for simple patterns, this will likely not matter. But if you want to craft efficient regular expressions, you must pay attention to use lazy quantifiers only when they are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Part 2. Cleansing text\n",
    "#### http://blog.keyrus.co.uk/fuzzy_matching_101_part_i.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stripping whitespace and unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "hello world    \n",
      " \n",
      "   hello world\n"
     ]
    }
   ],
   "source": [
    "# Whitespace stripping\n",
    "s = '   hello world    \\n '\n",
    "print(s.strip()) #strips whitespace on outside of string\n",
    "print(s.lstrip()) #strips whitespace on leftside of string\n",
    "print(s.rstrip()) #strips whitespace on rightside of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell-o=====\n",
      "-----hell-o=====\n",
      "hell-o\n"
     ]
    }
   ],
   "source": [
    "# Character stripping\n",
    "t = '-----hell-o====='\n",
    "print(t.lstrip('-')) #stripped specfied characters from the left\n",
    "print(t.lstrip('=')) #this will do nothing\n",
    "print(t.strip('-=')) #strip al characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# lower casing\n",
    "t = 'HELLO World'\n",
    "print(t.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing unwanted characters with maketrans()\n",
    "\n",
    "The maketrans() method takes 3 parameters:\n",
    "\n",
    "- x - If only one argument is supplied, it must be a dictionary.\n",
    "The dictionary should contain 1-to-1 mapping from a single character string to its translation OR a unicode number (97 for 'a') to its translation.\n",
    "- y - If two arguments are passed, it must be two strings with equal length.\n",
    "Each character in the first string is a replacement to its corresponding index in the second string.\n",
    "- z - If three arguments are passed, each character in the third argument is mapped to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{97: 49, 98: 50, 99: 51, 100: 52, 101: 53, 102: 54, 103: 55}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create unicode translation dictionary\n",
    "str.maketrans('abcdefg','1234567')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Th3s 3s M3k2s 1w2s4m2 str3ng'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# striping non-relevant punctuation\n",
    "s = 'This is Mikes awesome string'\n",
    "translation = str.maketrans('aeiou','12345')\n",
    "s.translate(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string with alot of random punctuation'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use translation to remove punctuation (replace all punctuation with none... use three args)\n",
    "s = \"This is' a s&tring^ with *alot( of >random !@#punctu*)(@ation)\"\n",
    "translation = str.maketrans(\"\",\"\",\"!@@#$%>^'&*()\")\n",
    "s.translate(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the str.replace() function\n",
    "\n",
    "The method replace() returns a copy of the string in which the occurrences of old have been replaced with new, optionally restricting the number of replacements to max. The replace method has 3 parameters:\n",
    "\n",
    "- old − This is old substring to be replaced.\n",
    "- new − This is new substring, which would replace old substring.\n",
    "- max − If this optional argument max is given, only the first count occurrences are replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HaileyHaileyHaileyHaileyHailey\n",
      "HaileyHaileyHaileyMikeMike\n"
     ]
    }
   ],
   "source": [
    "string = \"MikeMikeMikeMikeMike\"\n",
    "print(string.replace(\"Mike\", \"Hailey\"))\n",
    "print(string.replace(\"Mike\", \"Hailey\", 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Part 3. fuzzywuzzy\n",
    "\n",
    "Fuzzy String Matching, also called Approximate String Matching, is the process of finding strings that approximatively match a given pattern.\n",
    "The closeness of a match is often measured in terms of edit distance, which is the number of primitive operations necessary to convert the string into an exact match.\n",
    "\n",
    "Primitive operations are usually: \n",
    "- insertion (to insert a new character at a given position), \n",
    "- deletion (to delete a particular character) and \n",
    "- substitution (to replace a character with a new one).\n",
    "\n",
    "#### Fuzzy Wuzzy provides 4 types of fuzzy logic based matching, using LEVENSHTEIN DISTANCE to determine the similarity between two strings. This metric mathematically determines similarity by looking at the minimum number of edits required for two strings to converge / be equal.\n",
    "\n",
    "To quickly summarise the matching methods offered, there is:\n",
    "\n",
    "- Simple Ratio - Pure Levenshtein Distance based matching\n",
    "- Partial Ratio - Matches based on best substrings\n",
    "- Token Sort Ratio - Tokenises strings and sorts them alphabetically before matching\n",
    "- Token Set Ratio - Tokenise and compare intersection and remainder\n",
    "\n",
    "See this post for more info: http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "\n",
    "“Fuzzywuzzy” depends only on the **difflib python library**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9629629629629629\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "s1 = \"New York Mets\"\n",
    "s2 = \"New York Meats\"\n",
    "\n",
    "def ratio(s1,s2):\n",
    "    m = SequenceMatcher(None, s1, s2)\n",
    "    return(m.ratio())\n",
    "print(ratio(s1,s2))\n",
    "\n",
    "#fuzzy match equivalent\n",
    "print(fuzz.ratio( \"New York Mets\", \"New York Meats\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we’re done! Not quite. It turns out that the standard “string closeness” measurement **works fine for very short strings (such as a single word) and very long strings (such as a full book), but not so much for 3-10 word labels.** The naive approach is far too sensitive to minor differences in word order, missing or extra words, and other such issues.\n",
    "\n",
    "For example, in the example below, the first two strings are clearly referring to the same team, but the second two are clearly referring to different ones. Yet, the score of the “bad” match is higher than the “right” one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "print(fuzz.ratio(\"YANKEES\", \"NEW YORK YANKEES\"))\n",
    "print(fuzz.ratio(\"NEW YORK METS\", \"NEW YORK YANKEES\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fuzz.partial_ratio\n",
    "\n",
    "We use a heuristic called “best partial” when two strings are of noticeably different lengths (such as the case above). **If the shorter string is length m, and the longer string is length n, we’re basically interested in the score of the best matching length-m substring.** Partial_ratio effectively iterates through strings to find best match like so:        \n",
    "        \n",
    "        fuzz.ratio(\"YANKEES\", \"NEW YOR\") ⇒ 14\n",
    "        fuzz.ratio(\"YANKEES\", \"EW YORK\") ⇒ 28\n",
    "        fuzz.ratio(\"YANKEES\", \"W YORK \") ⇒ 28\n",
    "        fuzz.ratio(\"YANKEES\", \" YORK Y\") ⇒ 28\n",
    "        ...\n",
    "        fuzz.ratio(\"YANKEES\", \"YANKEES\") ⇒ 100\n",
    "\n",
    "##### NOTE: This basically assumes the 'short' string is the correct string, or the string we are searching for.  So this really should be called 'Substring match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "78\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "print(fuzz.partial_ratio(\"YANKEES\", \"NEW YORK YANKEES\"))\n",
    "print(fuzz.partial_ratio(\"YANKEEEES\", \"NEW YORK YANKEES\"))\n",
    "print(fuzz.partial_ratio(\"NEW YORK METS\", \"NEW YORK YANKEES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"YANKEEEES\", \"K YANKEES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"Air Canada\"\n",
    "b = \"The plane company Air Canada\"\n",
    "fuzz.partial_ratio(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this should produce a sub-perfect match because the shortest string does not perfectly allign to the longer string\n",
    "a = \"NY YANKEES\"\n",
    "b = \"NEW YORK YANKEES\"\n",
    "fuzz.partial_ratio(b,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Great! This gives us the answer we want. Lets now break apart the forumula using https://github.com/seatgeek/fuzzywuzzy/tree/master/fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.692\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "##################################### \n",
    "##### make string types UNICODE #####\n",
    "#####################################\n",
    "\n",
    "def make_type_consistent(s1, s2):\n",
    "    \"\"\"If both objects aren't either both string or unicode instances force them to unicode\"\"\"\n",
    "    if isinstance(s1, str) and isinstance(s2, str):\n",
    "        return s1, s2\n",
    "\n",
    "    elif isinstance(s1, unicode) and isinstance(s2, unicode):\n",
    "        return s1, s2\n",
    "\n",
    "    else:\n",
    "        return unicode(s1), unicode(s2)\n",
    "\n",
    "#####################################\n",
    "####### partial_ratio function ######\n",
    "#####################################\n",
    "\n",
    "def partial_ratio(s1, s2):\n",
    "    \"\"\"\"Return the ratio of the most similar substring\n",
    "    as a number between 0.0 and 1.\"\"\"\n",
    "    s1, s2 = make_type_consistent(s1, s2) #change to unicode!\n",
    "    \n",
    "    if len(s1) <= len(s2):\n",
    "        shorter = s1\n",
    "        longer = s2\n",
    "    else:\n",
    "        shorter = s2\n",
    "        longer=s1\n",
    "    \n",
    "    #create a sequence matcher using difflib\n",
    "    m = SequenceMatcher(None, shorter, longer)\n",
    "    \n",
    "    #create match blocks\n",
    "    blocks = m.get_matching_blocks()\n",
    "    #print(blocks)\n",
    "    # each block represents a sequence of matching characters in a string\n",
    "    # of the form (idx_1, idx_2, len)\n",
    "    # the best partial match will block align with at least one of those blocks\n",
    "    #   e.g. shorter = \"abcd\", longer = XXXbcdeEEE\n",
    "    #   block = (1,3,3) #shorter starts at 1, longer starts at 3, and block is 3 char long\n",
    "    #   best score === ratio(\"abcd\", \"Xbcd\")\n",
    "\n",
    "    scores = []\n",
    "    for block in blocks:\n",
    "        long_start = block[1] - block[0] if (block[1] - block[0])>0 else 0\n",
    "        long_end = long_start + len(shorter)\n",
    "        long_substr = longer[long_start:long_end]\n",
    "        \n",
    "        m2 = SequenceMatcher(None, shorter, long_substr)\n",
    "        r = m2.ratio()\n",
    "        if r > 0.995:\n",
    "            return 1.0\n",
    "        else:\n",
    "            scores.append(r)\n",
    "    return round(max(scores),3)\n",
    "\n",
    "print(partial_ratio(\"YANKEES\", \"NEW YORK YANKEES\"))\n",
    "print(partial_ratio(\"NEW YORK METS\", \"NEW YORK YANKEES\"))\n",
    "print(partial_ratio(\"METS\", \"NEW YORK METS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fuzz.token_sort_ratio()\n",
    "\n",
    "Sometimes strings may match, but they are in different order!  Here is an extremely common pattern, where one seller constructs strings as “HOME_TEAM vs AWAY_TEAM” and another constructs strings as “AWAY_TEAM vs HOME_TEAM”. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(fuzz.ratio(\"New York Mets vs Atlanta Braves\", \"Atlanta Braves vs New York Mets\"))\n",
    "print(fuzz.partial_ratio(\"New York Mets vs Atlanta Braves\", \"Atlanta Braves vs New York Mets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is obviously not good enough. We use 'token_sort' and 'token_set' approaches to deal with this issue. \n",
    "\n",
    "#### Token Sort:\n",
    "token_sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and then joing them back in a string. \n",
    "\n",
    "    \"new york mets vs atlanta braves\"   →→  \"atlanta braves mets new vs york\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_sort_ratio(\"New York Mets vs Atlanta Braves\", \"Atlanta Braves vs New York Mets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "a,b = \"Bank of Nova Scotia\", \"Scotia Bank\"\n",
    "print(fuzz.token_sort_ratio(a,b))\n",
    "print(fuzz.partial_ratio(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perfect! Lets break apart the function now. We a couple steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atest2 btest1\n",
      "atlanta braves mets new vs york\n",
      "atlanta braves mets new vs york\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "def StringProcessor(string_, rejoin=True):\n",
    "    #clean string and replace non-alphanumeric characters with spaces\n",
    "    regex = re.compile(r\"\\W\")\n",
    "    clean_string = regex.sub(\" \", string_.strip().lower())\n",
    "    \n",
    "    #return tokenized string\n",
    "    clean_sorted = sorted(clean_string.split())\n",
    "    \n",
    "    #join tokens into single string\n",
    "    if rejoin:\n",
    "        return \" \".join(clean_sorted).strip()\n",
    "    else:\n",
    "        return clean_sorted\n",
    "\n",
    "print(StringProcessor('     BTEsT1*&&&&ATEST2***          '))\n",
    "print(StringProcessor('New York Mets* vs Atlanta Braves'))\n",
    "print(StringProcessor('Atlanta Braves vs New York!! Mets!!!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike's fuzzy score:  1.0\n",
      "\n",
      "FuzzyWuzzy fuzzy score:  100\n"
     ]
    }
   ],
   "source": [
    "def token_sort_ratio(s1, s2, partial=False):\n",
    "    sorted1 = StringProcessor(s1)\n",
    "    sorted2 = StringProcessor(s2)\n",
    "    \n",
    "    if partial:\n",
    "        ratio_func = partial_ratio\n",
    "    else:\n",
    "        ratio_func = ratio\n",
    "    \n",
    "    t = \"Mike's fuzzy score: \"\n",
    "    print(t, ratio_func(sorted1, sorted2))\n",
    "    \n",
    "token_sort_ratio(\"New York Mets vs Atlanta Braves\", \"Atlanta Braves vs New York Mets\", partial=True)\n",
    "\n",
    "print(\"\\nFuzzyWuzzy fuzzy score: \",fuzz.token_sort_ratio(\"New York Mets vs Atlanta Braves\", \"Atlanta Braves vs New York Mets\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Set:\n",
    "token_set approach is similar to token_sort, but a little more expensive. Here we tokenize both strings, **but instead of immediately sotring and comparing, we split the tokens into two groups:** \n",
    "- intersection and \n",
    "- remainder. \n",
    "\n",
    "We use those sets to build up a comparison string:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"mariners vs angels\"\n",
    "s2 = \"los angeles angels of anaheim at seattle mariners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike's fuzzy score:  0.5074626865671642\n",
      "\n",
      "FuzzyWuzzy fuzzy score:  51\n"
     ]
    }
   ],
   "source": [
    "#first try token sort ratio\n",
    "token_sort_ratio(s1,s2, partial=False)\n",
    "print(\"\\nFuzzyWuzzy fuzzy score: \",fuzz.token_sort_ratio(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike's fuzzy score:  0.722\n",
      "\n",
      "FuzzyWuzzy fuzzy score:  72\n"
     ]
    }
   ],
   "source": [
    "#now try partial token sort ratio\n",
    "token_sort_ratio(s1,s2, partial=True)\n",
    "print(\"\\nFuzzyWuzzy fuzzy score: \",fuzz.partial_token_sort_ratio(s1,s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hmmm we should get a higher score because these are the same teams!!! \n",
    "\n",
    "So clearly token_sort is not sufficient here, because **the longer string has too many extra tokens that get interleaved with the sort... so we end up comparing:**\n",
    "\n",
    "    t1 = \"angels mariners vs\"\n",
    "    t2 = \"anaheim angeles angels los mariners of seattle vs\"\n",
    "\n",
    "The set method allows us to detect that 'angels' and 'mariners' are common to both strings, and separate those out (the set intersection). Now we construct and compare strings of the following form.\n",
    "\n",
    "    t0 = [SORTED_INTERSECTION]\n",
    "    t1 = [SORTED_INTERSECTION] + [SORTED_REST_OF_STRING1]\n",
    "    t2 = [SORTED_INTERSECTION] + [SORTED_REST_OF_STRING2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike's fuzzy score:  0.875\n"
     ]
    }
   ],
   "source": [
    "def token_set_ratio(s1,s2, partial=False):\n",
    "    \"\"\"Find all alphanumeric tokens in each string...\n",
    "        - treat them as a set\n",
    "        - construct two strings of the form:\n",
    "            <sorted_intersection><sorted_remainder>\n",
    "        - take ratios of those two strings\"\"\"\n",
    "    #create token sets\n",
    "    tokens1 = set(StringProcessor(s1, rejoin=False))\n",
    "    tokens2 = set(StringProcessor(s2, rejoin=False))\n",
    "    \n",
    "    #parse intersection and differences\n",
    "    intersection = sorted(tokens1.intersection(tokens2))\n",
    "    diff1to2 = sorted(tokens1.difference(tokens2))\n",
    "    diff2to1 = sorted(tokens2.difference(tokens1))\n",
    "    \n",
    "    joined_int = \" \".join(intersection)\n",
    "    joined_1to2 = \" \".join(diff1to2)\n",
    "    joined_2to1 = \" \".join(diff2to1)\n",
    "    \n",
    "    t0 = joined_int.strip()\n",
    "    t1 = (joined_int + \" \" + joined_1to2).strip()\n",
    "    t2 = (joined_int + \" \" + joined_2to1).strip()\n",
    "    \n",
    "    if partial:\n",
    "        ratio_func = partial_ratio\n",
    "    else:\n",
    "        ratio_func = ratio\n",
    "    \n",
    "    comps = [ratio_func(t0,t1),\n",
    "             ratio_func(t0,t2),\n",
    "             ratio_func(t1,t2)]\n",
    "    t = \"Mike's fuzzy score: \"\n",
    "    print(t, max(comps))\n",
    "\n",
    "token_set_ratio('this is a new test','this is a different test', partial=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try running these agin with the new token_set_ratio function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"mariners vs angels\"\n",
    "s2 = \"los angeles angels of anaheim at seattle mariners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike's fuzzy score:  0.9090909090909091\n",
      "\n",
      "FuzzyWuzzy fuzzy score:  91\n"
     ]
    }
   ],
   "source": [
    "#token_set RATIO\n",
    "token_set_ratio(s1,s2, partial=False)\n",
    "print(\"\\nFuzzyWuzzy fuzzy score: \",fuzz.token_set_ratio(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike's fuzzy score:  1.0\n",
      "\n",
      "FuzzyWuzzy fuzzy score:  100\n"
     ]
    }
   ],
   "source": [
    "#token_set PARTIAL RATIO\n",
    "token_set_ratio(s1,s2, partial=True)\n",
    "print(\"\\nFuzzyWuzzy fuzzy score: \",fuzz.partial_token_set_ratio(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "a,b = \"Bank of Nova Scotia\", \"ScotiaBank\"\n",
    "print(fuzz.token_sort_ratio(a,b))\n",
    "print(fuzz.partial_token_set_ratio(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### They match!!! WOOOT!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
