{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Term Frequency\n",
    "\n",
    "### CountVectorizer (TF)\n",
    "\n",
    "This is pretty simple. This just takes all the unique words in a series of documents and creates a count of each unqiue word **(ACROSS ALL INSTANCES)**.\n",
    "\n",
    "Here are some key arguments:\n",
    "- **inputs**: obviously just the entire corupus (all of the instances, usually a list of sentences I think!?!?!?)\n",
    "- **strip_accents:** remove accents during preprocessing step. Use ascii if encoding is ascii. Unicode will work on ALL encodings but it is slower.\n",
    "- **analyzer:** Whether each feature should be made of a single word, or a character n-grams.\n",
    "- **max_df:** float in range [0.0 1.0]. When building vocabulary ignore terms that have a document frequency strictly higher than the given threshold. SO GET RID OF WORDS THAT SHOW UP EVERYWHERE AFTER A CERTAIN THRESHOLD.  \n",
    "- **min_df:** float in range [0.0, 1.0] or int, default=1. When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "    \n",
    "Check out the documentation for more! (?CountVectorizer)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['catch', 'cheese', 'flies', 'honey', 'horse', 'more', 'mouse', 'than', 'vinegar', 'water', 'wine', 'with'] \n",
      "\n",
      "[[1 0 1 1 0 1 0 1 1 0 0 2]\n",
      " [0 0 0 0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 1 0 0 0 1 0]] \n",
      "\n",
      "  (0, 8)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 11)\t2\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 6)\t1 \n",
      "\n",
      "   catch  cheese  flies  honey  horse  more  mouse  than  vinegar  water  wine  with\n",
      "0      1       0      1      1      0     1      0     1        1      0     0     2\n",
      "1      0       0      0      0      1     0      0     0        0      1     0     0\n",
      "2      0       1      0      0      0     0      1     0        0      0     1     0\n",
      "3      0       1      0      0      0     0      1     0        0      0     1     0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "docs = [\"You can catch more flies with honey than you can with vinegar you you. Test\",\n",
    "        \"You can lead a horse to water, but you can't make him drink. Test\",\n",
    "        \"You can lead a mouse to cheese, but you can't make him drink wine Test.\",\n",
    "       \"You can lead a mouse to cheese, but you can't make him drink wine.\"]\n",
    "\n",
    "#DON'T INCLUDE WORDS THAT SHOW UP IN MORE THAN HALF OF THE INSTANCES\n",
    "vect = CountVectorizer(min_df=0., max_df=0.50, analyzer='word')\n",
    "X = vect.fit_transform(docs)\n",
    "\n",
    "\n",
    "#you can also get the feature names here\n",
    "print(vect.get_feature_names(),\"\\n\")\n",
    "\n",
    "#X.A will print the same data but represented in a matrix!!!\n",
    "print(X.A,\"\\n\")\n",
    "\n",
    "#printing just X will get you a set of tuples (one for each unique word in each instance and the corresponding count)\n",
    "print(X, \"\\n\")\n",
    "\n",
    "df_VC = pd.DataFrame(X.A, columns=vect.get_feature_names()).to_string()\n",
    "print(df_VC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Jaccard Similarity\n",
    "\n",
    "This is used to determine similarity between documents:\n",
    "- **jaccard_similarity:** It's simply the length of the intersection of the sets of tokens divided by the length of the union of the two sets.\n",
    "![](pictures/LP_NLP_ex1_jacard.jpg)\n",
    "\n",
    "A few key issues with this metric:\n",
    "\n",
    "- Length is irrelevant. (bias towards longer documents).\n",
    "- Words that appear in a lot of documents are weighted the same as those that appear in few. (bias towards longer documents as well as non-descriptive words)\n",
    "\n",
    "**We need a way to weigh certain words differently than others.** Words that appear in all the documents are not going to be good at identifying documents because of the fact that, well... they appear in all the documents. Next we will discuss another similarity measure that takes this into account: **TF-IDF and Cosine Similarity.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "    \n",
    "query= 'Hello my name is mike'.split(\" \")\n",
    "document = 'Hello my name is Jane'.split(\" \")\n",
    "jaccard_similarity(query, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TFIDF Vectorizer\n",
    "\n",
    "Taken from: http://billchambers.me/tutorials/2014/12/21/tf-idf-explained-in-python.html\n",
    "\n",
    "One technique for vectorizing documents is to pick the most frequently occurring terms. However, the most frequent word is a less useful metric since some words like 'this', 'a'  occur very frequently across all documents. \n",
    "\n",
    "**Hence, we also want a measure of how unique a word is i.e. how infrequently the word occurs across all documents (inverse document frequency or idf).** So TFIDF is the product of two components:\n",
    "1. **TF: term frequency** - how many times the word appears in the document\n",
    "2. **IDF: Inverse document frequency:** How many times the word appears in any document\n",
    "\n",
    "**EXAMPLE:** \n",
    "\n",
    "Consider a document containing **100 words wherein the word sun appears 3 times.** The term frequency (i.e., tf) for sun is then (3 / 100) = 0.03. Now, assume we have **10 million documents and the word sun appears in one thousand of these.** Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4 (with log base = 4). Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "\n",
    "- TF = 3/100 = 0.03\n",
    "- IDF = log(10,000,000/1,000) = 4\n",
    "- TF IDF = 0.03 * 4 = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "base = 10\n",
    "\n",
    "#basic term frequency\n",
    "tf = 3/100\n",
    "#apply natural log to doc frequency\n",
    "idf = math.log(10000000/1000, base)\n",
    "\n",
    "tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom tf idf algo\n",
    "Taken from: https://gist.github.com/anabranch/48c5c0124ba4e162b2e3\n",
    "\n",
    "The overall tfidf algorithm will be composed of 5 sub-functions. \n",
    "- **Tokenize:** this just splits documents into word tokens (making all lower case)\n",
    "- **term frequency**: simple way of conting the number of occurences of a token in a document\n",
    "- **sublinear term frequency:** Addresses bias towards longer documents by weighting terms according to document length. This is called normalization. There are two methods to do this, sublienar and augmented frequency (I think this is what sklearn uses?!?)\n",
    "![](pictures/LP_NLP_ex1_sublineartf.jpg)\n",
    "- **augmented term frequnecy:** Accomplished the same thing as sublinear term frequency with a slightly different method. \n",
    "- **IDF:** Inverse document frequency targets words that are unique to certain documents. Its the log of the number of documents (N) over the number of times the term (t) appears in a document (d) in the full list of documents (D).This gives us a weight for every token in every document. This helps determine the important words from the unimportant ones.\n",
    "\n",
    "\n",
    "![](pictures/LP_NLP_ex1_IDF.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build quick tokenize function\n",
    "tokenize = lambda document: document.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#term frequency\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    "\n",
    "tokenized_document=tokenize('Hello Hello my name is mike')\n",
    "term = 'hello'\n",
    "term_frequency(term, tokenized_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.09861228866811"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sublinear term frequency\n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    "\n",
    "tokenized_document=tokenize('Hello Hello Hello my name is mike')\n",
    "term = 'hello'\n",
    "sublinear_term_frequency(term, tokenized_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#augmented term frequency\n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    #picks max frequency of any given term\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    #term frequency relative to max term frequency in doc\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    "\n",
    "tokenized_document=tokenize('Hello Hello my name is mike mike mike')\n",
    "term = 'hello'\n",
    "augmented_term_frequency(term, tokenized_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice how unique words have higher weights!!!!\n",
      "Words in every document have weight of 1\n",
      "As the number of documents grow, the weight of unique words grows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hello': 1.2876820724517808,\n",
       " 'my': 1.0,\n",
       " 'name': 1.0,\n",
       " 'jane': 2.386294361119891,\n",
       " 'hi': 2.386294361119891,\n",
       " 'is': 1.0,\n",
       " 'mike': 2.386294361119891,\n",
       " 'john': 1.6931471805599454}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inverse document frequency\n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    #extract number of docs\n",
    "    num_docs = len(tokenized_documents)\n",
    "    #extract unique words from all docs combined into a single list\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    #loop through each token in all_tokens_set and create idf value dict\n",
    "    idf_values = {}\n",
    "    for tkn in all_tokens_set:\n",
    "        #count number of docs tkn is found in\n",
    "        contains_token = sum(map(lambda doc: tkn in doc, tokenized_documents))\n",
    "        #compute idf value for tkn\n",
    "        idf_values[tkn] = 1 + math.log(num_docs/contains_token)\n",
    "    return idf_values\n",
    "\n",
    "tokenized_documents = [tokenize(\"Hi my name is Mike\"),\n",
    "                      tokenize(\"Hello my name is Jane\"),\n",
    "                      tokenize(\"Hello my name is John\"),\n",
    "                      tokenize(\"Hello my name is John\")]\n",
    "\n",
    "print(\"Notice how unique words have higher weights!!!!\\nWords in every document have weight of 1\\nAs the number of documents grow, the weight of unique words grows\")\n",
    "inverse_document_frequencies(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result should be a n*m matrix, where n is the number of documents, and m is the number of unique words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>hello</th>\n",
       "      <th>my</th>\n",
       "      <th>name</th>\n",
       "      <th>jane</th>\n",
       "      <th>ciniello</th>\n",
       "      <th>hi</th>\n",
       "      <th>is</th>\n",
       "      <th>mike</th>\n",
       "      <th>john</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.386294</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                hello   my  name      jane  ciniello        hi   is      mike  \\\n",
       "0  0.000000  0.000000  1.0   1.0  0.000000  2.386294  1.287682  1.0  2.386294   \n",
       "1  0.000000  1.287682  1.0   1.0  2.386294  0.000000  1.287682  1.0  0.000000   \n",
       "2  2.386294  1.287682  1.0   1.0  0.000000  0.000000  1.287682  1.0  0.000000   \n",
       "3  0.000000  1.287682  1.0   1.0  0.000000  0.000000  0.000000  1.0  0.000000   \n",
       "\n",
       "       john  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  1.693147  \n",
       "3  1.693147  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put it all together with a tfidf function!!!\n",
    "def tfidf(documents):\n",
    "    #tokenize each document\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    #calcualte idf values for each unique term\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    #loop through document, and each unique idf term to get set of tfidf values for each document\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        #loop through each term\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            #calculate sublinear term frequency\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            #calculate FINAL tfidf values in single document\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        #append doc_tfidf to ALL documents list\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents, idf.keys()\n",
    "\n",
    "documents = [\"Hi my name is Mike Ciniello\",\n",
    "             \"Hello hi my name is Jane\",\n",
    "             \"Hello  hi my name is John\",\n",
    "             \"Hello my name is John\"]\n",
    "print(\"Result should be a n*m matrix, where n is the number of documents, and m is the number of unique words\")    \n",
    "vals, words = tfidf(documents)\n",
    "pd.DataFrame(vals, columns = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count:\n",
      "133\n",
      "Unique word count:\n",
      "94\n",
      "Shape of tfidf conversion (should be 7 rows and 94 features)\n"
     ]
    }
   ],
   "source": [
    "#test out the tfidf funciton\n",
    "document_0 = \"China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\"\n",
    "document_1 = \"At last, China seems serious about confronting an endemic problem: domestic violence and corruption.\"\n",
    "document_2 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_3 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_4 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_5 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_6 = \"Vladimir Putin is riding a horse while hunting deer. Vladimir Putin always seems so serious about things - even riding horses. Is he crazy?\"\n",
    "\n",
    "all_documents = [document_0, document_1, document_2, document_3, document_4, document_5, document_6]\n",
    "all_documents\n",
    "\n",
    "print(\"Total word count:\")\n",
    "print(sum([len(x.split(\" \")) for x in all_documents]))\n",
    "print(\"Unique word count:\")\n",
    "full_list = []\n",
    "[full_list.extend(item) for item in [tokenize(doc) for doc in all_documents]]\n",
    "print(len(set(full_list)))\n",
    "print('Shape of tfidf conversion (should be 7 rows and 94 features)')\n",
    "tfidf_conversion = tfidf(all_documents)\n",
    "#print(pd.DataFrame(tfidf_conversion).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 94)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare output to sklearn\n",
    "#import vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#initialize\n",
    "sklearn_tfidf = TfidfVectorizer(all_documents, \n",
    "                norm='l2', min_df=0, use_idf=True, \n",
    "                smooth_idf=False, \n",
    "                sublinear_tf=True,\n",
    "               tokenizer=tokenize)\n",
    "#fit transform\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(all_documents)\n",
    "\n",
    "sklearn_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual tfidf: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 1.336472236621213, 0.0, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5596157879354227, 0.0, 1.8472978603872037, 2.9459101490553135, 0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 3.8142592685777856, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0], [2.9459101490553135, 2.9459101490553135, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 0.0, 0.0, 2.262844199331851, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 2.9459101490553135, 0.0, 2.252762968495368, 2.9459101490553135, 2.252762968495368, 0.0, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 2.252762968495368, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 4.72767604914083, 0.0, 0.0, 0.0, 1.5596157879354227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.262844199331851, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 2.252762968495368, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5596157879354227, 0.0, 1.8472978603872037, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.336472236621213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0]]\n",
      "Sklearn tfidf: [0.         0.31120122 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Manual tfidf:\", tfidf_conversion[0][0:5])\n",
    "print(\"Sklearn tfidf:\", sklearn_representation.A[0][0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
