{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 12. Parts-of-Speech tagging (Part 2 - RNN)\n",
    "\n",
    "# Model 2: Recurrent Neural Network\n",
    "\n",
    "### Recurrent Neural Nets \n",
    "- In its simplest form, looks similar to feed forward, except it has a feedback loop where hidden goes back into itself\n",
    "- This allows model to take into account data from the past\n",
    "- This is useful for word sequences and taking into account context:\n",
    "    - p(tag | \"milk\") is ambiguous!!!\n",
    "    - p(tag | \"I just drank a glss of milk\") is more clear, \"milk\" is clearly a noun\n",
    "![](pictures/NLP_12_rnns.png)\n",
    "\n",
    "- GRUs and LSTMs are like little mini systems of neural networks. \n",
    "- These units help us learn long term dependencies, and avoid vanishing and exploding gradients\n",
    "- They can also be swapped out and changed pretty easily, as TensorFlow allows you to do so\n",
    "\n",
    "![](pictures/NLP_12_rnnsgrus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's write some code!\n",
    "\n",
    "# 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# ML package\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.contrib.rnn import static_rnn as get_rnn_output\n",
    "from tensorflow.contrib.rnn import BasicRNNCell, GRUCell\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Custom package to get data\n",
    "from MC_NLP_util import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create get_data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(split_sequences=False, train_data = 'NLP_12_chunking_train.txt', test_data='NLP_12_chunking_test.txt', wordidx_tag_start=0):\n",
    "    if not os.path.exists(train_data):\n",
    "        print(\"Training data is not in root folder.\")\n",
    "        print(\"Please check the comments to get the download link.\")\n",
    "        exit()\n",
    "    elif not os.path.exists(test_data):\n",
    "        print(\"Test data is not in root folder.\")\n",
    "        print(\"Please check the comments to get the download link.\")\n",
    "        exit()\n",
    "\n",
    "    word2idx = {}\n",
    "    tag2idx = {}\n",
    "    # word_idx MUST start at 1 for rnns, because TF will pad sequences with 0s\n",
    "    word_idx = wordidx_tag_start\n",
    "    tag_idx = wordidx_tag_start\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    # REMEMBER: Each LINE contains one word and one tag, \n",
    "    # and each SENTANCE is separated by ONE BLANK LINE\n",
    "    # IF the line is blank, \n",
    "    for line in open(train_data):\n",
    "        line = line.rstrip() #right strip empty chars\n",
    "        # CHECK IF LINE IS BLANK\n",
    "        if line:  \n",
    "            # SPLIT WORD AND TAG\n",
    "            r = line.split()\n",
    "            word, tag, _ = r\n",
    "            # ADD WORD TO WORD2IDX\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = word_idx\n",
    "                word_idx += 1\n",
    "            currentX.append(word2idx[word])\n",
    "            # ADD WORD TO TAG2IDX\n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            currentY.append(tag2idx[tag])\n",
    "        # If split sequences is false, then we just take each word/tag as our inputs\n",
    "        # If split sequences is true, then we append currentX/Y, which contains will sentence\n",
    "        # ... and then empt currentX/Y so we can collect the next sentence. \n",
    "        # if it is NOT true, then we can \n",
    "        elif split_sequences:\n",
    "            Xtrain.append(currentX) #append full sentence of words\n",
    "            Ytrain.append(currentY) #append full sentence of tags\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "    # Again, if split sequences is FALSE, we simply take all word/tags and toss them into long ass list!\n",
    "    if not split_sequences:\n",
    "        Xtrain = currentX\n",
    "        Ytrain = currentY\n",
    "\n",
    "    # load and score test data\n",
    "    Xtest = []\n",
    "    Ytest = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    for line in open(test_data):\n",
    "        line = line.rstrip()\n",
    "        if line:\n",
    "            r = line.split()\n",
    "            word, tag, _ = r\n",
    "            if word in word2idx:\n",
    "                currentX.append(word2idx[word])\n",
    "            else:\n",
    "                currentX.append(word_idx) # use this as unknown\n",
    "            currentY.append(tag2idx[tag])\n",
    "        elif split_sequences:\n",
    "            Xtest.append(currentX)\n",
    "            Ytest.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "    if not split_sequences:\n",
    "        Xtest = currentX\n",
    "        Ytest = currentY\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create RNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, Y, V=None, K=None, D=50, lr=1e-1, mu=0.99, batch_sz=100, epochs=6):\n",
    "        if V is None:\n",
    "            V = len(set(X))\n",
    "        if K is None:\n",
    "            K = len(set(Y))\n",
    "        N = len(X)\n",
    "\n",
    "        W = np.random.randn(V, K) / np.sqrt(V + K)\n",
    "        b = np.zeros(K)\n",
    "        self.W = theano.shared(W)\n",
    "        self.b = theano.shared(b)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        thX = T.ivector('X')\n",
    "        thY = T.ivector('Y')\n",
    "\n",
    "        py_x = T.nnet.softmax(self.W[thX] + self.b)\n",
    "        prediction = T.argmax(py_x, axis=1)\n",
    "\n",
    "        cost = -T.mean(T.log(py_x[T.arange(thY.shape[0]), thY]))\n",
    "        grads = T.grad(cost, self.params)\n",
    "        dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "        self.cost_predict_op = theano.function(\n",
    "            inputs=[thX, thY],\n",
    "            outputs=[cost, prediction],\n",
    "            allow_input_downcast=True,\n",
    "        )\n",
    "\n",
    "        updates = [(p, p + mu*dp - lr*g) for p, dp, g in zip(self.params, dparams, grads)] + [(dp, mu*dp - lr*g) for dp, g in zip(dparams, grads)]\n",
    "        \n",
    "        train_op = theano.function(inputs=[thX, thY], outputs=[cost, prediction], updates=updates, allow_input_downcast=True)\n",
    "\n",
    "        costs = []\n",
    "        n_batches = N // batch_sz\n",
    "        for i in range(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            print(\"epoch:\", i)\n",
    "            for j in range(n_batches):\n",
    "                Xbatch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                Ybatch = Y[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "\n",
    "                c, p = train_op(Xbatch, Ybatch)\n",
    "                costs.append(c)\n",
    "                if j % 200 == 0:\n",
    "                    print(\n",
    "                        \"i:\", i, \"j:\", j,\n",
    "                        \"n_batches:\", n_batches,\n",
    "                        \"cost:\", c,\n",
    "                        \"error:\", np.mean(p != Ybatch))\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        _, p = self.cost_predict_op(X, Y)\n",
    "        return np.mean(p == Y)\n",
    "\n",
    "    def f1_score(self, X, Y):\n",
    "        _, p = self.cost_predict_op(X, Y)\n",
    "        return f1_score(Y, p, average=None).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 19123\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "Xtrain, Ytrain, Xtest, Ytest, word2idx = get_data()\n",
    "\n",
    "# convert to numpy arrays\n",
    "Xtrain = np.array(Xtrain)\n",
    "Ytrain = np.array(Ytrain)\n",
    "\n",
    "# convert Xtrain to indicator matrix\n",
    "N = len(Xtrain)\n",
    "V = len(word2idx) + 1\n",
    "print(\"vocabulary size:\", V)\n",
    "# Xtrain_indicator = np.zeros((N, V))\n",
    "# Xtrain_indicator[np.arange(N), Xtrain] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. DecisionTree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt train score: 0.964959594194\n",
      "dt train f1: 0.907858696936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mciniello\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# without indicator\n",
    "dt.fit(Xtrain.reshape(N, 1), Ytrain)\n",
    "print(\"dt train score:\", dt.score(Xtrain.reshape(N, 1), Ytrain))\n",
    "p = dt.predict(Xtrain.reshape(N, 1))\n",
    "print(\"dt train f1:\", f1_score(Ytrain, p, average=None).mean())\n",
    "\n",
    "# with indicator -- too slow!!\n",
    "# dt.fit(Xtrain_indicator, Ytrain)\n",
    "# print(\"dt score:\", dt.score(Xtrain_indicator, Ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "i: 0 j: 0 n_batches: 2117 cost: 3.784513628317909 error: 0.98\n"
     ]
    }
   ],
   "source": [
    "# train and score\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain, V=V)\n",
    "print(\"training complete\")\n",
    "print(\"lr train score:\", model.score(Xtrain, Ytrain))\n",
    "print(\"lr train f1:\", model.f1_score(Xtrain, Ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Compare baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ntest = len(Xtest)\n",
    "Xtest = np.array(Xtest)\n",
    "Ytest = np.array(Ytest)\n",
    "# convert Xtest to indicator\n",
    "# Xtest_indicator = np.zeros((Ntest, V))\n",
    "# Xtest_indicator[np.arange(Ntest), Xtest] = 1\n",
    "\n",
    "# decision tree test score\n",
    "print(\"dt test score:\", dt.score(Xtest.reshape(Ntest, 1), Ytest))\n",
    "p = dt.predict(Xtest.reshape(Ntest, 1))\n",
    "print(\"dt test f1:\", f1_score(Ytest, p, average=None).mean())\n",
    "# print(\"dt test score:\", dt.score(Xtest_indicator, Ytest)) # too slow!\n",
    "\n",
    "# logistic test score -- too slow!!\n",
    "print(\"lr test score:\", model.score(Xtest, Ytest))\n",
    "print(\"lr test f1:\", model.f1_score(Xtest, Ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
