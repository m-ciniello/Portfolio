{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis Overview\n",
    "\n",
    "- multiple words with the same meaning (synonyms): 'buy' and 'puchase'\n",
    "- one word with multiple meanings (polysemy): 'Milk' or 'milk it'\n",
    "\n",
    "\n",
    "**Latent Variables:**\n",
    "- A variables that represents all synonyms\n",
    "- challenge is to transorm original data and transform them into the common latent variables\n",
    "    - z = 0.7 * computer + 0.5 * PC + 0.6 * Laptop\n",
    "- However this doesnt always work for polysemy, works better for synonyms\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanics of LSA\n",
    " - **LSA IS REALLY just the application of SVD (singular value decomposition) on the term document matrix.**\n",
    " - But before we learn about SVD we need to learn more about PCA, a simpler version of SVD\n",
    " \n",
    " ## PCA (Principal Components Analysis)\n",
    " \n",
    "- z = Qx\n",
    "- Q is a , I guess is x is the eigenvector matrix?\n",
    "- scalar * vector = another vector, same direction\n",
    "- matrix * vector = another vector, possible DIFFERENT direction\n",
    "- PCA rotates our original input vectors... So you could think of it as the same vectors but in a different coordinate system\n",
    "\n",
    "PCA does 3 things for us:\n",
    "- Decorrelates the input date\n",
    "- Transformed data is ordered by information content (variance)\n",
    "- Dimensionality reduction\n",
    "    - Note that removing information != decreasing predictive abilitiy\n",
    "    - If our original vocab is 1000 words, if we join all words by how often they co-occur in each document, the total number of destinct LATENT terms is only 100. \n",
    "    - This is also a good way to de-noise or smooth data\n",
    "    \n",
    "### Covariance matrix\n",
    "\n",
    "In the covariance matrix:\n",
    "- the diagonals: tell us the variance of that direction\n",
    "- the off diagonals: tell us how correlated 2 different dimensions are with each other\n",
    "\n",
    "###### Remember: more variance is synonymous with MORE information.\n",
    "\n",
    "The covariance matrix is computed by the following equation:\n",
    "\n",
    "$S = 1/N (X - \\pmb m)\\;(X - \\pmb m)^T$\n",
    "\n",
    "where **m** is the mean vector: $\\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k$\n",
    "\n",
    "### Eigenvalues and Eigenvecors\n",
    "- A = diagonal matrix of eigenvalues (there are D of them, D is num dimensions in the orignal dataset)\n",
    "- Q = matrix of stacked eigenvectors (there are D of them,  D is num dimensions in the orignal dataset)\n",
    "- we sort A so that eigenvalues are in descending order\n",
    "- remember that z = Qz\n",
    "- it turns out that A is the covariance matrix of z, therefore:\n",
    "    - variance aka information in Z is sorted in descending order\n",
    "    - none of the dimensions in Z are correlated.\n",
    "    \n",
    "Learn more : https://lazyprogrammer.me/tutorial-principal-components-analysis-pca/\n",
    "\n",
    "\n",
    "## Extending PCA\n",
    "\n",
    "PCA helps us combine input features in the term document matrix. But what if we wanted to combine and decorrelate by document? ** Just do PCA on the transpose?** \n",
    "\n",
    "**Strange Result:**\n",
    "- still have D eigenvalues (covariance is N * N)\n",
    "- they are the same eigenvalues we found before\n",
    "\n",
    "## SVD (singular value decomposition)\n",
    "\n",
    "SVD just does PCA on both of these at the same time! As mentioned above, the eigevnalues of the covariance matrices will be the same, but the eigenvectors will be different!\n",
    "\n",
    "1. Find eigenvalues (S^2) and eignevectors (U) of XX_T\n",
    "2. Find eigenvalues (S^2) and eignevectors (V) of X_TX\n",
    "\n",
    "There are related by:\n",
    "\n",
    "X=U\\*S\\*V_T\n",
    "\n",
    "- ie X is decomposed into 3 parts (U, S, V_T)\n",
    "- we can transform both terms AND documents\n",
    "- get the \"low-rank\" approximation of X by keeping first k elements of U,S,V\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets write some code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: http://lazyprogrammer.me\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "()\n",
    "#import data line by line\n",
    "#remove training whitespace with rstrip\n",
    "titles = [line.rstrip() for line in open('all_book_titles.txt')]\n",
    "\n",
    "#import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "len(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add more stopwords\n",
    "all_stopwords = all_stopwords.union({\n",
    "    'introduction', 'edition', 'series', 'application',\n",
    "    'approach', 'card', 'access', 'package', 'plus', 'etext',\n",
    "    'brief', 'vol', 'fundamental', 'guide', 'essential', 'printed',\n",
    "    'third', 'second', 'fourth', })\n",
    "len(all_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "#create tokenizer\n",
    "def mc_tokenizer(s):\n",
    "    s = s.lower() #downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t) > 2] #remove short words\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in all_stopwords] #remove stopwords\n",
    "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)]\n",
    "    return tokens\n",
    "    \n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First create wordindex and vector in Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length (instances) =  2373\n",
      "word map (vocab) length =  2133\n",
      "vector size =  (2133, 2373)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=mc_tokenizer)\n",
    "titles_countvec = vectorizer.fit_transform(titles)\n",
    "\n",
    "#transpose to put words along rows, and documents along columns\n",
    "titles_countvec = titles_countvec.T\n",
    "print('tokens length (instances) = ', len(titles))\n",
    "print('word map (vocab) length = ', len(vectorizer.vocabulary_))\n",
    "print('vector size = ', titles_countvec.A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Create WordIndex and Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors parsing file: 0 number of lines in file: 2373\n",
      "tokens length (instances) =  2373\n",
      "word map (vocab) length =  2131\n",
      "vector size =  (2131, 2373)\n"
     ]
    }
   ],
   "source": [
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "all_tokens = []\n",
    "all_titles = []\n",
    "index_word_map = []\n",
    "error_count = 0\n",
    "for title in titles:\n",
    "    try:\n",
    "        title = title.encode('ascii', 'ignore').decode('utf-8') # this will throw exception if bad characters\n",
    "        all_titles.append(title)\n",
    "        tokens = mc_tokenizer(title)\n",
    "        all_tokens.append(tokens)\n",
    "        for token in tokens:\n",
    "            if token not in word_index_map:\n",
    "                word_index_map[token] = current_index\n",
    "                current_index += 1\n",
    "                index_word_map.append(token)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(title)\n",
    "        error_count += 1\n",
    "\n",
    "print(\"Number of errors parsing file:\", error_count, \"number of lines in file:\", len(titles))\n",
    "if error_count == len(titles):\n",
    "    print(\"There is no data to do anything with! Quitting...\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# now let's create our input matrices\n",
    "# just indicator variables for this example - works better than proportions\n",
    "def tokens_to_vector(tokens):\n",
    "    x = np.zeros(len(word_index_map))\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] = 1\n",
    "    return x\n",
    "\n",
    "N = len(all_tokens)\n",
    "D = len(word_index_map)\n",
    "X = np.zeros((D, N)) # terms will go along rows, documents along columns\n",
    "i = 0\n",
    "for tokens in all_tokens:\n",
    "    X[:,i] = tokens_to_vector(tokens)\n",
    "    i += 1\n",
    "    \n",
    "print('tokens length (instances) = ', N)\n",
    "print('word map (vocab) length = ', D)\n",
    "print('vector size = ', X.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now run SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "def SVDfunc(X,figsize=(20,20)):\n",
    "    svd = TruncatedSVD()\n",
    "    Z = svd.fit_transform(X)\n",
    "    plt.scatter(Z[:,0], Z[:,1])\n",
    "    for i in range(D):\n",
    "        plt.annotate(s=index_word_map[i], xy=(Z[i,0], Z[i,1]))\n",
    "    plt.figure(figsize=(figsize))\n",
    "    plt.show()\n",
    "\n",
    "SVDfunc(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
