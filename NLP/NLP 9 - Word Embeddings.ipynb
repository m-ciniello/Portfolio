{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9. Word Embeddings\n",
    "\n",
    "# 1. Overview\n",
    "\n",
    "- Up to now we've been representing words as features in a one-hot-encoded matrix. Ie. each words gets its own feature. \n",
    "- This is problematic for datasets with HUGE vocabs... A vocab of 1 million = 1 million features to train on\n",
    "\n",
    "\n",
    "- **Problem:** One pair of words (car, vehicle) might be more related to each other than some other pair (cat, ocean)\n",
    "    - All pairs of words are the same distance apart!!!\n",
    "    - **All words are a distance of 1 from the origin, and a Manhattan distance of 2 from each other**\n",
    "    - vectors in this high dimensional space are ALL orthogonal to the axes! \n",
    "    - We can't tell \"how well are two words related?\"\n",
    "    \n",
    "    \n",
    "- **Solution:** Word embeddings! But how do we make them?\n",
    "\n",
    "    - Its actually pretty simply: **Apply PCA to a V \\* D matrix (where V is vocab, D is documents)**. \n",
    "    - **Each word becomes and overservation, and each document is a feature.**\n",
    "    - This give us a vector prepresentation of the word (PCA finds correlations and gives you smalled vector but retains information, as you know)\n",
    "    - You can also step it up a notch and used TF-IDF (instead of raw counts for your original matrix, before PCA) and non-linear t-SNE (instead of basic linear PCA). \n",
    "    - **word2vec and GLoVe use similar practices, in very creative ways, to create word embeddings**\n",
    "    \n",
    " ### We will first start by looking at some pretrained word2vec word embeddings (and experiment with Word Analogies). We will then build our own more simple tfidf word embeddings (not using word2vec). In the next section will will finally we will get into building our own custom word2vec implementation. WOOOT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Pre-trained word embeddings\n",
    "\n",
    "Stolen from here: https://blog.manash.me/how-to-use-pre-trained-word-vectors-from-facebooks-fasttext-a71e6d55f27\n",
    "\n",
    "- Training word embeddings with word2vec or GLoVe often takes a lot of computing power\n",
    "- Fortunately, you can use aweseom pretrained word embeddings! We can use gensim to load in pretrained vectors\n",
    "- The vectors were trained by FB and can be found at https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "    - from github: \"We are publishing pre-trained word vectors for 294 languages, trained on Wikipedia using fastText. These vectors in dimension 300 were obtained using the skip-gram model described in Bojanowski et al. (2016) with default parameters.\"\n",
    "    - The word vectors come in both the binary and text default formats of fastText. In the text format, each line contain a word followed by its embedding. Each value is space separated. Words are ordered by their frequency in a descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors load time:  44.94893479347229\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from time import time\n",
    "# Create the model\n",
    "t0 = time()\n",
    "en_model = KeyedVectors.load_word2vec_format('wiki.simple.vec')\n",
    "t1 = time()\n",
    "print(\"Vectors load time: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 111051\n",
      "Dimension of a word vector: 300\n",
      "Example vector (first n elements of 300): [ 0.28922001 -0.46075001  0.35141999 -0.41104001  0.16421001  0.17307\n",
      " -0.21562    -0.090636   -0.079495   -0.11149   ]\n"
     ]
    }
   ],
   "source": [
    "# Getting the tokens\n",
    "words = []\n",
    "for word in en_model.vocab:\n",
    "    words.append(word)\n",
    "    \n",
    "# Printing out number of tokens available\n",
    "print(\"Number of Words: {}\".format(len(words)))\n",
    "\n",
    "# Printing out the dimension of a word vector \n",
    "print(\"Dimension of a word vector: {}\".format(len(en_model[words[0]])))\n",
    "\n",
    "# Print out the vector of a word \n",
    "print(\"Example vector (first n elements of 300):\",en_model[words[0]][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analogies\n",
    "- Now that we have loaded our pretrained vectors in, lets experiment with them using word analogies\n",
    "- Generally speaking, words vectors with similar directions have similar meanings (especially if you use word2vec or GLoVe)\n",
    "- For instance:\n",
    "        king - man ~= Queen - woman\n",
    "        \n",
    " We will test this out with this simple exercise:\n",
    " \n",
    " 1. Convert some words to their word embeddings\n",
    "     - Eg. vec(\"king\") = WE[word2idx[\"king\"]]\n",
    "     - v0 = vec(king) - vec(man) + vec(woman)\n",
    "     - #v0 is just a vector in a space with an infinite number of values\n",
    " 2. Loop through all word vectors, find the one closest to v0, return that word\n",
    "     - There is no way to map directly from a vector to a word, as the vector space is infinite, and that would require an infinite amount of words.\n",
    "     - As such, there are several similarity metrics we will use to find the \\*CLOSEST\\* word\n",
    "     \n",
    "**Distance Metrics:**\n",
    "\n",
    "There are many distance metrics we could use:\n",
    "- Euclidean distance (plain old squared distance): $||a-b^2||$ \n",
    "- Cosine distance: $cos\\_distance(a,b) = 1-\\space a^Tb/(||a||\\space ||b||)$\n",
    "    - since: $a^Tb = ||a||\\space ||b|| (cos(a,b)$\n",
    "    - Paralell vectors (0 degree angle):\n",
    "        - $cos(0deg) = 1 $\n",
    "        - 1 is max val of cos\n",
    "    - Orthogonal vectors (90 degree angle)\n",
    "        - $cos(90deg)=0$\n",
    "    - Vectors in opposite direction (180 degree angle)\n",
    "        - $cos(180deg)=-1$\n",
    "        - (-1) is min val of cos\n",
    "    - So essentially, the closer the vectors are, the LARGER cos(y) will be... this is sort of the opposite of what we want\n",
    "        - $cos(theta)$ \n",
    "    - That said, we take the negative of the cos function...\n",
    "        - we want our \"distance\" function to be  $1 - cos(theta)$\n",
    "    - **For this distance, its useful to normalize all word embeddings so length is 1. So ALL word embeddings lie on the UNIT SPHERE**\n",
    "    \n",
    "**Finding closest vector matches:**\n",
    "\n",
    "Loop through the words and keep track of distances:\n",
    "    #pseudocode\n",
    "    min_dist = Infinity\n",
    "    best_word = ''\n",
    "    for word, idx in word2idx.iteritems():\n",
    "        v1 = WE[idx]\n",
    "        if dist(v0, v1) < min_dist:\n",
    "            min_dist = dist(v0, v1)\n",
    "            best_word = word\n",
    "    print(\"The closest word is: \", best_word)\n",
    "    \n",
    "#### NOTE: gensims similarity method computes the cosine similarity. We will use it as a check to make sure our function is working as expected. Gensim also has a 'most_similar' method which we will use instead of this loop (as the we have a big vocab in this test set and the loop would be inefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom cosine similarity:  0.544238\n",
      "Gensim cosine similarity:  0.544238451305\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as la\n",
    "#create cosine_sim function\n",
    "def cosine_sim(a, b):\n",
    "    numerator = a.T.dot(b)\n",
    "    denominator = la.norm(a) * la.norm(b)\n",
    "    return numerator / denominator\n",
    "\n",
    "a = 'queen'\n",
    "b = 'king'\n",
    "vec_a = en_model[a]\n",
    "vec_b = en_model[b]\n",
    "\n",
    "print(\"Custom cosine similarity: \", cosine_sim(vec_a, vec_b))\n",
    "print(\"Gensim cosine similarity: \", en_model.similarity(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alright, lets play around with some word analogies!\n",
    "We will use gensims similar by vector method here:\n",
    "    \n",
    "    similar_by_vector(vector, topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.5498616695404053),\n",
       " ('spider', 0.23319827020168304),\n",
       " ('woman', 0.23259012401103973),\n",
       " ('mischief', 0.20489361882209778),\n",
       " ('ejaculate', 0.20252743363380432),\n",
       " ('spiderleg', 0.19765730202198029),\n",
       " ('topless', 0.1948021799325943),\n",
       " ('naturopathy', 0.1926197111606598),\n",
       " ('ejaculated', 0.18959881365299225),\n",
       " ('spiderman', 0.18746206164360046)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'man'\n",
    "b = 'king'\n",
    "test = en_model[a] - en_model[b]\n",
    "\n",
    "en_model.similar_by_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.6832274794578552),\n",
       " ('kingship', 0.43243950605392456),\n",
       " ('kings', 0.430464506149292),\n",
       " ('kingz', 0.4212456941604614),\n",
       " ('queen', 0.35821980237960815),\n",
       " ('kingdoms', 0.3533620238304138),\n",
       " ('kingkong', 0.353018581867218),\n",
       " ('abdicates', 0.34669357538223267),\n",
       " ('kingdome', 0.34243044257164),\n",
       " ('reigned', 0.333217054605484)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'king'\n",
    "b = 'man'\n",
    "test = en_model[a] - en_model[b]\n",
    "\n",
    "en_model.similar_by_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6491795182228088),\n",
       " ('queene', 0.44217807054519653),\n",
       " ('queens', 0.38087162375450134),\n",
       " ('queenie', 0.37162333726882935),\n",
       " ('queenside', 0.3590025305747986),\n",
       " ('queensrÿche', 0.3425987958908081),\n",
       " ('queensway', 0.33737051486968994),\n",
       " ('consort', 0.3284097909927368),\n",
       " ('elizabeth', 0.32121044397354126),\n",
       " ('queensbury', 0.30972492694854736)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'queen'\n",
    "b = 'woman'\n",
    "test = en_model[a] - en_model[b]\n",
    "\n",
    "en_model.similar_by_vector(test, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tf-Idf & t-SNE to find word embedding\n",
    "\n",
    "We can use TF-IDF and t-SNE to give us a low dimension word embedding. **As described above, this is a precursor to word2vec, and its good to understand these fundementals prior to jumping into the big algos!**\n",
    "\n",
    "#### TF-IDF\n",
    "Recall that TF-IDF discounts words that appear in many different types of docs (taking into account the global frequency of a word). It also gives higher scores to words that appear in only a few number of documents. \n",
    "\n",
    "#### t-SNE\n",
    "t-SNE is a non linear dimensionality reduction method. Will it perform better thatn linear PCA? We shall see!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold import TSNE\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import operator\n",
    "\n",
    "#NOTE WE DO NOT NEED REUSABLE FUNCTIONS AS WE RELOAD CODE BELOW\n",
    "#import reusable function so we dont have to rewrite them in this notebook\n",
    "#from LazyProgrammerGitRepos.rnn_class.util import get_wikipedia_data\n",
    "#from LazyProgrammerGitRepos.rnn_class.brown import get_sentences_with_word2idx_limit_vocab, get_sentences_with_word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import old code\n",
    "Used to get data from brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GET SENTENCES FROM BROWN CORPUS\n",
    "def get_sentences():\n",
    "    # returns 57340 of the Brown corpus\n",
    "    # each sentence is represented as a list of individual string tokens\n",
    "    return brown.sents()\n",
    "test = get_sentences()\n",
    "\n",
    "#SENTANCES TO INDEX REPRESENTATION WITH LIMITED VOCAB\n",
    "KEEP_WORDS = set([\n",
    "  'king', 'man', 'queen', 'woman',\n",
    "  'italy', 'rome', 'france', 'paris',\n",
    "  'london', 'britain', 'england',\n",
    "])\n",
    "\n",
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, keep_words=KEEP_WORDS, print_v=False):\n",
    "    #initialize sentences and index\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    i = 2\n",
    "    #initialize start/end tags\n",
    "    #capitalized so wont get confused with actual words in corpus\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    \n",
    "    #Set start tokens to inf so they dont get removed when sorting by count\n",
    "    word_idx_count = {0: float('inf'), 1: float('inf')}\n",
    "    \n",
    "    #Count each word \n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "            # keep track of counts for later sorting\n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "\n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "    # restrict vocab size\n",
    "    # set all the words I want to keep to infinity\n",
    "    # so that they are included when I pick the most\n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "    #tell sorted funciton to use 2nd item to sort\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    #create new dictionary from old dict\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        if print_v:\n",
    "            print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    # replcae all infrequent words are replaced with 'UNKOWN\"\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "    \n",
    "    # sanit check to make sure all words wanted to keep are still there\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "\n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:  57013\n",
      "Sentences:  1501\n"
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=1500)\n",
    "print('Sentences: ',len(sentences))\n",
    "print('Sentences: ',len(word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build VxN term document matrix\n",
    "\n",
    "This is used to create the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences complete\n",
      "20000 sentences complete\n",
      "30000 sentences complete\n",
      "40000 sentences complete\n",
      "50000 sentences complete\n",
      "finished getting raw counts\n"
     ]
    }
   ],
   "source": [
    "V = len(word2idx)\n",
    "N = len(sentences)\n",
    "\n",
    "# create raw counts first\n",
    "A = np.zeros((V, N))\n",
    "j = 0\n",
    "for sentence in sentences:\n",
    "    for i in sentence:\n",
    "        A[i,j] += 1\n",
    "    j += 1\n",
    "    if j%10000 == 0:\n",
    "        print(\"{} sentences complete\".format(j))\n",
    "print(\"finished getting raw counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform VxN matrix using TFIDF\n",
    "\n",
    "#### Full disclosure, I have no idea if this is legit. It doesnt makes sense to me because for TFIDF to work, you need to have the documents along the rows... so that Inverse Document Frequency can be calculated accurately. Anyway, lets power through for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1501, 57013)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load TfidfTransformer again for funsies\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "A1 = transformer.fit_transform(A)\n",
    "\n",
    "#turn to array from sklearn sparse matrix\n",
    "A1 = A1.toarray()\n",
    "\n",
    "A1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get t-SNE word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  396.6635196208954\n"
     ]
    }
   ],
   "source": [
    "#initialize\n",
    "tsne = TSNE()\n",
    "t0 = time()\n",
    "#fit_transform\n",
    "Z = tsne.fit_transform(A1)\n",
    "t1 = time()\n",
    "\n",
    "print(\"Runtime: \",t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test out some word analogies\n",
    "First create find_analogies function, which performs the following function:\n",
    "    \n",
    "    W1 - W2 + W3 = ?\n",
    "    \n",
    "Then toss in a bunch of word sets and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the find_analogies function\n",
    "def find_analogies(w1, w2, w3, We, word2idx):\n",
    "    king = We[word2idx[w1]]\n",
    "    man = We[word2idx[w2]]\n",
    "    woman = We[word2idx[w3]]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    def dist1(a, b):\n",
    "        return np.linalg.norm(a - b)\n",
    "    def dist2(a, b):\n",
    "        return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    for dist, name in [(dist1, 'Euclidean'), (dist2, 'cosine')]:\n",
    "        min_dist = float('inf')\n",
    "        best_word = ''\n",
    "        for word, idx in word2idx.items():\n",
    "            if word not in (w1, w2, w3):\n",
    "                v1 = We[idx]\n",
    "                d = dist(v0, v1)\n",
    "                if d < min_dist:\n",
    "                    min_dist = d\n",
    "                    best_word = word\n",
    "        print(\"closest match by\", name, \"distance:\", best_word)\n",
    "        print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest match by Euclidean distance: de\n",
      "king - man = de - woman\n",
      "closest match by cosine distance: boys\n",
      "king - man = boys - woman\n",
      "closest match by Euclidean distance: finally\n",
      "france - paris = finally - london\n",
      "closest match by cosine distance: quiet\n",
      "france - paris = quiet - london\n",
      "closest match by Euclidean distance: nothing\n",
      "france - paris = nothing - rome\n",
      "closest match by cosine distance: '\n",
      "france - paris = ' - rome\n",
      "closest match by Euclidean distance: playing\n",
      "paris - france = playing - italy\n",
      "closest match by cosine distance: leadership\n",
      "paris - france = leadership - italy\n"
     ]
    }
   ],
   "source": [
    "analogies_to_try = (\n",
    "    ('king', 'man', 'woman'),\n",
    "    ('france', 'paris', 'london'),\n",
    "    ('france', 'paris', 'rome'),\n",
    "    ('paris', 'france', 'italy'),\n",
    ")\n",
    "\n",
    "# test to make sure all analogy words are in the vocab\n",
    "# otherwise it wont work\n",
    "notfound = False\n",
    "for word_list in analogies_to_try:\n",
    "    for w in word_list:\n",
    "        if w not in word2idx:\n",
    "            print(\"%s not found in vocab, remove it from \\\n",
    "                analogies to try or increase vocab size\")\n",
    "            notfound = True\n",
    "if notfound:\n",
    "    exit()\n",
    "    \n",
    "for word_list in analogies_to_try:\n",
    "    w1, w2, w3 = word_list\n",
    "    find_analogies(w1, w2, w3, Z, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "### As expected, these word embeddings suck, and so we don't get very good word analogies. Lets move onto word2vec!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
