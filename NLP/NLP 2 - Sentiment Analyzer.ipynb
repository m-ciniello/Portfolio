{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression review\n",
    "Start with some feature vector x.\n",
    "- $w^T x = 0$ (the line itself... this is the hyperplane that splits the classes!!!)\n",
    "- $w^T x > 0$ (assign to a positive class)\n",
    "- $w^T x < 0$ (assign to a negative class)\n",
    "\n",
    "$\\sum_{d=1}^{D} w_dx_d = w^T x$ \n",
    "\n",
    "- This is pretty much the same as a single neuron, as each input gets multiplied by its corresponding weight, and the resulting products get added together!\n",
    "\n",
    "### Sigmoid function\n",
    "With a logisitc regression, we pass the output of the $w^T x$ into the sigmoid funciton to get an output between 0 and 1.\n",
    "\n",
    "$\\sigma(x)=\\frac{1}{1+exp^{-x}}$\n",
    "\n",
    "The probability that y is 1 (binary class) given x inputs is equal to the dot product of weights and inputs passed through the sigmoid funciton:\n",
    "\n",
    "$p(y=1|x) = \\sigma(w^T x)$\n",
    "\n",
    "Sigmoid turns aboslute numbers into probabilities between 0 and 1:\n",
    "\n",
    "$w^T x > 0 \\rightarrow \\sigma(w^T x) > 0.5$\n",
    "\n",
    "$w^T x < 0 \\rightarrow \\sigma(w^T x) < 0.5$ \n",
    "\n",
    "### Examples\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "- x = [1,1], w = [2,0]\n",
    "- x.dot(w) = 2\n",
    "- sigmoid(x.dot(w)) = 0.88\n",
    "- 88% certain that this is a class 1\n",
    "\n",
    "*Note that the output is unaffected by the second component of x. This is because a weight of 0 tells you the corresponding input feature has NO predictive ability.*\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "- x = [1,1], w = [0.5,0]\n",
    "- x.dot(w) = 0.5\n",
    "- sigmoid(x.dot(w)) = 0.62\n",
    "- 62% certain this is a class 1\n",
    "\n",
    "*Larger magnitude weights correspond to more important features in terms of predictive abilitiy.*\n",
    "\n",
    "#### Example 3\n",
    "\n",
    "- x = [1,1], w = [0.5,-1]\n",
    "- x.dot(w) = -0.5\n",
    "- sigmoid(x.dot(w)) = 0.38\n",
    "- 62% certain this is a class 1\n",
    "\n",
    "*Larger negative weight has more influence on the output.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "- This is not just using .split(\" \").\n",
    "- Have to deal with contractions (Don't = do not)\n",
    "- Extra punctuation and symbols\n",
    "- most of what you need to do can be done with nltk.tokenize.word_tokenize\n",
    "\n",
    "### Custom tokenizer\n",
    "- sometimes we want to write out own tokenizer\n",
    "- remove stopwords like 'the', 'a', 'it'\n",
    "- remove number and single letter tokens\n",
    "- can be application specific (specific to your dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokens to Vectors\n",
    "- you could use sklearn\n",
    "- however we will be using our own method!!!\n",
    "\n",
    "**Binary bag of words:**\n",
    "- if a word appears in a sentence, then that word gets a 1 in the feature vector\n",
    "- otherwise 0\n",
    "- doesnt matter how many times the word appears - appearing at least once gets you the same result\n",
    "- since all feature vectors must be the same size, *AND they need to be able to represent every sentence in our dataset*, then the size of the feature vector must equal size of vocabulary... **ie n features must equal number of unique words in ALL instances**\n",
    "- V is usually used to represent size of vocab\n",
    "\n",
    "**The Bag of Words Feature Vector:**\n",
    "- We need to know which words to map to which location in th vector\n",
    "- For this we have a dictionary or map:\n",
    "    - Key: word\n",
    "    - value: a unique integer from 0 to V-1 (this tells us the index location in the vector)\n",
    "    \n",
    "**Problem with COUNT bag of words (term frequency):**\n",
    "- For sentiment analysis, you generally want to use RAW COUNTS isntead\n",
    "- However the problem with using counts is that really long sentences become REALLY long arrows (in the vector space).\n",
    "- In other words, longer sentences (with more words) will skew the data\n",
    "- So, what you want to do instead is **NORMALIZE each count vector by the total # of words in each document, which turns each element in the vector to a proportion**\n",
    "- **THIS IS SUPER IMPORTANT FOR LOGISTIC REGRESSION (OR REALLY ANY ML TASK)**, MAINLY BECAUSE SIGMOID APPROACHES 1 VERRRRY QUICKLY (ANYTHING 10 OR ABOVE WILL BE SUPER CLOSE TO 1).... SO YOU GOTTA NORMALIZE!\n",
    "\n",
    "### As we can see below, everything above 2.5 or below -2.5 is squeezed closed to abs(1)... sooo you gotta normalize when doing a logistic regression! This goes true for most ML tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(-10, 10))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "sigmoid(x)\n",
    "\n",
    "plt.plot(x,sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#this turns words like Cat's into cat.\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "# this is a premade list of usual stopwords\n",
    "# these typically have low predictive value...\n",
    "stopwords = set(w.rstrip() for w in open('data/stopwords.txt'))\n",
    "\n",
    "# you could also use NLTK stop words source as follows:\n",
    "#from nltk.corpus import stopwords\n",
    "#stopwords.words('enlgish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('data/positive.review').read(),'lxml')\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('data/negative.review').read(), 'lxml')\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<review_text>\n",
      "I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\n",
      "\n",
      "I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\n",
      "\n",
      "As always, Amazon had it to me in &lt;2 business days\n",
      "</review_text>, <review_text>\n",
      "I ordered 3 APC Back-UPS ES 500s on the recommendation of an employee of mine who used to work at APC. I've had them for about a month now without any problems. They've functioned properly through a few unexpected power interruptions. I'll gladly order more if the need arises.\n",
      "\n",
      "Pros:\n",
      " - Large plug spacing, good for power adapters\n",
      " - Simple design\n",
      " - Long cord\n",
      "\n",
      "Cons:\n",
      " - No line conditioning (usually an expensive option\n",
      "</review_text>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(positive_reviews[0:2])\n",
    "len(positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<review_text>\n",
      "cons\n",
      "tips extremely easy on carpet and if you have a lot of cds stacked at the top\n",
      "\n",
      "poorly designed, it is a vertical cd rack that doesnt have individual slots for cds, so if you want a cd from the bottom of a stack you have basically pull the whole stack to get to it\n",
      "\n",
      "putting it together was a pain, the one i bought i had to break a piece of metal just to fit it in its guide holes.\n",
      "\n",
      "again..poorly designed... doesnt even fit cds that well, there are gaps, and the cd casses are loose fitting\n",
      "\n",
      "pros\n",
      "..........\n",
      "i guess it can hold a lot of cds....\n",
      "</review_text>, <review_text>\n",
      "It's a nice look, but it tips over very easily. It is not steady on a rug surface dispite what the picture on the box shows. My advice is if you need a CD rack that holds a lot of CD's? Save your money and invest in something nicer and more sturdy\n",
      "</review_text>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(negative_reviews[0:2])\n",
    "len(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust for class imbalance\n",
    "\n",
    "##### Note: the site appears to have updated the data, and there are no more class imbalances! So dont run the code below...\n",
    "    #Option 1: take a random sample so we have balanced classes\n",
    "    np.random.shuffle(positive_reviews)\n",
    "    positive_reviews = positive_reviews[:len(negative_reviews)]\n",
    "\n",
    "    #Option 2: Oversample the negative reviews\n",
    "    diff = len(positive_reviews) - len(negative_reviews)\n",
    "    idxs = np.random.choice(len(negative_reviews), size=diff)\n",
    "    extra = [negative_reviews[i] for i in idxs]\n",
    "    negative_reviews += extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "First we will trey to use NLTKs tokenizer. However you will notice that is doesnt downcase so 'It' != 'it'. Thats not good! \n",
    "\n",
    "Also you really don't even want to include the word 'it'... its a stop word, and not of much use to us. It would probably only add noise to the model! Soooo after we use NLTK we will build are own tokenize function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',',\n",
       " '.',\n",
       " '2',\n",
       " '5',\n",
       " '<',\n",
       " 'Amazon',\n",
       " 'As',\n",
       " 'Equally',\n",
       " 'I',\n",
       " 'It',\n",
       " 'LCD',\n",
       " 'PC',\n",
       " 'This',\n",
       " 'a',\n",
       " 'always',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'area',\n",
       " 'bad',\n",
       " 'blackouts',\n",
       " 'business',\n",
       " 'cable',\n",
       " 'clean',\n",
       " 'compared',\n",
       " 'data',\n",
       " 'days',\n",
       " 'down',\n",
       " 'due',\n",
       " 'electronics',\n",
       " 'enough',\n",
       " 'equipment',\n",
       " 'failure',\n",
       " 'feel',\n",
       " 'for',\n",
       " 'frequent',\n",
       " 'going',\n",
       " 'had',\n",
       " 'important',\n",
       " 'in',\n",
       " 'investment',\n",
       " 'irregular',\n",
       " 'is',\n",
       " 'it',\n",
       " 'know',\n",
       " 'loss',\n",
       " 'me',\n",
       " 'minor',\n",
       " 'minutes',\n",
       " 'modem',\n",
       " 'monitor',\n",
       " 'more',\n",
       " 'my',\n",
       " 'of',\n",
       " 'or',\n",
       " 'power',\n",
       " 'purchased',\n",
       " 'receiving',\n",
       " 'router',\n",
       " 'run',\n",
       " 'save',\n",
       " 'shut',\n",
       " 'spike',\n",
       " 'supplies',\n",
       " 'supply',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'this',\n",
       " 'time',\n",
       " 'to',\n",
       " 'unit',\n",
       " 'valuable',\n",
       " 'will',\n",
       " 'work'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize with NLTK\n",
    "#notice how it keeps \"it\" and \"IT\"\n",
    "t = positive_reviews[0].text\n",
    "set(nltk.tokenize.word_tokenize(t)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['purchased',\n",
       " 'this',\n",
       " 'unit',\n",
       " 'due',\n",
       " 'frequent',\n",
       " 'blackout',\n",
       " 'power',\n",
       " 'supply',\n",
       " 'bad',\n",
       " 'run',\n",
       " 'cable',\n",
       " 'modem',\n",
       " 'router',\n",
       " 'lcd',\n",
       " 'monitor',\n",
       " 'minute',\n",
       " 'this',\n",
       " 'time',\n",
       " 'save',\n",
       " 'shut',\n",
       " 'equally',\n",
       " 'electronics',\n",
       " 'receiving',\n",
       " 'clean',\n",
       " 'power',\n",
       " 'feel',\n",
       " 'this',\n",
       " 'investment',\n",
       " 'minor',\n",
       " 'compared',\n",
       " 'loss',\n",
       " 'valuable',\n",
       " 'data',\n",
       " 'failure',\n",
       " 'equipment',\n",
       " 'due',\n",
       " 'power',\n",
       " 'spike',\n",
       " 'irregular',\n",
       " 'power',\n",
       " 'supply',\n",
       " 'amazon',\n",
       " 'business',\n",
       " 'day']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets create a custom tokenize funciton (leveraging nltk)\n",
    "def mikes_tokenizer(s):\n",
    "    s = s.lower() #downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) #split s into word tokens\n",
    "    tokens = [t for t in tokens if len(t)>2] #remove short words as they probably have low predictive quality\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] #puts words (like contractions) into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] #get rid of dem stopwords yo!\n",
    "    return tokens\n",
    "\n",
    "mikes_tokenizer(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Index Map\n",
    "\n",
    "Lets create a word-to-index map so that we can create our word frequency vectors later. Lets also save the tokenized versions so we dont have to tokenize again later.\n",
    "\n",
    "Note that for the word map, we first tokenize each document, loop through all the words, and add the word to the map dictionary (adding the word only if it is not yet in the dictionary, this ensures we get an index of unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = mikes_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    tokens = mikes_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11078"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02272727, 0.06818182, 0.02272727, ..., 0.        , 0.        ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map)+1) #last element is for label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() #normalize it before setting the label\n",
    "    x[-1] = label #set the label\n",
    "    return x\n",
    "\n",
    "tokens_to_vector(positive_tokenized[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 11079)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get length of all instances together\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "#put the instances together so we can shuffle them more easily\n",
    "\n",
    "data = np.zeros((N, len(word_index_map)+1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens,1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens,0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle data and create train test splits!\n",
    "np.random.shuffle(data)\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "X_train = X[:-100,]\n",
    "y_train = y[:-100,]\n",
    "X_test = X[-100:,]\n",
    "y_test = y[-100:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mciniello\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\mciniello\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model accuracy: 0.74\n",
      "Random Forest model accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "#build a logistic regression model!\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model accuracy:\", lr_model.score(X_test, y_test))\n",
    "\n",
    "#build a random forest model!\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Random Forest model accuracy:\", rf_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11078)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets look at the logistic regression weights of the words\n",
    "#this will help us find the most important words for this predictor\n",
    "print(lr_model.coef_.shape)\n",
    "\n",
    "plt.hist(lr_model.coef_[0], range=(-0.25,0.25), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'month': -0.8308978404073868,\n",
       " \"n't\": -1.9967793443415716,\n",
       " 'item': -0.963774184305869,\n",
       " 'wa': -1.7716706291161686,\n",
       " 'money': -1.110057087149993,\n",
       " 'buy': -0.9096837870362605,\n",
       " 'doe': -1.1921434831360513,\n",
       " 'support': -0.8819036041762232,\n",
       " 'returned': -0.8150971252448324,\n",
       " 'poor': -0.8016260293204787,\n",
       " 'then': -1.1413942994706288,\n",
       " 'tried': -0.8034127295300082,\n",
       " 'return': -1.2124963282742895,\n",
       " 'waste': -1.019238021697776}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words associated with bad reviews \n",
    "word_index_weights = {}\n",
    "threshold_high = -0.75\n",
    "threshold_low = -2.5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = lr_model.coef_[0][index]\n",
    "    if (weight < threshold_high) and (weight > threshold_low):\n",
    "        word_index_weights[word] = weight  \n",
    "word_index_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'ve\": 0.856243345075673,\n",
       " 'sound': 0.974533464700213,\n",
       " 'you': 0.9604589675104138,\n",
       " 'easy': 1.6615696255287813,\n",
       " 'quality': 1.417141215504284,\n",
       " 'perfect': 1.0047165596703336,\n",
       " 'fast': 0.905623165529749,\n",
       " 'memory': 0.9523414831354218,\n",
       " 'highly': 0.9785862268027192,\n",
       " 'little': 0.892228011349714,\n",
       " 'excellent': 1.315957065814149,\n",
       " 'love': 1.0778929252574783,\n",
       " 'speaker': 0.8548613068941959}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words associated with good reviews \n",
    "word_index_weights = {}\n",
    "threshold_high = 2.5\n",
    "threshold_low = 0.75\n",
    "for word, index in word_index_map.items():\n",
    "    weight = lr_model.coef_[0][index]\n",
    "    if (weight < threshold_high) and (weight > threshold_low):\n",
    "        word_index_weights[word] = weight  \n",
    "word_index_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
