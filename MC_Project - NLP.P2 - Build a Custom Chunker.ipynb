{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Hackers - Part 3 - Build a Custom Chunker\n",
    "\n",
    "Notes from the fantastic book 'NLP for Hackers' by George-Bogdan Ivanov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Chunker (Shallow Parsing) Basics\n",
    "\n",
    "Chunking, or **Shallow Parsing**, adds more structure on top of a Part-Of-Speech annotated text. The result of chunking is a **grouping of consecutive words that serve a single role.**\n",
    "\n",
    "![](images/nlpforhackers_chunker.jpg)\n",
    "\n",
    "Just with POS tags, as we saw in the last notebook, Chunking has it's own set of tags. Above you see:\n",
    "- NP: Noun Phrase\n",
    "- VP: Verb Phrase\n",
    "- PP: Prepositional Phrase\n",
    "\n",
    "There’s also **normal parsing, called deep parsing,** which has as a result an entire syntax tree. As you can probably guess, **Deep Parsing is a much more complex task, more prone to errors and much slower than simple chunking.** Obviously, it really depends on the situation and deciding which tool would best solve the task should be made on a case-by-case basis. \n",
    "\n",
    "Let’s see how shallow and deep parsers compare:\n",
    "\n",
    "**Shallow parser:**\n",
    "```\n",
    "(S\n",
    "    (NP The/DT quick/JJ brown/NN fox/NN)\n",
    "    (VP jumps/VBZ)\n",
    "    (PP over/IN)\n",
    "    (NP the/DT lazy/JJ dog/NN)\n",
    "./.)\n",
    "```\n",
    "\n",
    "**Deep parser:**\n",
    "\n",
    "```\n",
    "(S\n",
    "    (NP The/DT quick/JJ brown/JJ fox/NN)\n",
    "    (VP jumps/VBZ\n",
    "        (PP over/IN\n",
    "            (NP the/DT lazy/JJ dog/NN)))\n",
    "(. .))\n",
    "```\n",
    "\n",
    "We can use `nltk.ne_chunk` to visualize shallow parse trees. The ne_chunk method is simply applying a pre trained chunker that is installed with nlkt. It can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have to first add the ghost script to our environment so nltk.tree works properly\n",
    "# More details here https://www.ghostscript.com/download/gsdnld.html\n",
    "import os\n",
    "path_to_gs = \"C:\\\\Program Files\\\\gs\\\\gs9.24\\\\bin\"\n",
    "os.environ['PATH'] += os.pathsep + path_to_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Quick', 'NNP'),\n",
       " ('Brown', 'NNP'),\n",
       " ('Fox', 'NNP'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Lazy', 'NNP'),\n",
       " ('Dog', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tagged tokens\n",
    "import nltk\n",
    "sentence = \"\"\"The Quick Brown Fox jumps over the Lazy Dog.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION Quick/NNP Brown/NNP Fox/NNP)\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Lazy/NNP Dog/NNP)\n",
      "  ./.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Quick', 'NNP'), ('Brown', 'NNP'), ('Fox', 'NNP')]), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('Lazy', 'NNP'), ('Dog', 'NNP')]), ('.', '.')])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "#Use pretrained ne_chunk model\n",
    "ner_annotated_tree = nltk.ne_chunk(tagged_tokens)\n",
    "print(ner_annotated_tree)\n",
    "display(ner_annotated_tree) # NOTE this isnt working for some reaosn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Under the hood: IOB Tagging\n",
    "\n",
    "Shallow parsing is a pretty easy task and is **similar to Part-of-Speech tagging**. In fact, it can be **reduced to a tagging problem.** To understand how it works, we need to learn a new tagging technique, called **IOB Tagging (or BIO Tagging).** \n",
    "\n",
    "IOB Tagging is useful especially for **annotating multi-word (or consecutive words) sequences.** As you probably noticed in the previous example, some tokens are grouped under a single label representing such a grouping. Here’s how the IOB tags\n",
    "look:\n",
    "\n",
    "- `B-<CHUNK_TYPE>` : for the word from the Beginning of the chunk\n",
    "- `I-<CHUNK_TYPE>` : for words Inside the chunk\n",
    "- `O` : Outside any chunk\n",
    "\n",
    "To reiterate, the `B-<CHUNK_TYPE>` marks the beginning of a chunk CHUNK_TYPE and the `I-<CHUNK_TYPE>` marks that we’re inside of a chunk CHUNK_TYPE. The O marks that we’re outside of any type of chunk. \n",
    "\n",
    "Here’s how our sample sentence looks like when presented in IOB tagged form:\n",
    "\n",
    "```\n",
    "[('The', 'DT', 'B-NP'),\n",
    "('quick', 'JJ', 'I-NP'),\n",
    "('brown', 'NN', 'I-NP'),\n",
    "('fox', 'NN', 'I-NP'),\n",
    "('jumps', 'VBZ', 'B-VP'),\n",
    "('over', 'IN', 'B-PP'),\n",
    "('the', 'DT', 'B-NP'),\n",
    "('lazy', 'JJ', 'I-NP'),\n",
    "('dog', 'NN', 'I-NP'),\n",
    "('.', '.', 'O')]\n",
    "```\n",
    "\n",
    "NLTK also has a chunking corpus, CoNLL Dataset, which we will explore now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Tree('S', [Tree('NP', [('Confidence', 'NN')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('the', 'DT'), ('pound', 'NN')]), Tree('VP', [('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB')]), Tree('NP', [('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN')]), ('if', 'IN'), Tree('NP', [('trade', 'NN'), ('figures', 'NNS')]), Tree('PP', [('for', 'IN')]), Tree('NP', [('September', 'NNP')]), (',', ','), ('due', 'JJ'), Tree('PP', [('for', 'IN')]), Tree('NP', [('release', 'NN')]), Tree('NP', [('tomorrow', 'NN')]), (',', ','), Tree('VP', [('fail', 'VB'), ('to', 'TO'), ('show', 'VB')]), Tree('NP', [('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN')]), Tree('PP', [('from', 'IN')]), Tree('NP', [('July', 'NNP'), ('and', 'CC'), ('August', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS')]), ('.', '.')])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "# Import data in tree format\n",
    "tree_sent = conll2000.chunked_sents()\n",
    "print(len(tree_sent))\n",
    "tree_sent[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Tree('NP', [('Confidence', 'NN')]),\n",
       " Tree('PP', [('in', 'IN')]),\n",
       " Tree('NP', [('the', 'DT'), ('pound', 'NN')]),\n",
       " Tree('VP', [('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB')]),\n",
       " Tree('NP', [('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN')]),\n",
       " ('if', 'IN'),\n",
       " Tree('NP', [('trade', 'NN'), ('figures', 'NNS')]),\n",
       " Tree('PP', [('for', 'IN')]),\n",
       " Tree('NP', [('September', 'NNP')]),\n",
       " (',', ',')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data \n",
    "tree_words = conll2000.chunked_words()\n",
    "print(len(tree_words))\n",
    "tree_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting between nltk.trees and IOB tripliets\n",
    "As you can notice, in NLTK the default way of representing Shallow Parses is with **nltk.Tree.** We can transform the chunked sentence (represented as a nltk.Tree) to IOB tagged triplets using NLTKs `tree2conlltags` function, and revert back to a tree format using the `tree2conlltags` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Confidence', 'NN', 'B-NP'),\n",
       " ('in', 'IN', 'B-PP'),\n",
       " ('the', 'DT', 'B-NP'),\n",
       " ('pound', 'NN', 'I-NP'),\n",
       " ('is', 'VBZ', 'B-VP'),\n",
       " ('widely', 'RB', 'I-VP'),\n",
       " ('expected', 'VBN', 'I-VP'),\n",
       " ('to', 'TO', 'I-VP'),\n",
       " ('take', 'VB', 'I-VP'),\n",
       " ('another', 'DT', 'B-NP'),\n",
       " ('sharp', 'JJ', 'I-NP'),\n",
       " ('dive', 'NN', 'I-NP'),\n",
       " ('if', 'IN', 'O'),\n",
       " ('trade', 'NN', 'B-NP'),\n",
       " ('figures', 'NNS', 'I-NP')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tree to iob tags\n",
    "from nltk.chunk import tree2conlltags\n",
    "iob = tree2conlltags(tree_sent[0])\n",
    "iob[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT pound/NN)\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN figures/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  (PP for/IN)\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  (VP fail/VB to/TO show/VB)\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  (PP from/IN)\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Convert IOB tags to tree\n",
    "from nltk.chunk import conlltags2tree\n",
    "tree = conlltags2tree(iob)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementing the Chunk Parser\n",
    "\n",
    "The base class for implementing chunkers in NLTK is called `nltk.ChunkParserI`. Let’s extend that class and wrap inside it the ClassifierBasedTaggerBatchTrained we built in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input-Output function\n",
    "def print_io(function, input_, args=None, input_idx=0):\n",
    "    \"\"\"Function to quickly print inputs and outputs of a function:\n",
    "        function: function to run\n",
    "        input_: input to funciton\n",
    "        args: list of args to be fed into function\n",
    "        input_idx: index of input_ in the args list (defaults to zero)\"\"\"    \n",
    "    if args:\n",
    "        args.insert(input_idx, input_)\n",
    "        output = function(*args)\n",
    "    else:\n",
    "        output = function(input_)\n",
    "    func_name = function.__name__\n",
    "    print(\"{} INPUT:\\n\".format(func_name), input_)\n",
    "    print(\"\\n{} OUTPUT:\\n\".format(func_name),output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Build custome tree-to-tags functions \n",
    "\n",
    "Simple functions for converting triple tags to ((word,pos),chunk) format. This will be useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplets2tagged_pairs INPUT:\n",
      " [('Confidence', 'NN', 'B-NP'), ('in', 'IN', 'B-PP'), ('the', 'DT', 'B-NP'), ('pound', 'NN', 'I-NP'), ('is', 'VBZ', 'B-VP'), ('widely', 'RB', 'I-VP'), ('expected', 'VBN', 'I-VP'), ('to', 'TO', 'I-VP'), ('take', 'VB', 'I-VP'), ('another', 'DT', 'B-NP'), ('sharp', 'JJ', 'I-NP'), ('dive', 'NN', 'I-NP'), ('if', 'IN', 'O'), ('trade', 'NN', 'B-NP'), ('figures', 'NNS', 'I-NP'), ('for', 'IN', 'B-PP'), ('September', 'NNP', 'B-NP'), (',', ',', 'O'), ('due', 'JJ', 'O'), ('for', 'IN', 'B-PP'), ('release', 'NN', 'B-NP'), ('tomorrow', 'NN', 'B-NP'), (',', ',', 'O'), ('fail', 'VB', 'B-VP'), ('to', 'TO', 'I-VP'), ('show', 'VB', 'I-VP'), ('a', 'DT', 'B-NP'), ('substantial', 'JJ', 'I-NP'), ('improvement', 'NN', 'I-NP'), ('from', 'IN', 'B-PP'), ('July', 'NNP', 'B-NP'), ('and', 'CC', 'I-NP'), ('August', 'NNP', 'I-NP'), (\"'s\", 'POS', 'B-NP'), ('near-record', 'JJ', 'I-NP'), ('deficits', 'NNS', 'I-NP'), ('.', '.', 'O')]\n",
      "\n",
      "triplets2tagged_pairs OUTPUT:\n",
      " [(('Confidence', 'NN'), 'B-NP'), (('in', 'IN'), 'B-PP'), (('the', 'DT'), 'B-NP'), (('pound', 'NN'), 'I-NP'), (('is', 'VBZ'), 'B-VP'), (('widely', 'RB'), 'I-VP'), (('expected', 'VBN'), 'I-VP'), (('to', 'TO'), 'I-VP'), (('take', 'VB'), 'I-VP'), (('another', 'DT'), 'B-NP'), (('sharp', 'JJ'), 'I-NP'), (('dive', 'NN'), 'I-NP'), (('if', 'IN'), 'O'), (('trade', 'NN'), 'B-NP'), (('figures', 'NNS'), 'I-NP'), (('for', 'IN'), 'B-PP'), (('September', 'NNP'), 'B-NP'), ((',', ','), 'O'), (('due', 'JJ'), 'O'), (('for', 'IN'), 'B-PP'), (('release', 'NN'), 'B-NP'), (('tomorrow', 'NN'), 'B-NP'), ((',', ','), 'O'), (('fail', 'VB'), 'B-VP'), (('to', 'TO'), 'I-VP'), (('show', 'VB'), 'I-VP'), (('a', 'DT'), 'B-NP'), (('substantial', 'JJ'), 'I-NP'), (('improvement', 'NN'), 'I-NP'), (('from', 'IN'), 'B-PP'), (('July', 'NNP'), 'B-NP'), (('and', 'CC'), 'I-NP'), (('August', 'NNP'), 'I-NP'), ((\"'s\", 'POS'), 'B-NP'), (('near-record', 'JJ'), 'I-NP'), (('deficits', 'NNS'), 'I-NP'), (('.', '.'), 'O')]\n"
     ]
    }
   ],
   "source": [
    "def triplets2tagged_pairs(iob_sent):\n",
    "    \"\"\"\n",
    "    Transform the triplets to tagged pairs:\n",
    "    [(word1, pos1, iob1), (word2, pos2, iob2), ...] ->\n",
    "    [((word1, pos1), iob1), ((word2, pos2), iob2),...]\n",
    "    \"\"\"\n",
    "    return [((word, pos), chunk) for word, pos, chunk in iob_sent]\n",
    "\n",
    "def tagged_pairs2triplets(iob_sent):\n",
    "    \"\"\"\n",
    "    Transform the triplets to tagged pairs:\n",
    "    [((word1, pos1), iob1), ((word2, pos2), iob2),...] ->\n",
    "    [(word1, pos1, iob1), (word2, pos2, iob2), ...]\n",
    "    \"\"\"\n",
    "    return [(word, pos, chunk) for (word, pos), chunk in iob_sent]\n",
    "\n",
    "print_io(triplets2tagged_pairs, iob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Chunk Feature Detector\n",
    "\n",
    "Just like we did for the POS features, we need to create a function to extract IOB features. Note that nltk has its own chunk feature detector, accessed through via `nltk.chunk.named_entity import NEChunkParserTagger`, however as per usual, we are going to build our own from scratch! Just for funsies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Bring the shape function back in!\n",
    "def shape(word):\n",
    "    if re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word):\n",
    "        return 'number'\n",
    "    elif re.match('\\W+$', word):\n",
    "        return 'punct'\n",
    "    elif re.match('[A-Z][a-z]+$', word):\n",
    "        return 'capitalized'\n",
    "    elif re.match('[A-Z]+$', word):\n",
    "        return 'uppercase'\n",
    "    elif re.match('[a-z]+$', word):\n",
    "        return 'lowercase'\n",
    "    elif re.match('[A-Z][a-z]+[A-Z][a-z]+[A-Za-z]*$', word):\n",
    "        return 'camelcase'\n",
    "    elif re.match('[A-Za-z]+$', word):\n",
    "        return 'mixedcase'\n",
    "    elif re.match('__.+__$', word):\n",
    "        return 'wildcard'\n",
    "    elif re.match('[A-Za-z0-9]+\\.$', word):\n",
    "        return 'ending-dot'\n",
    "    elif re.match('[A-Za-z0-9]+\\.[A-Za-z0-9\\.]+\\.$', word):\n",
    "        return 'abbreviation'\n",
    "    elif re.match('[A-Za-z0-9]+\\-[A-Za-z0-9\\-]+.*$', word):\n",
    "        return 'contains-hyphen'\n",
    "    return 'other'\n",
    "\n",
    "def chunk_features(sentence, index, history, print_results=False):\n",
    "    \"\"\"\n",
    "    `sentence` = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index` = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    "    # Pad the sequence with placeholders\n",
    "    sentence = ([('__START2__', '__START2__'), ('__START1__', '__START1__')] + # APPEND START SENTENCE/TAGS\n",
    "              list(sentence) +\n",
    "              [('__END1__', '__END1__'), ('__END2__', '__END2__')]) # APPEND END SENTENCE/TAGS\n",
    "    history = ['__START2__', '__START1__'] + list(history)\n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    "    word, pos = sentence[index]\n",
    "    prevword, prevpos = sentence[index - 1]\n",
    "    prevprevword, prevprevpos = sentence[index - 2]\n",
    "    nextword, nextpos = sentence[index + 1]\n",
    "    nextnextword, nextnextpos = sentence[index + 2]\n",
    "    word_shape = shape(word)\n",
    "    features = {'word': word, \n",
    "            'lemma': stemmer.stem(word), \n",
    "            'shape': word_shape,\n",
    "            'pos': pos, \n",
    "            'next-word': nextword, \n",
    "            'next-pos': nextpos,\n",
    "            'next-next-word': nextnextword,\n",
    "            'nextnextpos': nextnextpos,\n",
    "            'prev-word': prevword,\n",
    "            'prev-pos': prevpos,\n",
    "            'prev-prev-word': prevprevword,\n",
    "            'prev-prev-pos': prevprevpos,\n",
    "            # Historical features\n",
    "            'prev-chunk': history[-1],\n",
    "            'prev-prev-chunk': history[-2]}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_features INPUT:\n",
      " (('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'))\n",
      "\n",
      "chunk_features OUTPUT:\n",
      " {'word': 'Confidence', 'lemma': 'confid', 'shape': 'capitalized', 'pos': 'NN', 'next-word': 'in', 'next-pos': 'IN', 'next-next-word': 'the', 'nextnextpos': 'DT', 'prev-word': '__START1__', 'prev-pos': '__START1__', 'prev-prev-word': '__START2__', 'prev-prev-pos': '__START2__', 'prev-chunk': '__START1__', 'prev-prev-chunk': '__START2__'}\n"
     ]
    }
   ],
   "source": [
    "# Extract a single \n",
    "tagged_pairs = triplets2tagged_pairs(tree2conlltags(tree_sent[0]))\n",
    "pos, chunks = zip(*tagged_pairs)\n",
    "print_io(chunk_features, input_=pos[0:5], args=[0,[]], input_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build the tagging classifier\n",
    "\n",
    "Since this is still just a tagging exercise, we can use the re-use our tagger from the previous exercise. Recall that the wrapper has the following steps:\n",
    "1. Convert dataset to necessary format\n",
    "2. Vectorizez the dataset (either with DictVectorizer or FeatureHasher)\n",
    "3. Fit the model (partial fit if you are using FeatureHasher)\n",
    "4. Predict and evaluate (to be run after model has been fitted)\n",
    "\n",
    "We will create a new function called `Mikes_Chunk_Tagger`, and we wrap it around `Mikes_POS_Tagger` so we don't have to write all the code again. However we have coppied and pasted the original pos tagger below for a refresher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_extract_all_labels',\n",
       " '_tags_to_dataset',\n",
       " '_vectorize',\n",
       " 'accuracy',\n",
       " 'evaluate',\n",
       " 'fit',\n",
       " 'partial_fit',\n",
       " 'tag',\n",
       " 'tag_sents']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from NLP_custom import Mikes_POS_Tagger\n",
    "[method for method in dir(Mikes_POS_Tagger) if not(method.startswith(\"__\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.tag.util import untag\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk.chunk import tree2conlltags\n",
    "from itertools import chain\n",
    "\n",
    "class Mikes_Chunk_Parser(Mikes_POS_Tagger):\n",
    "    def triplets2tagged_pairs(self, iob_sent):\n",
    "        \"\"\"\n",
    "        Transform the triplets to tagged pairs:\n",
    "        [(word1, pos1, iob1), (word2, pos2, iob2), ...] ->\n",
    "        [((word1, pos1), iob1), ((word2, pos2), iob2),...]\n",
    "        \"\"\"\n",
    "        return [((word, pos), chunk) for word, pos, chunk in iob_sent]\n",
    "\n",
    "    def trees2tagged_pairs(self, trees):\n",
    "        \"\"\"\n",
    "        Transform nltk.trees to tuples (word, pos_tag, iob_tag)\n",
    "        \"\"\"\n",
    "        # Use nltk.tree2conlltags to transform trees into triple suples (word, pos_tag, IOB_tag)\n",
    "        chunked_sentences = [tree2conlltags(sent) for sent in trees]\n",
    "        # Use custom util function to transform tuples to format: ((word, pos_tag),IOB_tag)\n",
    "        return [self.triplets2tagged_pairs(sent) for sent in chunked_sentences]\n",
    "    \n",
    "    def _tags_to_dataset(self, trees):\n",
    "        \"\"\"\n",
    "        Helper function:\n",
    "          Take in train data (tree sentences) and return feature dict (pre-vectorized dataset)\n",
    "        \"\"\"\n",
    "        chunked_sentences = self.trees2tagged_pairs(trees)\n",
    "        # Initialize empty featursets list\n",
    "        classifier_corpus = []\n",
    "        for sentence in chunked_sentences:\n",
    "            # Initialize empty history list (will be updated as loop through each word in sent)\n",
    "            history = []\n",
    "            # Use zip(* ) to zip tokens & tags into two separate lists\n",
    "            sentence_tokens, sentence_tags = zip(*sentence)\n",
    "            # Loop through each word in sentence\n",
    "            # Duplicate words are kept because contexts (prev/post words) may differ\n",
    "            for index in range(len(sentence)):\n",
    "                # Use the feature detector (eg. pos_features) initialized with the class\n",
    "                featureset = self.feature_detector(sentence_tokens, index, history)\n",
    "                # Update featursets list with tuple (featureset, tag)\n",
    "                classifier_corpus.append((featureset, sentence_tags[index]))\n",
    "                # Update history for next index word\n",
    "                history.append(sentence_tags[index])\n",
    "        return classifier_corpus\n",
    "    \n",
    "    def evaluate(self, test_set, verbose=None):\n",
    "        \"\"\"\n",
    "        Evaluation function:\n",
    "          Returns 'accuracy' metric using sklearn built-in 'score' method.\n",
    "        \"\"\"\n",
    "        t0=time()\n",
    "        test_set = self.trees2tagged_pairs(test_set)\n",
    "        untagged_sents = [untag(sent) for sent in test_set]\n",
    "        retagged_sents = self.tag_sents(untagged_sents)\n",
    "        preds = list(chain(*retagged_sents))\n",
    "        actual = list(chain(*test_set))\n",
    "        score = self.accuracy(actual, preds)\n",
    "        if verbose:\n",
    "            print(\"Accuracy: {}, Eval Runtime: {:.2f}\".format(score, time()-t0))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10948"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conll2000.chunked_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing feature extraction...\n",
      "Training model on features set size (213155, 105880)\n",
      "\n",
      "Training complete. Total runtime: 16.64 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9495962915406211"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = Mikes_Chunk_Parser(feature_detector=chunk_features, sparse=True)\n",
    "parser.fit(conll2000.chunked_sents()[0:9000])\n",
    "parser.evaluate(conll2000.chunked_sents()[9000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('The', 'DT'), 'B-NP'),\n",
       " (('quick', 'JJ'), 'I-NP'),\n",
       " (('brown', 'NN'), 'I-NP'),\n",
       " (('fox', 'NN'), 'I-NP'),\n",
       " (('jumps', 'VBZ'), 'B-VP'),\n",
       " (('over', 'IN'), 'B-PP'),\n",
       " (('the', 'DT'), 'B-NP'),\n",
       " (('lazy', 'JJ'), 'I-NP'),\n",
       " (('dog', 'NN'), 'I-NP'),\n",
       " (('.', '.'), 'O')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent  = \"The quick brown fox jumps over the lazy dog.\"\n",
    "test_tokens = nltk.word_tokenize(test_sent)\n",
    "test_tagged = nltk.pos_tag(test_tokens)\n",
    "test_parsed = parser.tag(test_tagged)\n",
    "test_parsed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
